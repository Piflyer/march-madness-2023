{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "city_data = pd.read_csv(\"data/Cities.csv\")\n",
    "conferences = pd.read_csv(\"data/Conferences.csv\")\n",
    "tourneygames = pd.read_csv(\"data/MConferenceTourneyGames.csv\")\n",
    "# tourneyresults = pd.read_csv(\"data/MNCAATourneyCompactResults.csv\")\n",
    "tourneydetailedresults = pd.read_csv(\"data/MNCAATourneyDetailedResults.csv\")\n",
    "tourneyseeds = pd.read_csv(\"data/MNCAATourneySeeds.csv\")\n",
    "# tourneyseedresutls = pd.read_csv(\"data/MNCAATourneySeedRoundSlots.csv\")\n",
    "regularseasoncompact = pd.read_csv(\"data/MRegularSeasonCompactResults.csv\")\n",
    "regularseasondetailed = pd.read_csv(\"data/MRegularSeasonDetailedResults.csv\")\n",
    "# seasons = pd.read_csv(\"data/MSeasons.csv\")\n",
    "teams = pd.read_csv(\"data/MTeams.csv\")\n",
    "# secondarytourneycompact = pd.read_csv(\"data/MSecondaryTourneyCompactResults.csv\")\n",
    "secondarytourneydetailed = pd.read_csv(\"data/MSecondaryTourneyTeams.csv\")\n",
    "# team_conerences = pd.read_csv(\"data/MTeamConferences.csv\")\n",
    "# team_spellings = pd.read_csv(\"data/MTeamSpellings.csv\")\n",
    "sample_submission = pd.read_csv(\"data/SampleSubmission2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  WFGA  \\\n0    2003      10     1104      68     1328      62    N      0    27    58   \n1    2003      10     1272      70     1393      63    N      0    26    62   \n2    2003      11     1266      73     1437      61    N      0    24    58   \n3    2003      11     1296      56     1457      50    N      0    18    38   \n4    2003      11     1400      77     1208      71    N      0    30    61   \n\n   ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n0  ...     10    16    22   10   22     8   18     9     2   20  \n1  ...     24     9    20   20   25     7   12     8     6   16  \n2  ...     26    14    23   31   22     9   12     2     5   23  \n3  ...     22     8    15   17   20     9   19     4     3   23  \n4  ...     16    17    27   21   15    12   10     7     1   14  \n\n[5 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>...</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>58</td>\n      <td>...</td>\n      <td>10</td>\n      <td>16</td>\n      <td>22</td>\n      <td>10</td>\n      <td>22</td>\n      <td>8</td>\n      <td>18</td>\n      <td>9</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>62</td>\n      <td>...</td>\n      <td>24</td>\n      <td>9</td>\n      <td>20</td>\n      <td>20</td>\n      <td>25</td>\n      <td>7</td>\n      <td>12</td>\n      <td>8</td>\n      <td>6</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>26</td>\n      <td>14</td>\n      <td>23</td>\n      <td>31</td>\n      <td>22</td>\n      <td>9</td>\n      <td>12</td>\n      <td>2</td>\n      <td>5</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>38</td>\n      <td>...</td>\n      <td>22</td>\n      <td>8</td>\n      <td>15</td>\n      <td>17</td>\n      <td>20</td>\n      <td>9</td>\n      <td>19</td>\n      <td>4</td>\n      <td>3</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>...</td>\n      <td>16</td>\n      <td>17</td>\n      <td>27</td>\n      <td>21</td>\n      <td>15</td>\n      <td>12</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularseasondetailed.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/5rcwl4d96zvcq7zxv7fzt3m80000gn/T/ipykernel_46974/2827940207.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  team_stats = team_stats.append(regularseasondetailed.loc[(regularseasondetailed[\"LTeamID\"] == team_id) & (regularseasondetailed[\"Season\"] == season)])\n"
     ]
    },
    {
     "data": {
      "text/plain": "       Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  \\\n82041    2019       1     1104      82     1380      62    H      0    27   \n82277    2019       6     1104      81     1111      73    H      0    25   \n82443    2019      11     1104      79     1123      61    N      0    33   \n82577    2019      13     1104      90     1455      86    N      0    25   \n82969    2019      21     1104      78     1293      72    H      0    25   \n83490    2019      34     1104      76     1112      73    H      0    26   \n83709    2019      43     1104      84     1251      75    N      0    26   \n83824    2019      46     1104      73     1336      64    H      0    28   \n84091    2019      55     1104      79     1372      69    A      0    27   \n84260    2019      61     1104      77     1246      75    H      0    27   \n84781    2019      72     1104      70     1281      60    A      0    26   \n85085    2019      78     1104      74     1279      53    H      0    25   \n85405    2019      85     1104      83     1280      79    H      0    30   \n85752    2019      93     1104      89     1208      74    H      0    33   \n85875    2019      96     1104      77     1435      67    A      0    24   \n86515    2019     110     1104      68     1435      61    H      0    24   \n86690    2019     113     1104      68     1376      62    A      0    24   \n87366    2019     129     1104      62     1279      57    N      0    24   \n82427    2019      10     1318      68     1104      52    N      0    22   \n83107    2019      24     1416      70     1104      64    H      0    21   \n83292    2019      29     1209      83     1104      80    A      0    30   \n84439    2019      64     1261      88     1104      79    H      0    28   \n84693    2019      68     1401      81     1104      80    A      0    28   \n85005    2019      75     1397      71     1104      68    H      0    28   \n85239    2019      82     1124      73     1104      68    H      0    30   \n85554    2019      89     1120      84     1104      63    H      0    30   \n86065    2019      99     1280      81     1104      62    H      0    29   \n86231    2019     103     1196      71     1104      53    A      0    27   \n86394    2019     106     1401      65     1104      56    H      0    22   \n86905    2019     117     1261      74     1104      69    A      0    28   \n87019    2019     120     1120      66     1104      60    A      0    24   \n87176    2019     124     1116      82     1104      70    H      0    29   \n87444    2019     130     1246      73     1104      55    N      0    28   \n\n       WFGA  ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n82041    55  ...     17    17    30   11   20     8   15     7     2   28  \n82277    65  ...     15    14    25   11   28     7   22     4     1   29  \n82443    65  ...     12    14    22    6   25    11   15     5     8   18  \n82577    49  ...     23    22    27   11   17     9    7     6     3   25  \n82969    56  ...     18     9    12   12   20    10   18     7     5   22  \n83490    56  ...     24     6     9   13   25    10   11     6     4   19  \n83709    50  ...     23    14    17    8   25    11   15     2     3   27  \n83824    53  ...     25     5     5   10   24    16   10     8     6   20  \n84091    61  ...     19     5     8   11   20     7   12     6     5   18  \n84260    59  ...     18    14    17    6   26    14   11     7     1   15  \n84781    55  ...     20     5    10    8   21    10   11     1     0   17  \n85085    65  ...     20     7    14   10   22     6   16     1     4   20  \n85405    61  ...     19    12    22   19   24    10   13     8     6   25  \n85752    58  ...     25    10    16   15   23    15   13     3     4   20  \n85875    59  ...     22    17    28    5   26    10   11     6     5   17  \n86515    57  ...     17    11    16    7   29    13   17     3     2   16  \n86690    56  ...     19    18    23   11   26     9    8     7     6   21  \n87366    59  ...     26     2     4    6   24    11    9     5     6   17  \n82427    42  ...     19    11    15    8   19    10    9     3     2   16  \n83107    45  ...     24     9    14   19   27    14   14     3     5   24  \n83292    66  ...     28    18    31    8   33    10   10     5     8   18  \n84439    57  ...     24    22    26   16   20    14   15     6     2   20  \n84693    64  ...     27    18    29   13   32    13    8     1     7   25  \n85005    64  ...     26     8    18   12   29    13   13     3     4   20  \n85239    67  ...     17     5     7    9   22    17   13     3     7   15  \n85554    58  ...     19    13    18    8   21    12   21     4     3   16  \n86065    59  ...     24    11    12    8   18    14   18     3     5   16  \n86231    50  ...     19    10    22    5   18     6   11     4     4   12  \n86394    54  ...     23     4     7    7   24    15   13     4     3   13  \n86905    66  ...     26    13    23   12   28    11   13     1     3   20  \n87019    60  ...     19    11    16    8   26    10   19     7     5   16  \n87176    65  ...     24    10    20   12   24     9   15     4     3   14  \n87444    59  ...     18    15    19   11   21     8   11     7     3   15  \n\n[33 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>...</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>82041</th>\n      <td>2019</td>\n      <td>1</td>\n      <td>1104</td>\n      <td>82</td>\n      <td>1380</td>\n      <td>62</td>\n      <td>H</td>\n      <td>0</td>\n      <td>27</td>\n      <td>55</td>\n      <td>...</td>\n      <td>17</td>\n      <td>17</td>\n      <td>30</td>\n      <td>11</td>\n      <td>20</td>\n      <td>8</td>\n      <td>15</td>\n      <td>7</td>\n      <td>2</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>82277</th>\n      <td>2019</td>\n      <td>6</td>\n      <td>1104</td>\n      <td>81</td>\n      <td>1111</td>\n      <td>73</td>\n      <td>H</td>\n      <td>0</td>\n      <td>25</td>\n      <td>65</td>\n      <td>...</td>\n      <td>15</td>\n      <td>14</td>\n      <td>25</td>\n      <td>11</td>\n      <td>28</td>\n      <td>7</td>\n      <td>22</td>\n      <td>4</td>\n      <td>1</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>82443</th>\n      <td>2019</td>\n      <td>11</td>\n      <td>1104</td>\n      <td>79</td>\n      <td>1123</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>33</td>\n      <td>65</td>\n      <td>...</td>\n      <td>12</td>\n      <td>14</td>\n      <td>22</td>\n      <td>6</td>\n      <td>25</td>\n      <td>11</td>\n      <td>15</td>\n      <td>5</td>\n      <td>8</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>82577</th>\n      <td>2019</td>\n      <td>13</td>\n      <td>1104</td>\n      <td>90</td>\n      <td>1455</td>\n      <td>86</td>\n      <td>N</td>\n      <td>0</td>\n      <td>25</td>\n      <td>49</td>\n      <td>...</td>\n      <td>23</td>\n      <td>22</td>\n      <td>27</td>\n      <td>11</td>\n      <td>17</td>\n      <td>9</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>82969</th>\n      <td>2019</td>\n      <td>21</td>\n      <td>1104</td>\n      <td>78</td>\n      <td>1293</td>\n      <td>72</td>\n      <td>H</td>\n      <td>0</td>\n      <td>25</td>\n      <td>56</td>\n      <td>...</td>\n      <td>18</td>\n      <td>9</td>\n      <td>12</td>\n      <td>12</td>\n      <td>20</td>\n      <td>10</td>\n      <td>18</td>\n      <td>7</td>\n      <td>5</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>83490</th>\n      <td>2019</td>\n      <td>34</td>\n      <td>1104</td>\n      <td>76</td>\n      <td>1112</td>\n      <td>73</td>\n      <td>H</td>\n      <td>0</td>\n      <td>26</td>\n      <td>56</td>\n      <td>...</td>\n      <td>24</td>\n      <td>6</td>\n      <td>9</td>\n      <td>13</td>\n      <td>25</td>\n      <td>10</td>\n      <td>11</td>\n      <td>6</td>\n      <td>4</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>83709</th>\n      <td>2019</td>\n      <td>43</td>\n      <td>1104</td>\n      <td>84</td>\n      <td>1251</td>\n      <td>75</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>50</td>\n      <td>...</td>\n      <td>23</td>\n      <td>14</td>\n      <td>17</td>\n      <td>8</td>\n      <td>25</td>\n      <td>11</td>\n      <td>15</td>\n      <td>2</td>\n      <td>3</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>83824</th>\n      <td>2019</td>\n      <td>46</td>\n      <td>1104</td>\n      <td>73</td>\n      <td>1336</td>\n      <td>64</td>\n      <td>H</td>\n      <td>0</td>\n      <td>28</td>\n      <td>53</td>\n      <td>...</td>\n      <td>25</td>\n      <td>5</td>\n      <td>5</td>\n      <td>10</td>\n      <td>24</td>\n      <td>16</td>\n      <td>10</td>\n      <td>8</td>\n      <td>6</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>84091</th>\n      <td>2019</td>\n      <td>55</td>\n      <td>1104</td>\n      <td>79</td>\n      <td>1372</td>\n      <td>69</td>\n      <td>A</td>\n      <td>0</td>\n      <td>27</td>\n      <td>61</td>\n      <td>...</td>\n      <td>19</td>\n      <td>5</td>\n      <td>8</td>\n      <td>11</td>\n      <td>20</td>\n      <td>7</td>\n      <td>12</td>\n      <td>6</td>\n      <td>5</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>84260</th>\n      <td>2019</td>\n      <td>61</td>\n      <td>1104</td>\n      <td>77</td>\n      <td>1246</td>\n      <td>75</td>\n      <td>H</td>\n      <td>0</td>\n      <td>27</td>\n      <td>59</td>\n      <td>...</td>\n      <td>18</td>\n      <td>14</td>\n      <td>17</td>\n      <td>6</td>\n      <td>26</td>\n      <td>14</td>\n      <td>11</td>\n      <td>7</td>\n      <td>1</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>84781</th>\n      <td>2019</td>\n      <td>72</td>\n      <td>1104</td>\n      <td>70</td>\n      <td>1281</td>\n      <td>60</td>\n      <td>A</td>\n      <td>0</td>\n      <td>26</td>\n      <td>55</td>\n      <td>...</td>\n      <td>20</td>\n      <td>5</td>\n      <td>10</td>\n      <td>8</td>\n      <td>21</td>\n      <td>10</td>\n      <td>11</td>\n      <td>1</td>\n      <td>0</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>85085</th>\n      <td>2019</td>\n      <td>78</td>\n      <td>1104</td>\n      <td>74</td>\n      <td>1279</td>\n      <td>53</td>\n      <td>H</td>\n      <td>0</td>\n      <td>25</td>\n      <td>65</td>\n      <td>...</td>\n      <td>20</td>\n      <td>7</td>\n      <td>14</td>\n      <td>10</td>\n      <td>22</td>\n      <td>6</td>\n      <td>16</td>\n      <td>1</td>\n      <td>4</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>85405</th>\n      <td>2019</td>\n      <td>85</td>\n      <td>1104</td>\n      <td>83</td>\n      <td>1280</td>\n      <td>79</td>\n      <td>H</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>...</td>\n      <td>19</td>\n      <td>12</td>\n      <td>22</td>\n      <td>19</td>\n      <td>24</td>\n      <td>10</td>\n      <td>13</td>\n      <td>8</td>\n      <td>6</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>85752</th>\n      <td>2019</td>\n      <td>93</td>\n      <td>1104</td>\n      <td>89</td>\n      <td>1208</td>\n      <td>74</td>\n      <td>H</td>\n      <td>0</td>\n      <td>33</td>\n      <td>58</td>\n      <td>...</td>\n      <td>25</td>\n      <td>10</td>\n      <td>16</td>\n      <td>15</td>\n      <td>23</td>\n      <td>15</td>\n      <td>13</td>\n      <td>3</td>\n      <td>4</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>85875</th>\n      <td>2019</td>\n      <td>96</td>\n      <td>1104</td>\n      <td>77</td>\n      <td>1435</td>\n      <td>67</td>\n      <td>A</td>\n      <td>0</td>\n      <td>24</td>\n      <td>59</td>\n      <td>...</td>\n      <td>22</td>\n      <td>17</td>\n      <td>28</td>\n      <td>5</td>\n      <td>26</td>\n      <td>10</td>\n      <td>11</td>\n      <td>6</td>\n      <td>5</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>86515</th>\n      <td>2019</td>\n      <td>110</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1435</td>\n      <td>61</td>\n      <td>H</td>\n      <td>0</td>\n      <td>24</td>\n      <td>57</td>\n      <td>...</td>\n      <td>17</td>\n      <td>11</td>\n      <td>16</td>\n      <td>7</td>\n      <td>29</td>\n      <td>13</td>\n      <td>17</td>\n      <td>3</td>\n      <td>2</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>86690</th>\n      <td>2019</td>\n      <td>113</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1376</td>\n      <td>62</td>\n      <td>A</td>\n      <td>0</td>\n      <td>24</td>\n      <td>56</td>\n      <td>...</td>\n      <td>19</td>\n      <td>18</td>\n      <td>23</td>\n      <td>11</td>\n      <td>26</td>\n      <td>9</td>\n      <td>8</td>\n      <td>7</td>\n      <td>6</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>87366</th>\n      <td>2019</td>\n      <td>129</td>\n      <td>1104</td>\n      <td>62</td>\n      <td>1279</td>\n      <td>57</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>59</td>\n      <td>...</td>\n      <td>26</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>24</td>\n      <td>11</td>\n      <td>9</td>\n      <td>5</td>\n      <td>6</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>82427</th>\n      <td>2019</td>\n      <td>10</td>\n      <td>1318</td>\n      <td>68</td>\n      <td>1104</td>\n      <td>52</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>42</td>\n      <td>...</td>\n      <td>19</td>\n      <td>11</td>\n      <td>15</td>\n      <td>8</td>\n      <td>19</td>\n      <td>10</td>\n      <td>9</td>\n      <td>3</td>\n      <td>2</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>83107</th>\n      <td>2019</td>\n      <td>24</td>\n      <td>1416</td>\n      <td>70</td>\n      <td>1104</td>\n      <td>64</td>\n      <td>H</td>\n      <td>0</td>\n      <td>21</td>\n      <td>45</td>\n      <td>...</td>\n      <td>24</td>\n      <td>9</td>\n      <td>14</td>\n      <td>19</td>\n      <td>27</td>\n      <td>14</td>\n      <td>14</td>\n      <td>3</td>\n      <td>5</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>83292</th>\n      <td>2019</td>\n      <td>29</td>\n      <td>1209</td>\n      <td>83</td>\n      <td>1104</td>\n      <td>80</td>\n      <td>A</td>\n      <td>0</td>\n      <td>30</td>\n      <td>66</td>\n      <td>...</td>\n      <td>28</td>\n      <td>18</td>\n      <td>31</td>\n      <td>8</td>\n      <td>33</td>\n      <td>10</td>\n      <td>10</td>\n      <td>5</td>\n      <td>8</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>84439</th>\n      <td>2019</td>\n      <td>64</td>\n      <td>1261</td>\n      <td>88</td>\n      <td>1104</td>\n      <td>79</td>\n      <td>H</td>\n      <td>0</td>\n      <td>28</td>\n      <td>57</td>\n      <td>...</td>\n      <td>24</td>\n      <td>22</td>\n      <td>26</td>\n      <td>16</td>\n      <td>20</td>\n      <td>14</td>\n      <td>15</td>\n      <td>6</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>84693</th>\n      <td>2019</td>\n      <td>68</td>\n      <td>1401</td>\n      <td>81</td>\n      <td>1104</td>\n      <td>80</td>\n      <td>A</td>\n      <td>0</td>\n      <td>28</td>\n      <td>64</td>\n      <td>...</td>\n      <td>27</td>\n      <td>18</td>\n      <td>29</td>\n      <td>13</td>\n      <td>32</td>\n      <td>13</td>\n      <td>8</td>\n      <td>1</td>\n      <td>7</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>85005</th>\n      <td>2019</td>\n      <td>75</td>\n      <td>1397</td>\n      <td>71</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>H</td>\n      <td>0</td>\n      <td>28</td>\n      <td>64</td>\n      <td>...</td>\n      <td>26</td>\n      <td>8</td>\n      <td>18</td>\n      <td>12</td>\n      <td>29</td>\n      <td>13</td>\n      <td>13</td>\n      <td>3</td>\n      <td>4</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>85239</th>\n      <td>2019</td>\n      <td>82</td>\n      <td>1124</td>\n      <td>73</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>H</td>\n      <td>0</td>\n      <td>30</td>\n      <td>67</td>\n      <td>...</td>\n      <td>17</td>\n      <td>5</td>\n      <td>7</td>\n      <td>9</td>\n      <td>22</td>\n      <td>17</td>\n      <td>13</td>\n      <td>3</td>\n      <td>7</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>85554</th>\n      <td>2019</td>\n      <td>89</td>\n      <td>1120</td>\n      <td>84</td>\n      <td>1104</td>\n      <td>63</td>\n      <td>H</td>\n      <td>0</td>\n      <td>30</td>\n      <td>58</td>\n      <td>...</td>\n      <td>19</td>\n      <td>13</td>\n      <td>18</td>\n      <td>8</td>\n      <td>21</td>\n      <td>12</td>\n      <td>21</td>\n      <td>4</td>\n      <td>3</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>86065</th>\n      <td>2019</td>\n      <td>99</td>\n      <td>1280</td>\n      <td>81</td>\n      <td>1104</td>\n      <td>62</td>\n      <td>H</td>\n      <td>0</td>\n      <td>29</td>\n      <td>59</td>\n      <td>...</td>\n      <td>24</td>\n      <td>11</td>\n      <td>12</td>\n      <td>8</td>\n      <td>18</td>\n      <td>14</td>\n      <td>18</td>\n      <td>3</td>\n      <td>5</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>86231</th>\n      <td>2019</td>\n      <td>103</td>\n      <td>1196</td>\n      <td>71</td>\n      <td>1104</td>\n      <td>53</td>\n      <td>A</td>\n      <td>0</td>\n      <td>27</td>\n      <td>50</td>\n      <td>...</td>\n      <td>19</td>\n      <td>10</td>\n      <td>22</td>\n      <td>5</td>\n      <td>18</td>\n      <td>6</td>\n      <td>11</td>\n      <td>4</td>\n      <td>4</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>86394</th>\n      <td>2019</td>\n      <td>106</td>\n      <td>1401</td>\n      <td>65</td>\n      <td>1104</td>\n      <td>56</td>\n      <td>H</td>\n      <td>0</td>\n      <td>22</td>\n      <td>54</td>\n      <td>...</td>\n      <td>23</td>\n      <td>4</td>\n      <td>7</td>\n      <td>7</td>\n      <td>24</td>\n      <td>15</td>\n      <td>13</td>\n      <td>4</td>\n      <td>3</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>86905</th>\n      <td>2019</td>\n      <td>117</td>\n      <td>1261</td>\n      <td>74</td>\n      <td>1104</td>\n      <td>69</td>\n      <td>A</td>\n      <td>0</td>\n      <td>28</td>\n      <td>66</td>\n      <td>...</td>\n      <td>26</td>\n      <td>13</td>\n      <td>23</td>\n      <td>12</td>\n      <td>28</td>\n      <td>11</td>\n      <td>13</td>\n      <td>1</td>\n      <td>3</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>87019</th>\n      <td>2019</td>\n      <td>120</td>\n      <td>1120</td>\n      <td>66</td>\n      <td>1104</td>\n      <td>60</td>\n      <td>A</td>\n      <td>0</td>\n      <td>24</td>\n      <td>60</td>\n      <td>...</td>\n      <td>19</td>\n      <td>11</td>\n      <td>16</td>\n      <td>8</td>\n      <td>26</td>\n      <td>10</td>\n      <td>19</td>\n      <td>7</td>\n      <td>5</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>87176</th>\n      <td>2019</td>\n      <td>124</td>\n      <td>1116</td>\n      <td>82</td>\n      <td>1104</td>\n      <td>70</td>\n      <td>H</td>\n      <td>0</td>\n      <td>29</td>\n      <td>65</td>\n      <td>...</td>\n      <td>24</td>\n      <td>10</td>\n      <td>20</td>\n      <td>12</td>\n      <td>24</td>\n      <td>9</td>\n      <td>15</td>\n      <td>4</td>\n      <td>3</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>87444</th>\n      <td>2019</td>\n      <td>130</td>\n      <td>1246</td>\n      <td>73</td>\n      <td>1104</td>\n      <td>55</td>\n      <td>N</td>\n      <td>0</td>\n      <td>28</td>\n      <td>59</td>\n      <td>...</td>\n      <td>18</td>\n      <td>15</td>\n      <td>19</td>\n      <td>11</td>\n      <td>21</td>\n      <td>8</td>\n      <td>11</td>\n      <td>7</td>\n      <td>3</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n<p>33 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_team_stats(team_id, season):\n",
    "    team_stats = regularseasondetailed.loc[(regularseasondetailed[\"WTeamID\"] == team_id) & (regularseasondetailed[\"Season\"] == season)]\n",
    "    team_stats = team_stats.append(regularseasondetailed.loc[(regularseasondetailed[\"LTeamID\"] == team_id) & (regularseasondetailed[\"Season\"] == season)])\n",
    "    return team_stats\n",
    "\n",
    "get_team_stats(1104, 2019)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "        Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  \\\n0         2003      10     1104      68     1328      62    N      0    27   \n1         2003      10     1272      70     1393      63    N      0    26   \n2         2003      11     1266      73     1437      61    N      0    24   \n3         2003      11     1296      56     1457      50    N      0    18   \n4         2003      11     1400      77     1208      71    N      0    30   \n...        ...     ...      ...     ...      ...     ...  ...    ...   ...   \n107472    2023     127     1439      67     1323      64    N      0    24   \n107473    2023     127     1465      69     1101      62    N      0    22   \n107474    2023     127     1467      67     1192      66    H      0    24   \n107475    2023     127     1469      80     1372      76    N      1    27   \n107476    2023     127     1470      74     1410      70    N      0    24   \n\n        WFGA  ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n0         58  ...     10    16    22   10   22     8   18     9     2   20  \n1         62  ...     24     9    20   20   25     7   12     8     6   16  \n2         58  ...     26    14    23   31   22     9   12     2     5   23  \n3         38  ...     22     8    15   17   20     9   19     4     3   23  \n4         61  ...     16    17    27   21   15    12   10     7     1   14  \n...      ...  ...    ...   ...   ...  ...  ...   ...  ...   ...   ...  ...  \n107472    50  ...     21     8    10    3   24     8    7     6     2   18  \n107473    47  ...     20     6     9   12   24    16   12     6     0   25  \n107474    58  ...     15    12    21   13   24    11    9     1     2   21  \n107475    55  ...     19    13    18    8   20    19    8    10     0   23  \n107476    52  ...     32     9    16    5   18    15   16     3     3   24  \n\n[107477 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>...</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>58</td>\n      <td>...</td>\n      <td>10</td>\n      <td>16</td>\n      <td>22</td>\n      <td>10</td>\n      <td>22</td>\n      <td>8</td>\n      <td>18</td>\n      <td>9</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>62</td>\n      <td>...</td>\n      <td>24</td>\n      <td>9</td>\n      <td>20</td>\n      <td>20</td>\n      <td>25</td>\n      <td>7</td>\n      <td>12</td>\n      <td>8</td>\n      <td>6</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>26</td>\n      <td>14</td>\n      <td>23</td>\n      <td>31</td>\n      <td>22</td>\n      <td>9</td>\n      <td>12</td>\n      <td>2</td>\n      <td>5</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>38</td>\n      <td>...</td>\n      <td>22</td>\n      <td>8</td>\n      <td>15</td>\n      <td>17</td>\n      <td>20</td>\n      <td>9</td>\n      <td>19</td>\n      <td>4</td>\n      <td>3</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>...</td>\n      <td>16</td>\n      <td>17</td>\n      <td>27</td>\n      <td>21</td>\n      <td>15</td>\n      <td>12</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>107472</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1439</td>\n      <td>67</td>\n      <td>1323</td>\n      <td>64</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>50</td>\n      <td>...</td>\n      <td>21</td>\n      <td>8</td>\n      <td>10</td>\n      <td>3</td>\n      <td>24</td>\n      <td>8</td>\n      <td>7</td>\n      <td>6</td>\n      <td>2</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>107473</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1465</td>\n      <td>69</td>\n      <td>1101</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>47</td>\n      <td>...</td>\n      <td>20</td>\n      <td>6</td>\n      <td>9</td>\n      <td>12</td>\n      <td>24</td>\n      <td>16</td>\n      <td>12</td>\n      <td>6</td>\n      <td>0</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>107474</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1467</td>\n      <td>67</td>\n      <td>1192</td>\n      <td>66</td>\n      <td>H</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>15</td>\n      <td>12</td>\n      <td>21</td>\n      <td>13</td>\n      <td>24</td>\n      <td>11</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>107475</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1469</td>\n      <td>80</td>\n      <td>1372</td>\n      <td>76</td>\n      <td>N</td>\n      <td>1</td>\n      <td>27</td>\n      <td>55</td>\n      <td>...</td>\n      <td>19</td>\n      <td>13</td>\n      <td>18</td>\n      <td>8</td>\n      <td>20</td>\n      <td>19</td>\n      <td>8</td>\n      <td>10</td>\n      <td>0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>107476</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1470</td>\n      <td>74</td>\n      <td>1410</td>\n      <td>70</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>52</td>\n      <td>...</td>\n      <td>32</td>\n      <td>9</td>\n      <td>16</td>\n      <td>5</td>\n      <td>18</td>\n      <td>15</td>\n      <td>16</td>\n      <td>3</td>\n      <td>3</td>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n<p>107477 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularseasondetailed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "      Season  Seed  TeamID\n0       1985   W01    1207\n1       1985   W02    1210\n2       1985   W03    1228\n3       1985   W04    1260\n4       1985   W05    1374\n...      ...   ...     ...\n2417    2022   Z13    1151\n2418    2022   Z14    1255\n2419    2022   Z15    1174\n2420    2022  Z16a    1136\n2421    2022  Z16b    1460\n\n[2422 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>Seed</th>\n      <th>TeamID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1985</td>\n      <td>W01</td>\n      <td>1207</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1985</td>\n      <td>W02</td>\n      <td>1210</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1985</td>\n      <td>W03</td>\n      <td>1228</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1985</td>\n      <td>W04</td>\n      <td>1260</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1985</td>\n      <td>W05</td>\n      <td>1374</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2417</th>\n      <td>2022</td>\n      <td>Z13</td>\n      <td>1151</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>2022</td>\n      <td>Z14</td>\n      <td>1255</td>\n    </tr>\n    <tr>\n      <th>2419</th>\n      <td>2022</td>\n      <td>Z15</td>\n      <td>1174</td>\n    </tr>\n    <tr>\n      <th>2420</th>\n      <td>2022</td>\n      <td>Z16a</td>\n      <td>1136</td>\n    </tr>\n    <tr>\n      <th>2421</th>\n      <td>2022</td>\n      <td>Z16b</td>\n      <td>1460</td>\n    </tr>\n  </tbody>\n</table>\n<p>2422 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tourneyseeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def win_loss(team, year, dataset):\n",
    "    wins = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    losses = dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    return len(wins), len(losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def free_throw_percentage(team, year, dataset):\n",
    "    wins = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    losses = dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    totalft = wins[\"WFTM\"].sum() + losses[\"LFTM\"].sum()\n",
    "    totalftattempted = wins[\"WFTA\"].sum() + losses[\"LFTA\"].sum()\n",
    "    return totalft / totalftattempted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def total_rebound_percentage(teams, year, dataset):\n",
    "    totalrebounds = 0\n",
    "    totalopprebounds = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == teams) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == teams) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == teams)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == teams)]\n",
    "    totalrebounds += wins[\"WOR\"].sum()\n",
    "    totalrebounds += wins[\"WDR\"].sum()\n",
    "    totalopprebounds += wins[\"LOR\"].sum()\n",
    "    totalopprebounds += wins[\"LDR\"].sum()\n",
    "    totalrebounds += losses[\"LOR\"].sum()\n",
    "    totalrebounds += losses[\"LDR\"].sum()\n",
    "    totalopprebounds += losses[\"WOR\"].sum()\n",
    "    totalopprebounds += losses[\"WDR\"].sum()\n",
    "    return totalrebounds / (totalrebounds + totalopprebounds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def three_point_percentage(teams, year, dataset):\n",
    "    totalthrees = 0\n",
    "    totalthreesattempted = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == teams) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == teams) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == teams)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == teams)]\n",
    "    totalthrees += wins[\"WFGM3\"].sum()\n",
    "    totalthreesattempted += wins[\"WFGA3\"].sum()\n",
    "    totalthrees += losses[\"LFGM3\"].sum()\n",
    "    totalthreesattempted += losses[\"LFGA3\"].sum()\n",
    "    return totalthrees / totalthreesattempted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def field_goals_percentage(teams, year, dataset):\n",
    "    totalfg = 0\n",
    "    totalfgattempted = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == teams) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == teams) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == teams)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == teams)]\n",
    "    totalfg += wins[\"WFGM\"].sum()\n",
    "    totalfgattempted += wins[\"WFGA\"].sum()\n",
    "    totalfg += losses[\"LFGM\"].sum()\n",
    "    totalfgattempted += losses[\"LFGA\"].sum()\n",
    "    return totalfg / totalfgattempted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def turnover_percentage(teams, year, dataset):\n",
    "    totalturnovers = 0\n",
    "    totalopppossessions = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == teams) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == teams) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == teams)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == teams)]\n",
    "    totalturnovers += wins[\"WTO\"].sum()\n",
    "    totalturnovers += losses[\"LTO\"].sum()\n",
    "    totalopppossessions += wins[\"LFGA\"].sum()\n",
    "    totalopppossessions += wins[\"LFGM3\"].sum()\n",
    "    totalopppossessions += wins[\"LFTA\"].sum()\n",
    "    totalopppossessions += losses[\"WFGA\"].sum()\n",
    "    totalopppossessions += losses[\"WFGM3\"].sum()\n",
    "    totalopppossessions += losses[\"WFTA\"].sum()\n",
    "    return totalturnovers / totalopppossessions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def steals_blocks_percentage(team, years, dataset):\n",
    "    totalsteals = 0\n",
    "    totalblocks = 0\n",
    "    totalopppossessions = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == years)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == years)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    totalsteals += wins[\"WStl\"].sum()\n",
    "    totalsteals += losses[\"LStl\"].sum()\n",
    "    totalblocks += wins[\"WBlk\"].sum()\n",
    "    totalblocks += losses[\"LBlk\"].sum()\n",
    "    totalopppossessions += wins[\"LFGA\"].sum()\n",
    "    totalopppossessions += wins[\"LFGM3\"].sum()\n",
    "    totalopppossessions += wins[\"LFTA\"].sum()\n",
    "    totalopppossessions += losses[\"WFGA\"].sum()\n",
    "    totalopppossessions += losses[\"WFGM3\"].sum()\n",
    "    totalopppossessions += losses[\"WFTA\"].sum()\n",
    "    return (totalsteals + totalblocks) / totalopppossessions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def offensive_rebound_percentage(team, years, dataset):\n",
    "    totalrebounds = 0\n",
    "    totalopprebounds = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == years)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == years)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    totalrebounds += wins[\"WOR\"].sum()\n",
    "    totalrebounds += wins[\"WDR\"].sum()\n",
    "    totalopprebounds += wins[\"LOR\"].sum()\n",
    "    totalopprebounds += wins[\"LDR\"].sum()\n",
    "    totalrebounds += losses[\"LOR\"].sum()\n",
    "    totalrebounds += losses[\"LDR\"].sum()\n",
    "    totalopprebounds += losses[\"WOR\"].sum()\n",
    "    totalopprebounds += losses[\"WDR\"].sum()\n",
    "    return totalrebounds / (totalrebounds + totalopprebounds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "      Season  Seed  TeamID\n0       1985   W01    1207\n1       1985   W02    1210\n2       1985   W03    1228\n3       1985   W04    1260\n4       1985   W05    1374\n...      ...   ...     ...\n2417    2022   Z13    1151\n2418    2022   Z14    1255\n2419    2022   Z15    1174\n2420    2022  Z16a    1136\n2421    2022  Z16b    1460\n\n[2422 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>Seed</th>\n      <th>TeamID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1985</td>\n      <td>W01</td>\n      <td>1207</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1985</td>\n      <td>W02</td>\n      <td>1210</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1985</td>\n      <td>W03</td>\n      <td>1228</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1985</td>\n      <td>W04</td>\n      <td>1260</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1985</td>\n      <td>W05</td>\n      <td>1374</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2417</th>\n      <td>2022</td>\n      <td>Z13</td>\n      <td>1151</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>2022</td>\n      <td>Z14</td>\n      <td>1255</td>\n    </tr>\n    <tr>\n      <th>2419</th>\n      <td>2022</td>\n      <td>Z15</td>\n      <td>1174</td>\n    </tr>\n    <tr>\n      <th>2420</th>\n      <td>2022</td>\n      <td>Z16a</td>\n      <td>1136</td>\n    </tr>\n    <tr>\n      <th>2421</th>\n      <td>2022</td>\n      <td>Z16b</td>\n      <td>1460</td>\n    </tr>\n  </tbody>\n</table>\n<p>2422 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tourneyseeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X09\n"
     ]
    },
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def seed(team, year):\n",
    "    try:\n",
    "        seedgrab = tourneyseeds.loc[(tourneyseeds[\"TeamID\"] == team) & (tourneyseeds[\"Season\"] == year)]\n",
    "        seedgrab = seedgrab[[\"Seed\"]].values[0][0]\n",
    "        seedgrab = str(seedgrab)\n",
    "        print(seedgrab)\n",
    "    except:\n",
    "        return 18\n",
    "    return int(re.search(r'\\d+', seedgrab).group())\n",
    "\n",
    "seed(1104, 2012)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def offensive_rating(team, year, dataset):\n",
    "    totalpoints = 0\n",
    "    totalpossessions = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    totalpoints += wins[\"WScore\"].sum()\n",
    "    totalpoints += losses[\"LScore\"].sum()\n",
    "    totalpossessions += wins[\"WFGA\"].sum()\n",
    "    totalpossessions += wins[\"WFGM3\"].sum()\n",
    "    totalpossessions += wins[\"WFTA\"].sum()\n",
    "    totalpossessions += losses[\"LFGA\"].sum()\n",
    "    totalpossessions += losses[\"LFGM3\"].sum()\n",
    "    totalpossessions += losses[\"LFTA\"].sum()\n",
    "    return totalpoints / totalpossessions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def defensive_rating(team, year, dataset):\n",
    "    totalpoints = 0\n",
    "    totalpossessions = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    totalpoints += wins[\"LScore\"].sum()\n",
    "    totalpoints += losses[\"WScore\"].sum()\n",
    "    totalpossessions += wins[\"LFGA\"].sum()\n",
    "    totalpossessions += wins[\"LFGM3\"].sum()\n",
    "    totalpossessions += wins[\"LFTA\"].sum()\n",
    "    totalpossessions += losses[\"WFGA\"].sum()\n",
    "    totalpossessions += losses[\"WFGM3\"].sum()\n",
    "    totalpossessions += losses[\"WFTA\"].sum()\n",
    "    return 100 * totalpoints / totalpossessions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def net_rating(team, year, dataset):\n",
    "    return offensive_rating(team, year, dataset) - defensive_rating(team, year, dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def pace(team, year, dataset):\n",
    "    totalpossessions = 0\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    totalpossessions += wins[\"WFGA\"].sum()\n",
    "    totalpossessions += wins[\"WFGM3\"].sum()\n",
    "    totalpossessions += wins[\"WFTA\"].sum()\n",
    "    totalpossessions += losses[\"LFGA\"].sum()\n",
    "    totalpossessions += losses[\"LFGM3\"].sum()\n",
    "    totalpossessions += losses[\"LFTA\"].sum()\n",
    "    return totalpossessions / (dataset.loc[(dataset[\"Season\"] == year)][\"WFGA\"].sum() + dataset.loc[(dataset[\"Season\"] == year)][\"WFGM3\"].sum() + dataset.loc[(dataset[\"Season\"] == year)][\"WFTA\"].sum() + dataset.loc[(dataset[\"Season\"] == year)][\"LFGA\"].sum() + dataset.loc[(dataset[\"Season\"] == year)][\"LFGM3\"].sum() + dataset.loc[(dataset[\"Season\"] == year)][\"LFTA\"].sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def win_percentage(team, year, dataset):\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    return wins[\"WTeamID\"].count() / (wins[\"WTeamID\"].count() + losses[\"LTeamID\"].count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def team_momentum(team, year, dataset):\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    return wins[\"WScore\"].sum() - losses[\"LScore\"].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def historical_momentum(team, year, dataset):\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] < year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] < year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    return wins[\"WScore\"].sum() - losses[\"LScore\"].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def win_percentage_under_5(team, year, dataset):\n",
    "    team_stats = dataset.loc[(dataset[\"WTeamID\"] == team) & (dataset[\"Season\"] == year)]\n",
    "    team_stats = team_stats.append(dataset.loc[(dataset[\"LTeamID\"] == team) & (dataset[\"Season\"] == year)])\n",
    "    wins = team_stats.loc[(team_stats[\"WTeamID\"] == team)]\n",
    "    losses = team_stats.loc[(team_stats[\"LTeamID\"] == team)]\n",
    "    return (wins.loc[(wins[\"WScore\"] - wins[\"LScore\"] < 5)][\"WTeamID\"].count() / (wins.loc[(wins[\"WScore\"] - wins[\"LScore\"] < 5)][\"WTeamID\"].count() + losses.loc[(losses[\"LScore\"] - losses[\"WScore\"] < 5)][\"LTeamID\"].count())) ** 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "## ToDO add in Tourney Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "%%capture\n",
    "data = []\n",
    "for year in range(2003, 2020):\n",
    "    for team in range(1101, 1478):\n",
    "        data.append([year, team, seed(team, year), offensive_rebound_percentage(team, year, regularseasondetailed), steals_blocks_percentage(team, year, regularseasondetailed), turnover_percentage(team, year, regularseasondetailed), field_goals_percentage(team, year, regularseasondetailed), three_point_percentage(team, year, regularseasondetailed), free_throw_percentage(team, year, regularseasondetailed), offensive_rating(team, year, regularseasondetailed), defensive_rating(team, year, regularseasondetailed), net_rating(team, year, regularseasondetailed), pace(team, year, regularseasondetailed), win_percentage(team, year, regularseasondetailed), team_momentum(team, year, regularseasondetailed), historical_momentum(team, year, regularseasondetailed), win_percentage_under_5(team, year, regularseasondetailed)])\n",
    "for year in range(2021, 2024):\n",
    "    for team in range(1101, 1478):\n",
    "        data.append([year, team, seed(team, year), offensive_rebound_percentage(team, year, regularseasondetailed), steals_blocks_percentage(team, year, regularseasondetailed), turnover_percentage(team, year, regularseasondetailed), field_goals_percentage(team, year, regularseasondetailed), three_point_percentage(team, year, regularseasondetailed), free_throw_percentage(team, year, regularseasondetailed), offensive_rating(team, year, regularseasondetailed), defensive_rating(team, year, regularseasondetailed), net_rating(team, year, regularseasondetailed), pace(team, year, regularseasondetailed), win_percentage(team, year, regularseasondetailed), team_momentum(team, year, regularseasondetailed), historical_momentum(team, year, regularseasondetailed), win_percentage_under_5(team, year, regularseasondetailed)])\n",
    "\n",
    "inputdata = pd.DataFrame(data, columns=[\"Year\", \"Team\", \"Seed\", \"Offensive Rebound Percentage\", \"Steals and Blocks Percentage\", \"Turnover Percentage\", \"Field Goals Percentage\", \"Three Point Percentage\", \"Free Throw Percentage\", \"Offensive Rating\", \"Defensive Rating\", \"Net Rating\", \"Pace\", \"Win Percentage\", \"Team Momentum\", \"Historical Momentum\", \"Win Percentage Under 5\"])\n",
    "tourneydata = []\n",
    "for year in range(2003, 2020):\n",
    "    for team in range(1101, 1478):\n",
    "        tourneydata.append([year, team, seed(team, year), offensive_rebound_percentage(team, year, tourneydetailedresults), steals_blocks_percentage(team, year, tourneydetailedresults), turnover_percentage(team, year, tourneydetailedresults), field_goals_percentage(team, year, tourneydetailedresults), three_point_percentage(team, year, tourneydetailedresults), free_throw_percentage(team, year, tourneydetailedresults), offensive_rating(team, year, tourneydetailedresults), defensive_rating(team, year, tourneydetailedresults), net_rating(team, year, tourneydetailedresults), pace(team, year, tourneydetailedresults), win_percentage(team, year, tourneydetailedresults), team_momentum(team, year, tourneydetailedresults), historical_momentum(team, year, tourneydetailedresults), win_percentage_under_5(team, year, tourneydetailedresults)])\n",
    "for year in range(2021, 2024):\n",
    "    for team in range(1101, 1478):\n",
    "        tourneydata.append([year, team, seed(team, year), offensive_rebound_percentage(team, year, tourneydetailedresults), steals_blocks_percentage(team, year, tourneydetailedresults), turnover_percentage(team, year, tourneydetailedresults), field_goals_percentage(team, year, tourneydetailedresults), three_point_percentage(team, year, tourneydetailedresults), free_throw_percentage(team, year, tourneydetailedresults), offensive_rating(team, year, tourneydetailedresults), defensive_rating(team, year, tourneydetailedresults), net_rating(team, year, tourneydetailedresults), pace(team, year, tourneydetailedresults), win_percentage(team, year, tourneydetailedresults), team_momentum(team, year, tourneydetailedresults), historical_momentum(team, year, tourneydetailedresults), win_percentage_under_5(team, year, tourneydetailedresults)])\n",
    "\n",
    "inputdata1 = pd.DataFrame(tourneydata, columns=[\"Year\", \"Team\", \"Seed\", \"Offensive Rebound Percentage\", \"Steals and Blocks Percentage\", \"Turnover Percentage\", \"Field Goals Percentage\", \"Three Point Percentage\", \"Free Throw Percentage\", \"Offensive Rating\", \"Defensive Rating\", \"Net Rating\", \"Pace\", \"Win Percentage\", \"Team Momentum\", \"Historical Momentum\", \"Win Percentage Under 5\"])\n",
    "\n",
    ";"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "      Year  Team  Seed  Offensive Rebound Percentage  \\\n0     2003  1101    18                           NaN   \n1     2003  1102    18                      0.413793   \n2     2003  1103    18                      0.465738   \n3     2003  1104    10                      0.527903   \n4     2003  1105    18                      0.480565   \n...    ...   ...   ...                           ...   \n7535  2023  1473    18                      0.459733   \n7536  2023  1474    18                      0.513238   \n7537  2023  1475    18                      0.528224   \n7538  2023  1476    18                      0.447640   \n7539  2023  1477    18                      0.478807   \n\n      Steals and Blocks Percentage  Turnover Percentage  \\\n0                              NaN                  NaN   \n1                         0.116667             0.172043   \n2                         0.111782             0.147173   \n3                         0.131555             0.168174   \n4                         0.126875             0.207887   \n...                            ...                  ...   \n7535                      0.106132             0.147013   \n7536                      0.097503             0.141498   \n7537                      0.099923             0.139275   \n7538                      0.117836             0.153908   \n7539                      0.100671             0.148397   \n\n      Field Goals Percentage  Three Point Percentage  Free Throw Percentage  \\\n0                        NaN                     NaN                    NaN   \n1                   0.481149                0.375643               0.651357   \n2                   0.486074                0.338710               0.736390   \n3                   0.420362                0.320144               0.709898   \n4                   0.395755                0.364815               0.705986   \n...                      ...                     ...                    ...   \n7535                0.423402                0.362098               0.731343   \n7536                0.435520                0.347882               0.722397   \n7537                0.426884                0.365918               0.681282   \n7538                0.436745                0.348228               0.758893   \n7539                0.451488                0.319226               0.699805   \n\n      Offensive Rating  Defensive Rating  Net Rating      Pace  \\\n0                  NaN               NaN         NaN  0.000000   \n1             0.884658         85.806452  -84.921794  0.002364   \n2             0.903952         91.066034  -90.162081  0.003070   \n3             0.820296         82.278481  -81.458185  0.003085   \n4             0.788340         85.426489  -84.638150  0.003088   \n...                ...               ...         ...       ...   \n7535          0.837676         86.713836  -85.876160  0.002602   \n7536          0.848859         89.813714  -88.964855  0.002971   \n7537          0.830438         85.879630  -85.049192  0.002944   \n7538          0.860738         84.649299  -83.788561  0.002562   \n7539          0.872636         88.553318  -87.680682  0.002880   \n\n      Win Percentage  Team Momentum  Historical Momentum  \\\n0                NaN              0                    0   \n1           0.428571             47                    0   \n2           0.481481            155                    0   \n3           0.607143            600                    0   \n4           0.269231           -754                    0   \n...              ...            ...                  ...   \n7535        0.275862           -629                    0   \n7536        0.500000            109                    0   \n7537        0.448276           -109                    0   \n7538        0.433333           -144                    0   \n7539        0.375000           -533                    0   \n\n      Win Percentage Under 5  \n0                        NaN  \n1                   0.003460  \n2                   0.069252  \n3                   0.000000  \n4                   0.009070  \n...                      ...  \n7535                0.015625  \n7536                0.081633  \n7537                0.024931  \n7538                0.051653  \n7539                0.067215  \n\n[7540 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Team</th>\n      <th>Seed</th>\n      <th>Offensive Rebound Percentage</th>\n      <th>Steals and Blocks Percentage</th>\n      <th>Turnover Percentage</th>\n      <th>Field Goals Percentage</th>\n      <th>Three Point Percentage</th>\n      <th>Free Throw Percentage</th>\n      <th>Offensive Rating</th>\n      <th>Defensive Rating</th>\n      <th>Net Rating</th>\n      <th>Pace</th>\n      <th>Win Percentage</th>\n      <th>Team Momentum</th>\n      <th>Historical Momentum</th>\n      <th>Win Percentage Under 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>1101</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>1102</td>\n      <td>18</td>\n      <td>0.413793</td>\n      <td>0.116667</td>\n      <td>0.172043</td>\n      <td>0.481149</td>\n      <td>0.375643</td>\n      <td>0.651357</td>\n      <td>0.884658</td>\n      <td>85.806452</td>\n      <td>-84.921794</td>\n      <td>0.002364</td>\n      <td>0.428571</td>\n      <td>47</td>\n      <td>0</td>\n      <td>0.003460</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>1103</td>\n      <td>18</td>\n      <td>0.465738</td>\n      <td>0.111782</td>\n      <td>0.147173</td>\n      <td>0.486074</td>\n      <td>0.338710</td>\n      <td>0.736390</td>\n      <td>0.903952</td>\n      <td>91.066034</td>\n      <td>-90.162081</td>\n      <td>0.003070</td>\n      <td>0.481481</td>\n      <td>155</td>\n      <td>0</td>\n      <td>0.069252</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>1104</td>\n      <td>10</td>\n      <td>0.527903</td>\n      <td>0.131555</td>\n      <td>0.168174</td>\n      <td>0.420362</td>\n      <td>0.320144</td>\n      <td>0.709898</td>\n      <td>0.820296</td>\n      <td>82.278481</td>\n      <td>-81.458185</td>\n      <td>0.003085</td>\n      <td>0.607143</td>\n      <td>600</td>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>1105</td>\n      <td>18</td>\n      <td>0.480565</td>\n      <td>0.126875</td>\n      <td>0.207887</td>\n      <td>0.395755</td>\n      <td>0.364815</td>\n      <td>0.705986</td>\n      <td>0.788340</td>\n      <td>85.426489</td>\n      <td>-84.638150</td>\n      <td>0.003088</td>\n      <td>0.269231</td>\n      <td>-754</td>\n      <td>0</td>\n      <td>0.009070</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7535</th>\n      <td>2023</td>\n      <td>1473</td>\n      <td>18</td>\n      <td>0.459733</td>\n      <td>0.106132</td>\n      <td>0.147013</td>\n      <td>0.423402</td>\n      <td>0.362098</td>\n      <td>0.731343</td>\n      <td>0.837676</td>\n      <td>86.713836</td>\n      <td>-85.876160</td>\n      <td>0.002602</td>\n      <td>0.275862</td>\n      <td>-629</td>\n      <td>0</td>\n      <td>0.015625</td>\n    </tr>\n    <tr>\n      <th>7536</th>\n      <td>2023</td>\n      <td>1474</td>\n      <td>18</td>\n      <td>0.513238</td>\n      <td>0.097503</td>\n      <td>0.141498</td>\n      <td>0.435520</td>\n      <td>0.347882</td>\n      <td>0.722397</td>\n      <td>0.848859</td>\n      <td>89.813714</td>\n      <td>-88.964855</td>\n      <td>0.002971</td>\n      <td>0.500000</td>\n      <td>109</td>\n      <td>0</td>\n      <td>0.081633</td>\n    </tr>\n    <tr>\n      <th>7537</th>\n      <td>2023</td>\n      <td>1475</td>\n      <td>18</td>\n      <td>0.528224</td>\n      <td>0.099923</td>\n      <td>0.139275</td>\n      <td>0.426884</td>\n      <td>0.365918</td>\n      <td>0.681282</td>\n      <td>0.830438</td>\n      <td>85.879630</td>\n      <td>-85.049192</td>\n      <td>0.002944</td>\n      <td>0.448276</td>\n      <td>-109</td>\n      <td>0</td>\n      <td>0.024931</td>\n    </tr>\n    <tr>\n      <th>7538</th>\n      <td>2023</td>\n      <td>1476</td>\n      <td>18</td>\n      <td>0.447640</td>\n      <td>0.117836</td>\n      <td>0.153908</td>\n      <td>0.436745</td>\n      <td>0.348228</td>\n      <td>0.758893</td>\n      <td>0.860738</td>\n      <td>84.649299</td>\n      <td>-83.788561</td>\n      <td>0.002562</td>\n      <td>0.433333</td>\n      <td>-144</td>\n      <td>0</td>\n      <td>0.051653</td>\n    </tr>\n    <tr>\n      <th>7539</th>\n      <td>2023</td>\n      <td>1477</td>\n      <td>18</td>\n      <td>0.478807</td>\n      <td>0.100671</td>\n      <td>0.148397</td>\n      <td>0.451488</td>\n      <td>0.319226</td>\n      <td>0.699805</td>\n      <td>0.872636</td>\n      <td>88.553318</td>\n      <td>-87.680682</td>\n      <td>0.002880</td>\n      <td>0.375000</td>\n      <td>-533</td>\n      <td>0</td>\n      <td>0.067215</td>\n    </tr>\n  </tbody>\n</table>\n<p>7540 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputdata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "      Year  Team  Seed  Offensive Rebound Percentage  \\\n0     2003  1101    18                           NaN   \n1     2003  1102    18                           NaN   \n2     2003  1103    18                           NaN   \n3     2003  1104    10                      0.453125   \n4     2003  1105    18                           NaN   \n...    ...   ...   ...                           ...   \n7535  2023  1473    18                           NaN   \n7536  2023  1474    18                           NaN   \n7537  2023  1475    18                           NaN   \n7538  2023  1476    18                           NaN   \n7539  2023  1477    18                           NaN   \n\n      Steals and Blocks Percentage  Turnover Percentage  \\\n0                              NaN                  NaN   \n1                              NaN                  NaN   \n2                              NaN                  NaN   \n3                         0.097561             0.097561   \n4                              NaN                  NaN   \n...                            ...                  ...   \n7535                           NaN                  NaN   \n7536                           NaN                  NaN   \n7537                           NaN                  NaN   \n7538                           NaN                  NaN   \n7539                           NaN                  NaN   \n\n      Field Goals Percentage  Three Point Percentage  Free Throw Percentage  \\\n0                        NaN                     NaN                    NaN   \n1                        NaN                     NaN                    NaN   \n2                        NaN                     NaN                    NaN   \n3                   0.423077                0.416667                 0.8125   \n4                        NaN                     NaN                    NaN   \n...                      ...                     ...                    ...   \n7535                     NaN                     NaN                    NaN   \n7536                     NaN                     NaN                    NaN   \n7537                     NaN                     NaN                    NaN   \n7538                     NaN                     NaN                    NaN   \n7539                     NaN                     NaN                    NaN   \n\n      Offensive Rating  Defensive Rating  Net Rating     Pace  Win Percentage  \\\n0                  NaN               NaN         NaN  0.00000             NaN   \n1                  NaN               NaN         NaN  0.00000             NaN   \n2                  NaN               NaN         NaN  0.00000             NaN   \n3             0.849315         81.707317  -80.858002  0.00679             0.0   \n4                  NaN               NaN         NaN  0.00000             NaN   \n...                ...               ...         ...      ...             ...   \n7535               NaN               NaN         NaN      NaN             NaN   \n7536               NaN               NaN         NaN      NaN             NaN   \n7537               NaN               NaN         NaN      NaN             NaN   \n7538               NaN               NaN         NaN      NaN             NaN   \n7539               NaN               NaN         NaN      NaN             NaN   \n\n      Team Momentum  Historical Momentum  Win Percentage Under 5  \n0                 0                    0                     NaN  \n1                 0                    0                     NaN  \n2                 0                    0                     NaN  \n3               -62                    0                     0.0  \n4                 0                    0                     NaN  \n...             ...                  ...                     ...  \n7535              0                    0                     NaN  \n7536              0                    0                     NaN  \n7537              0                    0                     NaN  \n7538              0                    0                     NaN  \n7539              0                    0                     NaN  \n\n[7540 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Team</th>\n      <th>Seed</th>\n      <th>Offensive Rebound Percentage</th>\n      <th>Steals and Blocks Percentage</th>\n      <th>Turnover Percentage</th>\n      <th>Field Goals Percentage</th>\n      <th>Three Point Percentage</th>\n      <th>Free Throw Percentage</th>\n      <th>Offensive Rating</th>\n      <th>Defensive Rating</th>\n      <th>Net Rating</th>\n      <th>Pace</th>\n      <th>Win Percentage</th>\n      <th>Team Momentum</th>\n      <th>Historical Momentum</th>\n      <th>Win Percentage Under 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>1101</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>1102</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>1103</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>1104</td>\n      <td>10</td>\n      <td>0.453125</td>\n      <td>0.097561</td>\n      <td>0.097561</td>\n      <td>0.423077</td>\n      <td>0.416667</td>\n      <td>0.8125</td>\n      <td>0.849315</td>\n      <td>81.707317</td>\n      <td>-80.858002</td>\n      <td>0.00679</td>\n      <td>0.0</td>\n      <td>-62</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>1105</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7535</th>\n      <td>2023</td>\n      <td>1473</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7536</th>\n      <td>2023</td>\n      <td>1474</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7537</th>\n      <td>2023</td>\n      <td>1475</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7538</th>\n      <td>2023</td>\n      <td>1476</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7539</th>\n      <td>2023</td>\n      <td>1477</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>7540 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "        Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  \\\n0         2003      10     1104      68     1328      62    N      0    27   \n1         2003      10     1272      70     1393      63    N      0    26   \n2         2003      11     1266      73     1437      61    N      0    24   \n3         2003      11     1296      56     1457      50    N      0    18   \n4         2003      11     1400      77     1208      71    N      0    30   \n...        ...     ...      ...     ...      ...     ...  ...    ...   ...   \n107472    2023     127     1439      67     1323      64    N      0    24   \n107473    2023     127     1465      69     1101      62    N      0    22   \n107474    2023     127     1467      67     1192      66    H      0    24   \n107475    2023     127     1469      80     1372      76    N      1    27   \n107476    2023     127     1470      74     1410      70    N      0    24   \n\n        WFGA  ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n0         58  ...     10    16    22   10   22     8   18     9     2   20  \n1         62  ...     24     9    20   20   25     7   12     8     6   16  \n2         58  ...     26    14    23   31   22     9   12     2     5   23  \n3         38  ...     22     8    15   17   20     9   19     4     3   23  \n4         61  ...     16    17    27   21   15    12   10     7     1   14  \n...      ...  ...    ...   ...   ...  ...  ...   ...  ...   ...   ...  ...  \n107472    50  ...     21     8    10    3   24     8    7     6     2   18  \n107473    47  ...     20     6     9   12   24    16   12     6     0   25  \n107474    58  ...     15    12    21   13   24    11    9     1     2   21  \n107475    55  ...     19    13    18    8   20    19    8    10     0   23  \n107476    52  ...     32     9    16    5   18    15   16     3     3   24  \n\n[107477 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>...</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>58</td>\n      <td>...</td>\n      <td>10</td>\n      <td>16</td>\n      <td>22</td>\n      <td>10</td>\n      <td>22</td>\n      <td>8</td>\n      <td>18</td>\n      <td>9</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>62</td>\n      <td>...</td>\n      <td>24</td>\n      <td>9</td>\n      <td>20</td>\n      <td>20</td>\n      <td>25</td>\n      <td>7</td>\n      <td>12</td>\n      <td>8</td>\n      <td>6</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>26</td>\n      <td>14</td>\n      <td>23</td>\n      <td>31</td>\n      <td>22</td>\n      <td>9</td>\n      <td>12</td>\n      <td>2</td>\n      <td>5</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>38</td>\n      <td>...</td>\n      <td>22</td>\n      <td>8</td>\n      <td>15</td>\n      <td>17</td>\n      <td>20</td>\n      <td>9</td>\n      <td>19</td>\n      <td>4</td>\n      <td>3</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>...</td>\n      <td>16</td>\n      <td>17</td>\n      <td>27</td>\n      <td>21</td>\n      <td>15</td>\n      <td>12</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>107472</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1439</td>\n      <td>67</td>\n      <td>1323</td>\n      <td>64</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>50</td>\n      <td>...</td>\n      <td>21</td>\n      <td>8</td>\n      <td>10</td>\n      <td>3</td>\n      <td>24</td>\n      <td>8</td>\n      <td>7</td>\n      <td>6</td>\n      <td>2</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>107473</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1465</td>\n      <td>69</td>\n      <td>1101</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>47</td>\n      <td>...</td>\n      <td>20</td>\n      <td>6</td>\n      <td>9</td>\n      <td>12</td>\n      <td>24</td>\n      <td>16</td>\n      <td>12</td>\n      <td>6</td>\n      <td>0</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>107474</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1467</td>\n      <td>67</td>\n      <td>1192</td>\n      <td>66</td>\n      <td>H</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>15</td>\n      <td>12</td>\n      <td>21</td>\n      <td>13</td>\n      <td>24</td>\n      <td>11</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>107475</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1469</td>\n      <td>80</td>\n      <td>1372</td>\n      <td>76</td>\n      <td>N</td>\n      <td>1</td>\n      <td>27</td>\n      <td>55</td>\n      <td>...</td>\n      <td>19</td>\n      <td>13</td>\n      <td>18</td>\n      <td>8</td>\n      <td>20</td>\n      <td>19</td>\n      <td>8</td>\n      <td>10</td>\n      <td>0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>107476</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1470</td>\n      <td>74</td>\n      <td>1410</td>\n      <td>70</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>52</td>\n      <td>...</td>\n      <td>32</td>\n      <td>9</td>\n      <td>16</td>\n      <td>5</td>\n      <td>18</td>\n      <td>15</td>\n      <td>16</td>\n      <td>3</td>\n      <td>3</td>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n<p>107477 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularseasondetailed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "     TeamID          TeamName  FirstD1Season  LastD1Season\n0      1101       Abilene Chr           2014          2023\n1      1102         Air Force           1985          2023\n2      1103             Akron           1985          2023\n3      1104           Alabama           1985          2023\n4      1105       Alabama A&M           2000          2023\n..      ...               ...            ...           ...\n372    1473        Lindenwood           2023          2023\n373    1474         Queens NC           2023          2023\n374    1475  Southern Indiana           2023          2023\n375    1476         Stonehill           2023          2023\n376    1477   TX A&M Commerce           2023          2023\n\n[377 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TeamID</th>\n      <th>TeamName</th>\n      <th>FirstD1Season</th>\n      <th>LastD1Season</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1101</td>\n      <td>Abilene Chr</td>\n      <td>2014</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1102</td>\n      <td>Air Force</td>\n      <td>1985</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1103</td>\n      <td>Akron</td>\n      <td>1985</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1104</td>\n      <td>Alabama</td>\n      <td>1985</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1105</td>\n      <td>Alabama A&amp;M</td>\n      <td>2000</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>372</th>\n      <td>1473</td>\n      <td>Lindenwood</td>\n      <td>2023</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>373</th>\n      <td>1474</td>\n      <td>Queens NC</td>\n      <td>2023</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>374</th>\n      <td>1475</td>\n      <td>Southern Indiana</td>\n      <td>2023</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>375</th>\n      <td>1476</td>\n      <td>Stonehill</td>\n      <td>2023</td>\n      <td>2023</td>\n    </tr>\n    <tr>\n      <th>376</th>\n      <td>1477</td>\n      <td>TX A&amp;M Commerce</td>\n      <td>2023</td>\n      <td>2023</td>\n    </tr>\n  </tbody>\n</table>\n<p>377 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "tourneydetailedresults = pd.read_csv(\"data/MNCAATourneyDetailedResults.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "      Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  \\\n0       2003     134     1421      92     1411      84    N      1    32   \n1       2003     136     1112      80     1436      51    N      0    31   \n2       2003     136     1113      84     1272      71    N      0    31   \n3       2003     136     1141      79     1166      73    N      0    29   \n4       2003     136     1143      76     1301      74    N      1    27   \n...      ...     ...      ...     ...      ...     ...  ...    ...   ...   \n1243    2022     146     1242      76     1274      50    N      0    29   \n1244    2022     146     1314      69     1389      49    N      0    25   \n1245    2022     152     1242      81     1437      65    N      0    29   \n1246    2022     152     1314      81     1181      77    N      0    27   \n1247    2022     154     1242      72     1314      69    N      0    29   \n\n      WFGA  ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n0       69  ...     31    14    31   17   28    16   15     5     0   22  \n1       66  ...     16     7     7    8   26    12   17    10     3   15  \n2       59  ...     28    14    21   20   22    11   12     2     5   18  \n3       53  ...     17    12    17   14   17    20   21     6     6   21  \n4       64  ...     21    15    20   10   26    16   14     5     8   19  \n...    ...  ...    ...   ...   ...  ...  ...   ...  ...   ...   ...  ...  \n1243    58  ...     21     9    13    5   21     7   14     7     4   20  \n1244    61  ...     16     9    10    4   25    11    7     4     7   18  \n1245    54  ...     31     8    10   12   17    12    9     3     0   11  \n1246    64  ...     22    12    20   13   25    12    4     7     4   18  \n1247    66  ...     23    18    22   20   29     9   13     2     6   13  \n\n[1248 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>...</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>134</td>\n      <td>1421</td>\n      <td>92</td>\n      <td>1411</td>\n      <td>84</td>\n      <td>N</td>\n      <td>1</td>\n      <td>32</td>\n      <td>69</td>\n      <td>...</td>\n      <td>31</td>\n      <td>14</td>\n      <td>31</td>\n      <td>17</td>\n      <td>28</td>\n      <td>16</td>\n      <td>15</td>\n      <td>5</td>\n      <td>0</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>136</td>\n      <td>1112</td>\n      <td>80</td>\n      <td>1436</td>\n      <td>51</td>\n      <td>N</td>\n      <td>0</td>\n      <td>31</td>\n      <td>66</td>\n      <td>...</td>\n      <td>16</td>\n      <td>7</td>\n      <td>7</td>\n      <td>8</td>\n      <td>26</td>\n      <td>12</td>\n      <td>17</td>\n      <td>10</td>\n      <td>3</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>136</td>\n      <td>1113</td>\n      <td>84</td>\n      <td>1272</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>31</td>\n      <td>59</td>\n      <td>...</td>\n      <td>28</td>\n      <td>14</td>\n      <td>21</td>\n      <td>20</td>\n      <td>22</td>\n      <td>11</td>\n      <td>12</td>\n      <td>2</td>\n      <td>5</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>136</td>\n      <td>1141</td>\n      <td>79</td>\n      <td>1166</td>\n      <td>73</td>\n      <td>N</td>\n      <td>0</td>\n      <td>29</td>\n      <td>53</td>\n      <td>...</td>\n      <td>17</td>\n      <td>12</td>\n      <td>17</td>\n      <td>14</td>\n      <td>17</td>\n      <td>20</td>\n      <td>21</td>\n      <td>6</td>\n      <td>6</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>136</td>\n      <td>1143</td>\n      <td>76</td>\n      <td>1301</td>\n      <td>74</td>\n      <td>N</td>\n      <td>1</td>\n      <td>27</td>\n      <td>64</td>\n      <td>...</td>\n      <td>21</td>\n      <td>15</td>\n      <td>20</td>\n      <td>10</td>\n      <td>26</td>\n      <td>16</td>\n      <td>14</td>\n      <td>5</td>\n      <td>8</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1243</th>\n      <td>2022</td>\n      <td>146</td>\n      <td>1242</td>\n      <td>76</td>\n      <td>1274</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>29</td>\n      <td>58</td>\n      <td>...</td>\n      <td>21</td>\n      <td>9</td>\n      <td>13</td>\n      <td>5</td>\n      <td>21</td>\n      <td>7</td>\n      <td>14</td>\n      <td>7</td>\n      <td>4</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1244</th>\n      <td>2022</td>\n      <td>146</td>\n      <td>1314</td>\n      <td>69</td>\n      <td>1389</td>\n      <td>49</td>\n      <td>N</td>\n      <td>0</td>\n      <td>25</td>\n      <td>61</td>\n      <td>...</td>\n      <td>16</td>\n      <td>9</td>\n      <td>10</td>\n      <td>4</td>\n      <td>25</td>\n      <td>11</td>\n      <td>7</td>\n      <td>4</td>\n      <td>7</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1245</th>\n      <td>2022</td>\n      <td>152</td>\n      <td>1242</td>\n      <td>81</td>\n      <td>1437</td>\n      <td>65</td>\n      <td>N</td>\n      <td>0</td>\n      <td>29</td>\n      <td>54</td>\n      <td>...</td>\n      <td>31</td>\n      <td>8</td>\n      <td>10</td>\n      <td>12</td>\n      <td>17</td>\n      <td>12</td>\n      <td>9</td>\n      <td>3</td>\n      <td>0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1246</th>\n      <td>2022</td>\n      <td>152</td>\n      <td>1314</td>\n      <td>81</td>\n      <td>1181</td>\n      <td>77</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>64</td>\n      <td>...</td>\n      <td>22</td>\n      <td>12</td>\n      <td>20</td>\n      <td>13</td>\n      <td>25</td>\n      <td>12</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1247</th>\n      <td>2022</td>\n      <td>154</td>\n      <td>1242</td>\n      <td>72</td>\n      <td>1314</td>\n      <td>69</td>\n      <td>N</td>\n      <td>0</td>\n      <td>29</td>\n      <td>66</td>\n      <td>...</td>\n      <td>23</td>\n      <td>18</td>\n      <td>22</td>\n      <td>20</td>\n      <td>29</td>\n      <td>9</td>\n      <td>13</td>\n      <td>2</td>\n      <td>6</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>1248 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tourneydetailedresults"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TeamID        TeamName  FirstD1Season  LastD1Season\n",
      "213    1314  North Carolina           1985          2023\n"
     ]
    }
   ],
   "source": [
    "print(teams.loc[teams[\"TeamID\"] == 1314])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "        Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  \\\n0         2003      10     1104      68     1328      62    N      0    27   \n1         2003      10     1272      70     1393      63    N      0    26   \n2         2003      11     1266      73     1437      61    N      0    24   \n3         2003      11     1296      56     1457      50    N      0    18   \n4         2003      11     1400      77     1208      71    N      0    30   \n...        ...     ...      ...     ...      ...     ...  ...    ...   ...   \n107472    2023     127     1439      67     1323      64    N      0    24   \n107473    2023     127     1465      69     1101      62    N      0    22   \n107474    2023     127     1467      67     1192      66    H      0    24   \n107475    2023     127     1469      80     1372      76    N      1    27   \n107476    2023     127     1470      74     1410      70    N      0    24   \n\n        WFGA  ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n0         58  ...     10    16    22   10   22     8   18     9     2   20  \n1         62  ...     24     9    20   20   25     7   12     8     6   16  \n2         58  ...     26    14    23   31   22     9   12     2     5   23  \n3         38  ...     22     8    15   17   20     9   19     4     3   23  \n4         61  ...     16    17    27   21   15    12   10     7     1   14  \n...      ...  ...    ...   ...   ...  ...  ...   ...  ...   ...   ...  ...  \n107472    50  ...     21     8    10    3   24     8    7     6     2   18  \n107473    47  ...     20     6     9   12   24    16   12     6     0   25  \n107474    58  ...     15    12    21   13   24    11    9     1     2   21  \n107475    55  ...     19    13    18    8   20    19    8    10     0   23  \n107476    52  ...     32     9    16    5   18    15   16     3     3   24  \n\n[107477 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>...</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>58</td>\n      <td>...</td>\n      <td>10</td>\n      <td>16</td>\n      <td>22</td>\n      <td>10</td>\n      <td>22</td>\n      <td>8</td>\n      <td>18</td>\n      <td>9</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>62</td>\n      <td>...</td>\n      <td>24</td>\n      <td>9</td>\n      <td>20</td>\n      <td>20</td>\n      <td>25</td>\n      <td>7</td>\n      <td>12</td>\n      <td>8</td>\n      <td>6</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>26</td>\n      <td>14</td>\n      <td>23</td>\n      <td>31</td>\n      <td>22</td>\n      <td>9</td>\n      <td>12</td>\n      <td>2</td>\n      <td>5</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>38</td>\n      <td>...</td>\n      <td>22</td>\n      <td>8</td>\n      <td>15</td>\n      <td>17</td>\n      <td>20</td>\n      <td>9</td>\n      <td>19</td>\n      <td>4</td>\n      <td>3</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>...</td>\n      <td>16</td>\n      <td>17</td>\n      <td>27</td>\n      <td>21</td>\n      <td>15</td>\n      <td>12</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>107472</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1439</td>\n      <td>67</td>\n      <td>1323</td>\n      <td>64</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>50</td>\n      <td>...</td>\n      <td>21</td>\n      <td>8</td>\n      <td>10</td>\n      <td>3</td>\n      <td>24</td>\n      <td>8</td>\n      <td>7</td>\n      <td>6</td>\n      <td>2</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>107473</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1465</td>\n      <td>69</td>\n      <td>1101</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>47</td>\n      <td>...</td>\n      <td>20</td>\n      <td>6</td>\n      <td>9</td>\n      <td>12</td>\n      <td>24</td>\n      <td>16</td>\n      <td>12</td>\n      <td>6</td>\n      <td>0</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>107474</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1467</td>\n      <td>67</td>\n      <td>1192</td>\n      <td>66</td>\n      <td>H</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>...</td>\n      <td>15</td>\n      <td>12</td>\n      <td>21</td>\n      <td>13</td>\n      <td>24</td>\n      <td>11</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>107475</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1469</td>\n      <td>80</td>\n      <td>1372</td>\n      <td>76</td>\n      <td>N</td>\n      <td>1</td>\n      <td>27</td>\n      <td>55</td>\n      <td>...</td>\n      <td>19</td>\n      <td>13</td>\n      <td>18</td>\n      <td>8</td>\n      <td>20</td>\n      <td>19</td>\n      <td>8</td>\n      <td>10</td>\n      <td>0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>107476</th>\n      <td>2023</td>\n      <td>127</td>\n      <td>1470</td>\n      <td>74</td>\n      <td>1410</td>\n      <td>70</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>52</td>\n      <td>...</td>\n      <td>32</td>\n      <td>9</td>\n      <td>16</td>\n      <td>5</td>\n      <td>18</td>\n      <td>15</td>\n      <td>16</td>\n      <td>3</td>\n      <td>3</td>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n<p>107477 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularseasondetailed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import random\n",
    "x_train = pd.DataFrame()\n",
    "y_train = []\n",
    "for year in range(2003, 2020):\n",
    "    for game in range(len(regularseasondetailed.loc[(regularseasondetailed[\"Season\"] == year)])):\n",
    "        ranseed = random.randint(1, 2)\n",
    "        first = \"\"\n",
    "        sec = \"\"\n",
    "        if ranseed == 1:\n",
    "            first = \"WTeamID\"\n",
    "            sec = \"LTeamID\"\n",
    "        else:\n",
    "            first = \"LTeamID\"\n",
    "            sec = \"WTeamID\"\n",
    "        t1 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == regularseasondetailed[first][game])]\n",
    "        t1 = t1.rename(columns={\"Team\": \"Team1\", \"Seed\": \"Seed1\", \"Offensive Rebound Percentage\": \"ORP1\", \"Steals and Blocks Percentage\": \"SBP1\", \"Turnover Percentage\": \"TP1\", \"Field Goals Percentage\": \"FGP1\", \"Three Point Percentage\": \"TPP1\", \"Free Throw Percentage\": \"FTP1\", \"Offensive Rating\": \"OR1\", \"Defensive Rating\": \"DR1\", \"Net Rating\": \"NR1\", \"Pace\": \"Pace1\", \"Win Percentage\": \"WP1\", \"Team Momentum\": \"TM1\", \"Historical Momentum\" : \"HM1\", \"Win Percentage Under 5\": \"WP51\" }, errors=\"raise\")\n",
    "        t2 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == regularseasondetailed[sec][game])]\n",
    "        t2 = t2.rename( columns={\"Team\": \"Team2\", \"Seed\": \"Seed2\", \"Offensive Rebound Percentage\": \"ORP2\", \"Steals and Blocks Percentage\": \"SBP2\", \"Turnover Percentage\": \"TP2\", \"Field Goals Percentage\": \"FGP2\", \"Three Point Percentage\": \"TPP2\", \"Free Throw Percentage\": \"FTP2\", \"Offensive Rating\": \"OR2\", \"Defensive Rating\": \"DR2\", \"Net Rating\": \"NR2\", \"Pace\": \"Pace2\", \"Win Percentage\": \"WP2\", \"Team Momentum\": \"TM2\", \"Historical Momentum\" : \"HM2\", \"Win Percentage Under 5\": \"WP52\"}, errors=\"raise\")\n",
    "        t2 = t2.drop(columns=[\"Year\"])\n",
    "        t1.reset_index(drop=True, inplace=True)\n",
    "        t2.reset_index(drop=True, inplace=True)\n",
    "        temp = pd.concat([t1, t2], axis=1)\n",
    "        x_train = pd.concat([x_train, temp], axis=0, ignore_index=True)\n",
    "        try:\n",
    "            y_train.append(ranseed -1)\n",
    "        except:\n",
    "            y_train = [ranseed -1]\n",
    "    for game in range(len(tourneydetailedresults.loc[(tourneydetailedresults[\"Season\"] == year)])):\n",
    "        ranseed = random.randint(1, 2)\n",
    "        first = \"\"\n",
    "        sec = \"\"\n",
    "        if ranseed == 1:\n",
    "            first = \"WTeamID\"\n",
    "            sec = \"LTeamID\"\n",
    "        else:\n",
    "            first = \"LTeamID\"\n",
    "            sec = \"WTeamID\"\n",
    "    for game in range(len(tourneydetailedresults.loc[(tourneydetailedresults[\"Season\"] == year)])):\n",
    "        ranseed = random.randint(1, 2)\n",
    "        first = \"\"\n",
    "        sec = \"\"\n",
    "        if ranseed == 1:\n",
    "            first = \"WTeamID\"\n",
    "            sec = \"LTeamID\"\n",
    "        else:\n",
    "            first = \"LTeamID\"\n",
    "            sec = \"WTeamID\"\n",
    "        t1 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == tourneydetailedresults[first][game])]\n",
    "        t1 = t1.rename(columns={\"Team\": \"Team1\", \"Seed\": \"Seed1\", \"Offensive Rebound Percentage\": \"ORP1\", \"Steals and Blocks Percentage\": \"SBP1\", \"Turnover Percentage\": \"TP1\", \"Field Goals Percentage\": \"FGP1\", \"Three Point Percentage\": \"TPP1\", \"Free Throw Percentage\": \"FTP1\", \"Offensive Rating\": \"OR1\", \"Defensive Rating\": \"DR1\", \"Net Rating\": \"NR1\", \"Pace\": \"Pace1\", \"Win Percentage\": \"WP1\", \"Team Momentum\": \"TM1\", \"Historical Momentum\" : \"HM1\", \"Win Percentage Under 5\": \"WP51\"}, errors=\"raise\")\n",
    "        t2 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == tourneydetailedresults[sec][game])]\n",
    "        t2 = t2.rename( columns={\"Team\": \"Team2\", \"Seed\": \"Seed2\", \"Offensive Rebound Percentage\": \"ORP2\", \"Steals and Blocks Percentage\": \"SBP2\", \"Turnover Percentage\": \"TP2\", \"Field Goals Percentage\": \"FGP2\", \"Three Point Percentage\": \"TPP2\", \"Free Throw Percentage\": \"FTP2\", \"Offensive Rating\": \"OR2\", \"Defensive Rating\": \"DR2\", \"Net Rating\": \"NR2\", \"Pace\": \"Pace2\", \"Win Percentage\": \"WP2\", \"Team Momentum\": \"TM2\", \"Historical Momentum\" : \"HM2\", \"Win Percentage Under 5\": \"WP52\"}, errors=\"raise\")\n",
    "        t2 = t2.drop(columns=[\"Year\"])\n",
    "        t1.reset_index(drop=True, inplace=True)\n",
    "        t2.reset_index(drop=True, inplace=True)\n",
    "        temp = pd.concat([t1, t2], axis=1)\n",
    "        x_train = pd.concat([x_train, temp], axis=0, ignore_index=True)\n",
    "        try:\n",
    "            y_train.append(ranseed -1)\n",
    "        except:\n",
    "            y_train = [ranseed -1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "x_train_table = pd.DataFrame(x_train)\n",
    "y_train_table = pd.DataFrame(y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "       Year  Team1  Seed1      ORP1      SBP1       TP1      FGP1      TPP1  \\\n0      2003   1328      1  0.530758  0.139991  0.154383  0.446934  0.393673   \n1      2003   1393      3  0.519791  0.172322  0.150591  0.470067  0.330435   \n2      2003   1266      3  0.543319  0.114068  0.160541  0.483810  0.379391   \n3      2003   1296     18  0.532530  0.135409  0.205058  0.458967  0.383104   \n4      2003   1400      1  0.547850  0.118693  0.155500  0.448513  0.348936   \n...     ...    ...    ...       ...       ...       ...       ...       ...   \n88614  2019   1393      8  0.483980  0.158865  0.151493  0.424030  0.329560   \n88615  2019   1242      4  0.517186  0.124407  0.154237  0.462151  0.350427   \n88616  2019   1250     18  0.504797  0.084972  0.147121  0.482254  0.422680   \n88617  2019   1356     18  0.507757  0.133568  0.175480  0.466279  0.373684   \n88618  2019   1436     13  0.529608  0.124480  0.146545  0.452680  0.353107   \n\n           FTP1       OR1  ...      TPP2      FTP2       OR2        DR2  \\\n0      0.707885  0.861582  ...  0.320144  0.709898  0.820296  82.278481   \n1      0.693431  0.879591  ...  0.348797  0.653614  0.828922  77.918367   \n2      0.770045  0.915346  ...  0.349040  0.712575  0.820455  82.405956   \n3      0.652738  0.851618  ...  0.351687  0.635762  0.817494  79.956989   \n4      0.714715  0.856478  ...  0.380252  0.714041  0.882013  84.264392   \n...         ...       ...  ...       ...       ...       ...        ...   \n88614  0.680820  0.821492  ...  0.340426  0.709091  0.847940  83.619345   \n88615  0.697428  0.879245  ...  0.329560  0.680820  0.821492  79.911537   \n88616  0.774194  0.924310  ...  0.363839  0.602105  0.825243  82.688218   \n88617  0.685455  0.883609  ...  0.338663  0.667590  0.836807  84.853949   \n88618  0.753145  0.877652  ...  0.344262  0.680187  0.854137  84.981949   \n\n             NR2     Pace2       WP2   TM2    HM2      WP52  \n0     -81.458185  0.003085  0.607143   600      0  0.000000  \n1     -77.089445  0.003401  0.793103  1281      0  0.111111  \n2     -81.585502  0.003444  0.500000   236      0  0.003906  \n3     -79.139496  0.003102  0.642857   754      0  0.111111  \n4     -83.382379  0.003162  0.703704   986      0  0.074380  \n...          ...       ...       ...   ...    ...       ...  \n88614 -82.771405  0.002898  0.500000   204  14951  0.040000  \n88615 -79.090045  0.003040  0.606061   647  19652  0.017778  \n88616 -81.862975  0.002348  0.344828  -389  -7432  0.009070  \n88617 -84.017142  0.003073  0.545455   411   8096  0.044321  \n88618 -84.127812  0.003096  0.484848   214  16366  0.003086  \n\n[88619 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Team1</th>\n      <th>Seed1</th>\n      <th>ORP1</th>\n      <th>SBP1</th>\n      <th>TP1</th>\n      <th>FGP1</th>\n      <th>TPP1</th>\n      <th>FTP1</th>\n      <th>OR1</th>\n      <th>...</th>\n      <th>TPP2</th>\n      <th>FTP2</th>\n      <th>OR2</th>\n      <th>DR2</th>\n      <th>NR2</th>\n      <th>Pace2</th>\n      <th>WP2</th>\n      <th>TM2</th>\n      <th>HM2</th>\n      <th>WP52</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>1328</td>\n      <td>1</td>\n      <td>0.530758</td>\n      <td>0.139991</td>\n      <td>0.154383</td>\n      <td>0.446934</td>\n      <td>0.393673</td>\n      <td>0.707885</td>\n      <td>0.861582</td>\n      <td>...</td>\n      <td>0.320144</td>\n      <td>0.709898</td>\n      <td>0.820296</td>\n      <td>82.278481</td>\n      <td>-81.458185</td>\n      <td>0.003085</td>\n      <td>0.607143</td>\n      <td>600</td>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>1393</td>\n      <td>3</td>\n      <td>0.519791</td>\n      <td>0.172322</td>\n      <td>0.150591</td>\n      <td>0.470067</td>\n      <td>0.330435</td>\n      <td>0.693431</td>\n      <td>0.879591</td>\n      <td>...</td>\n      <td>0.348797</td>\n      <td>0.653614</td>\n      <td>0.828922</td>\n      <td>77.918367</td>\n      <td>-77.089445</td>\n      <td>0.003401</td>\n      <td>0.793103</td>\n      <td>1281</td>\n      <td>0</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>1266</td>\n      <td>3</td>\n      <td>0.543319</td>\n      <td>0.114068</td>\n      <td>0.160541</td>\n      <td>0.483810</td>\n      <td>0.379391</td>\n      <td>0.770045</td>\n      <td>0.915346</td>\n      <td>...</td>\n      <td>0.349040</td>\n      <td>0.712575</td>\n      <td>0.820455</td>\n      <td>82.405956</td>\n      <td>-81.585502</td>\n      <td>0.003444</td>\n      <td>0.500000</td>\n      <td>236</td>\n      <td>0</td>\n      <td>0.003906</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>1296</td>\n      <td>18</td>\n      <td>0.532530</td>\n      <td>0.135409</td>\n      <td>0.205058</td>\n      <td>0.458967</td>\n      <td>0.383104</td>\n      <td>0.652738</td>\n      <td>0.851618</td>\n      <td>...</td>\n      <td>0.351687</td>\n      <td>0.635762</td>\n      <td>0.817494</td>\n      <td>79.956989</td>\n      <td>-79.139496</td>\n      <td>0.003102</td>\n      <td>0.642857</td>\n      <td>754</td>\n      <td>0</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>1400</td>\n      <td>1</td>\n      <td>0.547850</td>\n      <td>0.118693</td>\n      <td>0.155500</td>\n      <td>0.448513</td>\n      <td>0.348936</td>\n      <td>0.714715</td>\n      <td>0.856478</td>\n      <td>...</td>\n      <td>0.380252</td>\n      <td>0.714041</td>\n      <td>0.882013</td>\n      <td>84.264392</td>\n      <td>-83.382379</td>\n      <td>0.003162</td>\n      <td>0.703704</td>\n      <td>986</td>\n      <td>0</td>\n      <td>0.074380</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88614</th>\n      <td>2019</td>\n      <td>1393</td>\n      <td>8</td>\n      <td>0.483980</td>\n      <td>0.158865</td>\n      <td>0.151493</td>\n      <td>0.424030</td>\n      <td>0.329560</td>\n      <td>0.680820</td>\n      <td>0.821492</td>\n      <td>...</td>\n      <td>0.340426</td>\n      <td>0.709091</td>\n      <td>0.847940</td>\n      <td>83.619345</td>\n      <td>-82.771405</td>\n      <td>0.002898</td>\n      <td>0.500000</td>\n      <td>204</td>\n      <td>14951</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <th>88615</th>\n      <td>2019</td>\n      <td>1242</td>\n      <td>4</td>\n      <td>0.517186</td>\n      <td>0.124407</td>\n      <td>0.154237</td>\n      <td>0.462151</td>\n      <td>0.350427</td>\n      <td>0.697428</td>\n      <td>0.879245</td>\n      <td>...</td>\n      <td>0.329560</td>\n      <td>0.680820</td>\n      <td>0.821492</td>\n      <td>79.911537</td>\n      <td>-79.090045</td>\n      <td>0.003040</td>\n      <td>0.606061</td>\n      <td>647</td>\n      <td>19652</td>\n      <td>0.017778</td>\n    </tr>\n    <tr>\n      <th>88616</th>\n      <td>2019</td>\n      <td>1250</td>\n      <td>18</td>\n      <td>0.504797</td>\n      <td>0.084972</td>\n      <td>0.147121</td>\n      <td>0.482254</td>\n      <td>0.422680</td>\n      <td>0.774194</td>\n      <td>0.924310</td>\n      <td>...</td>\n      <td>0.363839</td>\n      <td>0.602105</td>\n      <td>0.825243</td>\n      <td>82.688218</td>\n      <td>-81.862975</td>\n      <td>0.002348</td>\n      <td>0.344828</td>\n      <td>-389</td>\n      <td>-7432</td>\n      <td>0.009070</td>\n    </tr>\n    <tr>\n      <th>88617</th>\n      <td>2019</td>\n      <td>1356</td>\n      <td>18</td>\n      <td>0.507757</td>\n      <td>0.133568</td>\n      <td>0.175480</td>\n      <td>0.466279</td>\n      <td>0.373684</td>\n      <td>0.685455</td>\n      <td>0.883609</td>\n      <td>...</td>\n      <td>0.338663</td>\n      <td>0.667590</td>\n      <td>0.836807</td>\n      <td>84.853949</td>\n      <td>-84.017142</td>\n      <td>0.003073</td>\n      <td>0.545455</td>\n      <td>411</td>\n      <td>8096</td>\n      <td>0.044321</td>\n    </tr>\n    <tr>\n      <th>88618</th>\n      <td>2019</td>\n      <td>1436</td>\n      <td>13</td>\n      <td>0.529608</td>\n      <td>0.124480</td>\n      <td>0.146545</td>\n      <td>0.452680</td>\n      <td>0.353107</td>\n      <td>0.753145</td>\n      <td>0.877652</td>\n      <td>...</td>\n      <td>0.344262</td>\n      <td>0.680187</td>\n      <td>0.854137</td>\n      <td>84.981949</td>\n      <td>-84.127812</td>\n      <td>0.003096</td>\n      <td>0.484848</td>\n      <td>214</td>\n      <td>16366</td>\n      <td>0.003086</td>\n    </tr>\n  </tbody>\n</table>\n<p>88619 rows × 33 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "       0\n0      1\n1      1\n2      0\n3      0\n4      0\n...   ..\n88614  0\n88615  1\n88616  1\n88617  1\n88618  1\n\n[88619 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88614</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>88615</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>88616</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>88617</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>88618</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>88619 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "x_valid = pd.DataFrame()\n",
    "y_valid = []\n",
    "for year in [2021, 2022]:\n",
    "    for game in range(len(regularseasondetailed.loc[(regularseasondetailed[\"Season\"] == year)])):\n",
    "        ranseed = random.randint(1, 2)\n",
    "        first = \"\"\n",
    "        sec = \"\"\n",
    "        if ranseed == 1:\n",
    "            first = \"WTeamID\"\n",
    "            sec = \"LTeamID\"\n",
    "        else:\n",
    "            first = \"LTeamID\"\n",
    "            sec = \"WTeamID\"\n",
    "        t1 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == regularseasondetailed[first][game])]\n",
    "        t1 = t1.rename(columns={\"Team\": \"Team1\", \"Seed\": \"Seed1\", \"Offensive Rebound Percentage\": \"ORP1\", \"Steals and Blocks Percentage\": \"SBP1\", \"Turnover Percentage\": \"TP1\", \"Field Goals Percentage\": \"FGP1\", \"Three Point Percentage\": \"TPP1\", \"Free Throw Percentage\": \"FTP1\", \"Offensive Rating\": \"OR1\", \"Defensive Rating\": \"DR1\", \"Net Rating\": \"NR1\", \"Pace\": \"Pace1\", \"Win Percentage\": \"WP1\", \"Team Momentum\": \"TM1\", \"Historical Momentum\" : \"HM1\", \"Win Percentage Under 5\": \"WP51\"}, errors=\"raise\")\n",
    "        t2 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == regularseasondetailed[sec][game])]\n",
    "        t2 = t2.rename( columns={\"Team\": \"Team2\", \"Seed\": \"Seed2\", \"Offensive Rebound Percentage\": \"ORP2\", \"Steals and Blocks Percentage\": \"SBP2\", \"Turnover Percentage\": \"TP2\", \"Field Goals Percentage\": \"FGP2\", \"Three Point Percentage\": \"TPP2\", \"Free Throw Percentage\": \"FTP2\", \"Offensive Rating\": \"OR2\", \"Defensive Rating\": \"DR2\", \"Net Rating\": \"NR2\", \"Pace\": \"Pace2\", \"Win Percentage\": \"WP2\", \"Team Momentum\": \"TM2\", \"Historical Momentum\" : \"HM2\", \"Win Percentage Under 5\": \"WP52\"}, errors=\"raise\")\n",
    "        t2 = t2.drop(columns=[\"Year\"])\n",
    "        t1.reset_index(drop=True, inplace=True)\n",
    "        t2.reset_index(drop=True, inplace=True)\n",
    "        temp = pd.concat([t1, t2], axis=1)\n",
    "        x_valid = pd.concat([x_valid, temp], axis=0, ignore_index=True)\n",
    "        try:\n",
    "            y_valid.append(ranseed - 1)\n",
    "        except:\n",
    "            y_valid = [ranseed - 1]\n",
    "    for game in range(len(tourneydetailedresults.loc[(tourneydetailedresults[\"Season\"] == year )])):\n",
    "        ranseed = random.randint(1,2)\n",
    "        first = \"\"\n",
    "        sec = \"\"\n",
    "        if ranseed == 1:\n",
    "            first = \"WTeamID\"\n",
    "            sec = \"LTeamID\"\n",
    "        else:\n",
    "            first = \"LTeamID\"\n",
    "            sec = \"WTeamID\"\n",
    "        t1 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == tourneydetailedresults[first][game])]\n",
    "        t1 = t1.rename(columns={\"Team\": \"Team1\", \"Seed\": \"Seed1\", \"Offensive Rebound Percentage\": \"ORP1\", \"Steals and Blocks Percentage\": \"SBP1\", \"Turnover Percentage\": \"TP1\", \"Field Goals Percentage\": \"FGP1\", \"Three Point Percentage\": \"TPP1\", \"Free Throw Percentage\": \"FTP1\", \"Offensive Rating\": \"OR1\", \"Defensive Rating\": \"DR1\", \"Net Rating\": \"NR1\", \"Pace\": \"Pace1\", \"Win Percentage\": \"WP1\", \"Team Momentum\": \"TM1\", \"Historical Momentum\" : \"HM1\", \"Win Percentage Under 5\": \"WP51\"}, errors=\"raise\")\n",
    "        t2 = inputdata.loc[(inputdata[\"Year\"] == year) & (inputdata[\"Team\"] == tourneydetailedresults[sec][game])]\n",
    "        t2 = t2.rename( columns={\"Team\": \"Team2\", \"Seed\": \"Seed2\", \"Offensive Rebound Percentage\": \"ORP2\", \"Steals and Blocks Percentage\": \"SBP2\", \"Turnover Percentage\": \"TP2\", \"Field Goals Percentage\": \"FGP2\", \"Three Point Percentage\": \"TPP2\", \"Free Throw Percentage\": \"FTP2\", \"Offensive Rating\": \"OR2\", \"Defensive Rating\": \"DR2\", \"Net Rating\": \"NR2\", \"Pace\": \"Pace2\", \"Win Percentage\": \"WP2\", \"Team Momentum\": \"TM2\", \"Historical Momentum\" : \"HM2\", \"Win Percentage Under 5\": \"WP52\"}, errors=\"raise\")\n",
    "        t2 = t2.drop(columns=[\"Year\"])\n",
    "        t1.reset_index(drop=True, inplace=True)\n",
    "        t2.reset_index(drop=True, inplace=True)\n",
    "        temp = pd.concat([t1, t2], axis=1)\n",
    "        x_valid = pd.concat([x_valid, temp], axis=0, ignore_index=True)\n",
    "        try:\n",
    "            y_valid.append(ranseed - 1)\n",
    "        except:\n",
    "            y_valid = [ranseed - 1]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "x_valid_table = pd.DataFrame(x_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "y_valid_table = pd.DataFrame(y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "      Year  Team1  Seed1      ORP1      SBP1       TP1      FGP1      TPP1  \\\n0     2021   1328      8  0.499702  0.131068  0.127184  0.441661  0.338409   \n1     2021   1393     11  0.489117  0.160151  0.124823  0.441042  0.336650   \n2     2021   1437      5  0.511923  0.087237  0.108899  0.449690  0.352431   \n3     2021   1457     12  0.576163  0.125322  0.169159  0.460697  0.353043   \n4     2021   1208     18  0.498188  0.123878  0.175045  0.455311  0.323232   \n...    ...    ...    ...       ...       ...       ...       ...       ...   \n9328  2022   1393     18  0.490014  0.121733  0.117263  0.450739  0.376412   \n9329  2022   1242      1  0.538949  0.127395  0.145138  0.480988  0.355263   \n9330  2022   1197     18  0.463469  0.105462  0.140756  0.421855  0.297362   \n9331  2022   1356     18  0.490208  0.107700  0.150685  0.441366  0.351310   \n9332  2022   1163      5  0.561272  0.151251  0.138362  0.435533  0.353116   \n\n          FTP1       OR1  ...      TPP2      FTP2       OR2        DR2  \\\n0     0.744344  0.865679  ...  0.350877  0.720217  0.849164  79.491075   \n1     0.784722  0.873733  ...  0.354455  0.620619  0.830533  76.715811   \n2     0.765464  0.882634  ...  0.322523  0.734615  0.862574  82.170881   \n3     0.685824  0.872486  ...  0.323988  0.615385  0.778947  89.531345   \n4     0.686792  0.863921  ...  0.356577  0.707602  0.871269  80.224215   \n...        ...       ...  ...       ...       ...       ...        ...   \n9328  0.736934  0.879132  ...  0.323344  0.748682  0.863403  79.932970   \n9329  0.724398  0.911323  ...  0.376412  0.736934  0.879132  85.178817   \n9330  0.705757  0.822796  ...  0.381818  0.741935  0.878939  86.018590   \n9331  0.690265  0.855735  ...  0.308411  0.729970  0.859013  84.223217   \n9332  0.749585  0.855923  ...  0.367666  0.746835  0.943590  81.734007   \n\n            NR2     Pace2       WP2   TM2    HM2      WP52  \n0    -78.641911  0.004380  0.800000  1621   8781  0.206612  \n1    -75.885278  0.003246  0.666667   714  23535  0.040000  \n2    -81.308306  0.003402  0.481481   111  15598  0.049383  \n3    -88.752398  0.002368  0.157895  -764  -5915  0.003460  \n4    -79.352946  0.003498  0.730769   962  15796  0.284444  \n...         ...       ...       ...   ...    ...       ...  \n9328 -79.069567  0.002880  0.656250   815  16758  0.071111  \n9329 -84.299685  0.003302  0.484848   195  21397  0.022500  \n9330 -85.139652  0.002743  0.387097  -220   5613  0.018595  \n9331 -83.364204  0.003387  0.593750   599  10402  0.055363  \n9332 -80.790417  0.002882  0.843750  1790  18654  0.081633  \n\n[9333 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Team1</th>\n      <th>Seed1</th>\n      <th>ORP1</th>\n      <th>SBP1</th>\n      <th>TP1</th>\n      <th>FGP1</th>\n      <th>TPP1</th>\n      <th>FTP1</th>\n      <th>OR1</th>\n      <th>...</th>\n      <th>TPP2</th>\n      <th>FTP2</th>\n      <th>OR2</th>\n      <th>DR2</th>\n      <th>NR2</th>\n      <th>Pace2</th>\n      <th>WP2</th>\n      <th>TM2</th>\n      <th>HM2</th>\n      <th>WP52</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021</td>\n      <td>1328</td>\n      <td>8</td>\n      <td>0.499702</td>\n      <td>0.131068</td>\n      <td>0.127184</td>\n      <td>0.441661</td>\n      <td>0.338409</td>\n      <td>0.744344</td>\n      <td>0.865679</td>\n      <td>...</td>\n      <td>0.350877</td>\n      <td>0.720217</td>\n      <td>0.849164</td>\n      <td>79.491075</td>\n      <td>-78.641911</td>\n      <td>0.004380</td>\n      <td>0.800000</td>\n      <td>1621</td>\n      <td>8781</td>\n      <td>0.206612</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021</td>\n      <td>1393</td>\n      <td>11</td>\n      <td>0.489117</td>\n      <td>0.160151</td>\n      <td>0.124823</td>\n      <td>0.441042</td>\n      <td>0.336650</td>\n      <td>0.784722</td>\n      <td>0.873733</td>\n      <td>...</td>\n      <td>0.354455</td>\n      <td>0.620619</td>\n      <td>0.830533</td>\n      <td>76.715811</td>\n      <td>-75.885278</td>\n      <td>0.003246</td>\n      <td>0.666667</td>\n      <td>714</td>\n      <td>23535</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021</td>\n      <td>1437</td>\n      <td>5</td>\n      <td>0.511923</td>\n      <td>0.087237</td>\n      <td>0.108899</td>\n      <td>0.449690</td>\n      <td>0.352431</td>\n      <td>0.765464</td>\n      <td>0.882634</td>\n      <td>...</td>\n      <td>0.322523</td>\n      <td>0.734615</td>\n      <td>0.862574</td>\n      <td>82.170881</td>\n      <td>-81.308306</td>\n      <td>0.003402</td>\n      <td>0.481481</td>\n      <td>111</td>\n      <td>15598</td>\n      <td>0.049383</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021</td>\n      <td>1457</td>\n      <td>12</td>\n      <td>0.576163</td>\n      <td>0.125322</td>\n      <td>0.169159</td>\n      <td>0.460697</td>\n      <td>0.353043</td>\n      <td>0.685824</td>\n      <td>0.872486</td>\n      <td>...</td>\n      <td>0.323988</td>\n      <td>0.615385</td>\n      <td>0.778947</td>\n      <td>89.531345</td>\n      <td>-88.752398</td>\n      <td>0.002368</td>\n      <td>0.157895</td>\n      <td>-764</td>\n      <td>-5915</td>\n      <td>0.003460</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021</td>\n      <td>1208</td>\n      <td>18</td>\n      <td>0.498188</td>\n      <td>0.123878</td>\n      <td>0.175045</td>\n      <td>0.455311</td>\n      <td>0.323232</td>\n      <td>0.686792</td>\n      <td>0.863921</td>\n      <td>...</td>\n      <td>0.356577</td>\n      <td>0.707602</td>\n      <td>0.871269</td>\n      <td>80.224215</td>\n      <td>-79.352946</td>\n      <td>0.003498</td>\n      <td>0.730769</td>\n      <td>962</td>\n      <td>15796</td>\n      <td>0.284444</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9328</th>\n      <td>2022</td>\n      <td>1393</td>\n      <td>18</td>\n      <td>0.490014</td>\n      <td>0.121733</td>\n      <td>0.117263</td>\n      <td>0.450739</td>\n      <td>0.376412</td>\n      <td>0.736934</td>\n      <td>0.879132</td>\n      <td>...</td>\n      <td>0.323344</td>\n      <td>0.748682</td>\n      <td>0.863403</td>\n      <td>79.932970</td>\n      <td>-79.069567</td>\n      <td>0.002880</td>\n      <td>0.656250</td>\n      <td>815</td>\n      <td>16758</td>\n      <td>0.071111</td>\n    </tr>\n    <tr>\n      <th>9329</th>\n      <td>2022</td>\n      <td>1242</td>\n      <td>1</td>\n      <td>0.538949</td>\n      <td>0.127395</td>\n      <td>0.145138</td>\n      <td>0.480988</td>\n      <td>0.355263</td>\n      <td>0.724398</td>\n      <td>0.911323</td>\n      <td>...</td>\n      <td>0.376412</td>\n      <td>0.736934</td>\n      <td>0.879132</td>\n      <td>85.178817</td>\n      <td>-84.299685</td>\n      <td>0.003302</td>\n      <td>0.484848</td>\n      <td>195</td>\n      <td>21397</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <th>9330</th>\n      <td>2022</td>\n      <td>1197</td>\n      <td>18</td>\n      <td>0.463469</td>\n      <td>0.105462</td>\n      <td>0.140756</td>\n      <td>0.421855</td>\n      <td>0.297362</td>\n      <td>0.705757</td>\n      <td>0.822796</td>\n      <td>...</td>\n      <td>0.381818</td>\n      <td>0.741935</td>\n      <td>0.878939</td>\n      <td>86.018590</td>\n      <td>-85.139652</td>\n      <td>0.002743</td>\n      <td>0.387097</td>\n      <td>-220</td>\n      <td>5613</td>\n      <td>0.018595</td>\n    </tr>\n    <tr>\n      <th>9331</th>\n      <td>2022</td>\n      <td>1356</td>\n      <td>18</td>\n      <td>0.490208</td>\n      <td>0.107700</td>\n      <td>0.150685</td>\n      <td>0.441366</td>\n      <td>0.351310</td>\n      <td>0.690265</td>\n      <td>0.855735</td>\n      <td>...</td>\n      <td>0.308411</td>\n      <td>0.729970</td>\n      <td>0.859013</td>\n      <td>84.223217</td>\n      <td>-83.364204</td>\n      <td>0.003387</td>\n      <td>0.593750</td>\n      <td>599</td>\n      <td>10402</td>\n      <td>0.055363</td>\n    </tr>\n    <tr>\n      <th>9332</th>\n      <td>2022</td>\n      <td>1163</td>\n      <td>5</td>\n      <td>0.561272</td>\n      <td>0.151251</td>\n      <td>0.138362</td>\n      <td>0.435533</td>\n      <td>0.353116</td>\n      <td>0.749585</td>\n      <td>0.855923</td>\n      <td>...</td>\n      <td>0.367666</td>\n      <td>0.746835</td>\n      <td>0.943590</td>\n      <td>81.734007</td>\n      <td>-80.790417</td>\n      <td>0.002882</td>\n      <td>0.843750</td>\n      <td>1790</td>\n      <td>18654</td>\n      <td>0.081633</td>\n    </tr>\n  </tbody>\n</table>\n<p>9333 rows × 33 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "      0\n0     1\n1     1\n2     1\n3     1\n4     1\n...  ..\n9328  0\n9329  1\n9330  0\n9331  1\n9332  0\n\n[9333 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9328</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9329</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9330</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9331</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9332</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9333 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trying Squaring and Subtracting Teams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "team1 = x_train_table[['Seed1', 'ORP1', 'SBP1', 'TP1', 'FGP1', 'TPP1', 'FTP1', 'OR1', 'DR1', 'NR1', 'Pace1', 'WP1', 'TM1', 'HM1', 'WP51']]\n",
    "team2 = x_train_table[['Seed2', 'ORP2', 'SBP2', 'TP2', 'FGP2', 'TPP2', 'FTP2', 'OR2', 'DR2', 'NR2', 'Pace2', 'WP2', 'TM2', 'HM2', 'WP52']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/5rcwl4d96zvcq7zxv7fzt3m80000gn/T/ipykernel_46974/3407452372.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team1.rename(columns={\"Seed1\": \"SDF\", \"ORP1\": \"ORP\", \"SBP1\": \"SBP\", \"TP1\": \"TP\", \"FGP1\": \"FGP\", \"TPP1\": \"TPP\", \"FTP1\": \"FTP\", \"OR1\": \"OR\", \"DR1\": \"DR\", \"NR1\": \"NR\", \"Pace1\": \"Pace\", \"WP1\": \"WP\", \"TM1\": \"TM\", \"HM1\": \"HM\", \"WP51\": \"WP5\"}, inplace=True)\n",
      "/var/folders/qw/5rcwl4d96zvcq7zxv7fzt3m80000gn/T/ipykernel_46974/3407452372.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team2.rename(columns={\"Seed2\": \"SDF\", \"ORP2\": \"ORP\", \"SBP2\": \"SBP\", \"TP2\": \"TP\", \"FGP2\": \"FGP\", \"TPP2\": \"TPP\", \"FTP2\": \"FTP\", \"OR2\": \"OR\", \"DR2\": \"DR\", \"NR2\": \"NR\", \"Pace2\": \"Pace\", \"WP2\": \"WP\", \"TM2\": \"TM\", \"HM2\": \"HM\", \"WP52\": \"WP5\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "team1.rename(columns={\"Seed1\": \"SDF\", \"ORP1\": \"ORP\", \"SBP1\": \"SBP\", \"TP1\": \"TP\", \"FGP1\": \"FGP\", \"TPP1\": \"TPP\", \"FTP1\": \"FTP\", \"OR1\": \"OR\", \"DR1\": \"DR\", \"NR1\": \"NR\", \"Pace1\": \"Pace\", \"WP1\": \"WP\", \"TM1\": \"TM\", \"HM1\": \"HM\", \"WP51\": \"WP5\"}, inplace=True)\n",
    "team2.rename(columns={\"Seed2\": \"SDF\", \"ORP2\": \"ORP\", \"SBP2\": \"SBP\", \"TP2\": \"TP\", \"FGP2\": \"FGP\", \"TPP2\": \"TPP\", \"FTP2\": \"FTP\", \"OR2\": \"OR\", \"DR2\": \"DR\", \"NR2\": \"NR\", \"Pace2\": \"Pace\", \"WP2\": \"WP\", \"TM2\": \"TM\", \"HM2\": \"HM\", \"WP52\": \"WP5\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "       SDF       ORP       SBP        TP       FGP       TPP       FTP  \\\n0        1  0.530758  0.139991  0.154383  0.446934  0.393673  0.707885   \n1        3  0.519791  0.172322  0.150591  0.470067  0.330435  0.693431   \n2        3  0.543319  0.114068  0.160541  0.483810  0.379391  0.770045   \n3       18  0.532530  0.135409  0.205058  0.458967  0.383104  0.652738   \n4        1  0.547850  0.118693  0.155500  0.448513  0.348936  0.714715   \n...    ...       ...       ...       ...       ...       ...       ...   \n88614    8  0.483980  0.158865  0.151493  0.424030  0.329560  0.680820   \n88615    4  0.517186  0.124407  0.154237  0.462151  0.350427  0.697428   \n88616   18  0.504797  0.084972  0.147121  0.482254  0.422680  0.774194   \n88617   18  0.507757  0.133568  0.175480  0.466279  0.373684  0.685455   \n88618   13  0.529608  0.124480  0.146545  0.452680  0.353107  0.753145   \n\n             OR         DR         NR      Pace        WP    TM     HM  \\\n0      0.861582  78.717837 -77.856255  0.003233  0.800000  1461      0   \n1      0.879591  77.277926 -76.398335  0.003445  0.827586  1691      0   \n2      0.915346  80.059147 -79.143800  0.003128  0.821429  1451      0   \n3      0.851618  84.202335 -83.350717  0.003306  0.548387   316      0   \n4      0.856478  79.528536 -78.672058  0.003363  0.785714  1234      0   \n...         ...        ...        ...       ...       ...   ...    ...   \n88614  0.821492  79.911537 -79.090045  0.003040  0.606061   647  19652   \n88615  0.879245  80.779661 -79.900416  0.003164  0.735294  1385  28579   \n88616  0.924310  84.726124 -83.801813  0.002911  0.645161   895   5788   \n88617  0.883609  83.979632 -83.096023  0.002695  0.531250   408   7079   \n88618  0.877652  83.721898 -82.844247  0.002865  0.812500  1537  15290   \n\n            WP5  \n0      0.111111  \n1      0.250000  \n2      0.081633  \n3      0.031142  \n4      0.020408  \n...         ...  \n88614  0.017778  \n88615  0.094675  \n88616  0.006944  \n88617  0.003906  \n88618  0.020408  \n\n[88619 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SDF</th>\n      <th>ORP</th>\n      <th>SBP</th>\n      <th>TP</th>\n      <th>FGP</th>\n      <th>TPP</th>\n      <th>FTP</th>\n      <th>OR</th>\n      <th>DR</th>\n      <th>NR</th>\n      <th>Pace</th>\n      <th>WP</th>\n      <th>TM</th>\n      <th>HM</th>\n      <th>WP5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.530758</td>\n      <td>0.139991</td>\n      <td>0.154383</td>\n      <td>0.446934</td>\n      <td>0.393673</td>\n      <td>0.707885</td>\n      <td>0.861582</td>\n      <td>78.717837</td>\n      <td>-77.856255</td>\n      <td>0.003233</td>\n      <td>0.800000</td>\n      <td>1461</td>\n      <td>0</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0.519791</td>\n      <td>0.172322</td>\n      <td>0.150591</td>\n      <td>0.470067</td>\n      <td>0.330435</td>\n      <td>0.693431</td>\n      <td>0.879591</td>\n      <td>77.277926</td>\n      <td>-76.398335</td>\n      <td>0.003445</td>\n      <td>0.827586</td>\n      <td>1691</td>\n      <td>0</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.543319</td>\n      <td>0.114068</td>\n      <td>0.160541</td>\n      <td>0.483810</td>\n      <td>0.379391</td>\n      <td>0.770045</td>\n      <td>0.915346</td>\n      <td>80.059147</td>\n      <td>-79.143800</td>\n      <td>0.003128</td>\n      <td>0.821429</td>\n      <td>1451</td>\n      <td>0</td>\n      <td>0.081633</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>0.532530</td>\n      <td>0.135409</td>\n      <td>0.205058</td>\n      <td>0.458967</td>\n      <td>0.383104</td>\n      <td>0.652738</td>\n      <td>0.851618</td>\n      <td>84.202335</td>\n      <td>-83.350717</td>\n      <td>0.003306</td>\n      <td>0.548387</td>\n      <td>316</td>\n      <td>0</td>\n      <td>0.031142</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.547850</td>\n      <td>0.118693</td>\n      <td>0.155500</td>\n      <td>0.448513</td>\n      <td>0.348936</td>\n      <td>0.714715</td>\n      <td>0.856478</td>\n      <td>79.528536</td>\n      <td>-78.672058</td>\n      <td>0.003363</td>\n      <td>0.785714</td>\n      <td>1234</td>\n      <td>0</td>\n      <td>0.020408</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88614</th>\n      <td>8</td>\n      <td>0.483980</td>\n      <td>0.158865</td>\n      <td>0.151493</td>\n      <td>0.424030</td>\n      <td>0.329560</td>\n      <td>0.680820</td>\n      <td>0.821492</td>\n      <td>79.911537</td>\n      <td>-79.090045</td>\n      <td>0.003040</td>\n      <td>0.606061</td>\n      <td>647</td>\n      <td>19652</td>\n      <td>0.017778</td>\n    </tr>\n    <tr>\n      <th>88615</th>\n      <td>4</td>\n      <td>0.517186</td>\n      <td>0.124407</td>\n      <td>0.154237</td>\n      <td>0.462151</td>\n      <td>0.350427</td>\n      <td>0.697428</td>\n      <td>0.879245</td>\n      <td>80.779661</td>\n      <td>-79.900416</td>\n      <td>0.003164</td>\n      <td>0.735294</td>\n      <td>1385</td>\n      <td>28579</td>\n      <td>0.094675</td>\n    </tr>\n    <tr>\n      <th>88616</th>\n      <td>18</td>\n      <td>0.504797</td>\n      <td>0.084972</td>\n      <td>0.147121</td>\n      <td>0.482254</td>\n      <td>0.422680</td>\n      <td>0.774194</td>\n      <td>0.924310</td>\n      <td>84.726124</td>\n      <td>-83.801813</td>\n      <td>0.002911</td>\n      <td>0.645161</td>\n      <td>895</td>\n      <td>5788</td>\n      <td>0.006944</td>\n    </tr>\n    <tr>\n      <th>88617</th>\n      <td>18</td>\n      <td>0.507757</td>\n      <td>0.133568</td>\n      <td>0.175480</td>\n      <td>0.466279</td>\n      <td>0.373684</td>\n      <td>0.685455</td>\n      <td>0.883609</td>\n      <td>83.979632</td>\n      <td>-83.096023</td>\n      <td>0.002695</td>\n      <td>0.531250</td>\n      <td>408</td>\n      <td>7079</td>\n      <td>0.003906</td>\n    </tr>\n    <tr>\n      <th>88618</th>\n      <td>13</td>\n      <td>0.529608</td>\n      <td>0.124480</td>\n      <td>0.146545</td>\n      <td>0.452680</td>\n      <td>0.353107</td>\n      <td>0.753145</td>\n      <td>0.877652</td>\n      <td>83.721898</td>\n      <td>-82.844247</td>\n      <td>0.002865</td>\n      <td>0.812500</td>\n      <td>1537</td>\n      <td>15290</td>\n      <td>0.020408</td>\n    </tr>\n  </tbody>\n</table>\n<p>88619 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "difference = team1.subtract(team2, axis=\"columns\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "       SDF       ORP       SBP        TP       FGP       TPP       FTP  \\\n0       -9  0.002855  0.008436 -0.013791  0.026572  0.073529 -0.002012   \n1       -4 -0.007217  0.024975 -0.012674  0.032135 -0.018362  0.039816   \n2      -15  0.015121 -0.014066 -0.027939  0.063381  0.030351  0.057471   \n3        0  0.032778 -0.021151  0.028714  0.026860  0.031417  0.016976   \n4      -17  0.042850 -0.019473  0.022451 -0.015622 -0.031316  0.000674   \n...    ...       ...       ...       ...       ...       ...       ...   \n88614  -10 -0.010079  0.030940  0.014597 -0.009411 -0.010866 -0.028271   \n88615   -4  0.033206 -0.034458  0.002744  0.038121  0.020868  0.016608   \n88616    0  0.026335 -0.049864 -0.058749  0.044221  0.058841  0.172088   \n88617    0 -0.014128  0.023219  0.013561  0.025448  0.035021  0.017865   \n88618   -5  0.023339 -0.000069 -0.007607  0.006534  0.008845  0.072957   \n\n             OR        DR        NR      Pace        WP    TM     HM       WP5  \n0      0.041286 -3.560644  3.601930  0.000147  0.192857   861      0  0.111111  \n1      0.050669 -0.640441  0.691110  0.000044  0.034483   410      0  0.138889  \n2      0.094892 -2.346810  2.441701 -0.000316  0.321429  1215      0  0.077726  \n3      0.034124  4.245345 -4.211221  0.000204 -0.094470  -438      0 -0.079969  \n4     -0.025535 -4.735856  4.710321  0.000201  0.082011   248      0 -0.053972  \n...         ...       ...       ...       ...       ...   ...    ...       ...  \n88614 -0.026448 -3.707808  3.681360  0.000142  0.106061   443   4701 -0.022222  \n88615  0.057753  0.868124 -0.810371  0.000124  0.129234   738   8927  0.076897  \n88616  0.099067  2.037906 -1.938838  0.000563  0.300334  1284  13220 -0.002126  \n88617  0.046802 -0.874317  0.921119 -0.000378 -0.014205    -3  -1017 -0.040415  \n88618  0.023514 -1.260051  1.283565 -0.000230  0.327652  1323  -1076  0.017322  \n\n[88619 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SDF</th>\n      <th>ORP</th>\n      <th>SBP</th>\n      <th>TP</th>\n      <th>FGP</th>\n      <th>TPP</th>\n      <th>FTP</th>\n      <th>OR</th>\n      <th>DR</th>\n      <th>NR</th>\n      <th>Pace</th>\n      <th>WP</th>\n      <th>TM</th>\n      <th>HM</th>\n      <th>WP5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-9</td>\n      <td>0.002855</td>\n      <td>0.008436</td>\n      <td>-0.013791</td>\n      <td>0.026572</td>\n      <td>0.073529</td>\n      <td>-0.002012</td>\n      <td>0.041286</td>\n      <td>-3.560644</td>\n      <td>3.601930</td>\n      <td>0.000147</td>\n      <td>0.192857</td>\n      <td>861</td>\n      <td>0</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4</td>\n      <td>-0.007217</td>\n      <td>0.024975</td>\n      <td>-0.012674</td>\n      <td>0.032135</td>\n      <td>-0.018362</td>\n      <td>0.039816</td>\n      <td>0.050669</td>\n      <td>-0.640441</td>\n      <td>0.691110</td>\n      <td>0.000044</td>\n      <td>0.034483</td>\n      <td>410</td>\n      <td>0</td>\n      <td>0.138889</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-15</td>\n      <td>0.015121</td>\n      <td>-0.014066</td>\n      <td>-0.027939</td>\n      <td>0.063381</td>\n      <td>0.030351</td>\n      <td>0.057471</td>\n      <td>0.094892</td>\n      <td>-2.346810</td>\n      <td>2.441701</td>\n      <td>-0.000316</td>\n      <td>0.321429</td>\n      <td>1215</td>\n      <td>0</td>\n      <td>0.077726</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.032778</td>\n      <td>-0.021151</td>\n      <td>0.028714</td>\n      <td>0.026860</td>\n      <td>0.031417</td>\n      <td>0.016976</td>\n      <td>0.034124</td>\n      <td>4.245345</td>\n      <td>-4.211221</td>\n      <td>0.000204</td>\n      <td>-0.094470</td>\n      <td>-438</td>\n      <td>0</td>\n      <td>-0.079969</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-17</td>\n      <td>0.042850</td>\n      <td>-0.019473</td>\n      <td>0.022451</td>\n      <td>-0.015622</td>\n      <td>-0.031316</td>\n      <td>0.000674</td>\n      <td>-0.025535</td>\n      <td>-4.735856</td>\n      <td>4.710321</td>\n      <td>0.000201</td>\n      <td>0.082011</td>\n      <td>248</td>\n      <td>0</td>\n      <td>-0.053972</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88614</th>\n      <td>-10</td>\n      <td>-0.010079</td>\n      <td>0.030940</td>\n      <td>0.014597</td>\n      <td>-0.009411</td>\n      <td>-0.010866</td>\n      <td>-0.028271</td>\n      <td>-0.026448</td>\n      <td>-3.707808</td>\n      <td>3.681360</td>\n      <td>0.000142</td>\n      <td>0.106061</td>\n      <td>443</td>\n      <td>4701</td>\n      <td>-0.022222</td>\n    </tr>\n    <tr>\n      <th>88615</th>\n      <td>-4</td>\n      <td>0.033206</td>\n      <td>-0.034458</td>\n      <td>0.002744</td>\n      <td>0.038121</td>\n      <td>0.020868</td>\n      <td>0.016608</td>\n      <td>0.057753</td>\n      <td>0.868124</td>\n      <td>-0.810371</td>\n      <td>0.000124</td>\n      <td>0.129234</td>\n      <td>738</td>\n      <td>8927</td>\n      <td>0.076897</td>\n    </tr>\n    <tr>\n      <th>88616</th>\n      <td>0</td>\n      <td>0.026335</td>\n      <td>-0.049864</td>\n      <td>-0.058749</td>\n      <td>0.044221</td>\n      <td>0.058841</td>\n      <td>0.172088</td>\n      <td>0.099067</td>\n      <td>2.037906</td>\n      <td>-1.938838</td>\n      <td>0.000563</td>\n      <td>0.300334</td>\n      <td>1284</td>\n      <td>13220</td>\n      <td>-0.002126</td>\n    </tr>\n    <tr>\n      <th>88617</th>\n      <td>0</td>\n      <td>-0.014128</td>\n      <td>0.023219</td>\n      <td>0.013561</td>\n      <td>0.025448</td>\n      <td>0.035021</td>\n      <td>0.017865</td>\n      <td>0.046802</td>\n      <td>-0.874317</td>\n      <td>0.921119</td>\n      <td>-0.000378</td>\n      <td>-0.014205</td>\n      <td>-3</td>\n      <td>-1017</td>\n      <td>-0.040415</td>\n    </tr>\n    <tr>\n      <th>88618</th>\n      <td>-5</td>\n      <td>0.023339</td>\n      <td>-0.000069</td>\n      <td>-0.007607</td>\n      <td>0.006534</td>\n      <td>0.008845</td>\n      <td>0.072957</td>\n      <td>0.023514</td>\n      <td>-1.260051</td>\n      <td>1.283565</td>\n      <td>-0.000230</td>\n      <td>0.327652</td>\n      <td>1323</td>\n      <td>-1076</td>\n      <td>0.017322</td>\n    </tr>\n  </tbody>\n</table>\n<p>88619 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference #Try to remove games where both are seed 18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "teamid = x_train_table[['Team1', 'Team2']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "       Team1  Team2\n0       1328   1104\n1       1393   1272\n2       1266   1437\n3       1296   1457\n4       1400   1208\n...      ...    ...\n88614   1393   1400\n88615   1242   1393\n88616   1250   1197\n88617   1356   1104\n88618   1436   1163\n\n[88619 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Team1</th>\n      <th>Team2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1328</td>\n      <td>1104</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1393</td>\n      <td>1272</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1266</td>\n      <td>1437</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1296</td>\n      <td>1457</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1400</td>\n      <td>1208</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88614</th>\n      <td>1393</td>\n      <td>1400</td>\n    </tr>\n    <tr>\n      <th>88615</th>\n      <td>1242</td>\n      <td>1393</td>\n    </tr>\n    <tr>\n      <th>88616</th>\n      <td>1250</td>\n      <td>1197</td>\n    </tr>\n    <tr>\n      <th>88617</th>\n      <td>1356</td>\n      <td>1104</td>\n    </tr>\n    <tr>\n      <th>88618</th>\n      <td>1436</td>\n      <td>1163</td>\n    </tr>\n  </tbody>\n</table>\n<p>88619 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teamid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "      Year  Team1  Seed1      ORP1      SBP1       TP1      FGP1      TPP1  \\\n0     2021   1328      8  0.499702  0.131068  0.127184  0.441661  0.338409   \n1     2021   1393     11  0.489117  0.160151  0.124823  0.441042  0.336650   \n2     2021   1437      5  0.511923  0.087237  0.108899  0.449690  0.352431   \n3     2021   1457     12  0.576163  0.125322  0.169159  0.460697  0.353043   \n4     2021   1208     18  0.498188  0.123878  0.175045  0.455311  0.323232   \n...    ...    ...    ...       ...       ...       ...       ...       ...   \n9328  2022   1393     18  0.490014  0.121733  0.117263  0.450739  0.376412   \n9329  2022   1242      1  0.538949  0.127395  0.145138  0.480988  0.355263   \n9330  2022   1197     18  0.463469  0.105462  0.140756  0.421855  0.297362   \n9331  2022   1356     18  0.490208  0.107700  0.150685  0.441366  0.351310   \n9332  2022   1163      5  0.561272  0.151251  0.138362  0.435533  0.353116   \n\n          FTP1       OR1  ...      TPP2      FTP2       OR2        DR2  \\\n0     0.744344  0.865679  ...  0.350877  0.720217  0.849164  79.491075   \n1     0.784722  0.873733  ...  0.354455  0.620619  0.830533  76.715811   \n2     0.765464  0.882634  ...  0.322523  0.734615  0.862574  82.170881   \n3     0.685824  0.872486  ...  0.323988  0.615385  0.778947  89.531345   \n4     0.686792  0.863921  ...  0.356577  0.707602  0.871269  80.224215   \n...        ...       ...  ...       ...       ...       ...        ...   \n9328  0.736934  0.879132  ...  0.323344  0.748682  0.863403  79.932970   \n9329  0.724398  0.911323  ...  0.376412  0.736934  0.879132  85.178817   \n9330  0.705757  0.822796  ...  0.381818  0.741935  0.878939  86.018590   \n9331  0.690265  0.855735  ...  0.308411  0.729970  0.859013  84.223217   \n9332  0.749585  0.855923  ...  0.367666  0.746835  0.943590  81.734007   \n\n            NR2     Pace2       WP2   TM2    HM2      WP52  \n0    -78.641911  0.004380  0.800000  1621   8781  0.206612  \n1    -75.885278  0.003246  0.666667   714  23535  0.040000  \n2    -81.308306  0.003402  0.481481   111  15598  0.049383  \n3    -88.752398  0.002368  0.157895  -764  -5915  0.003460  \n4    -79.352946  0.003498  0.730769   962  15796  0.284444  \n...         ...       ...       ...   ...    ...       ...  \n9328 -79.069567  0.002880  0.656250   815  16758  0.071111  \n9329 -84.299685  0.003302  0.484848   195  21397  0.022500  \n9330 -85.139652  0.002743  0.387097  -220   5613  0.018595  \n9331 -83.364204  0.003387  0.593750   599  10402  0.055363  \n9332 -80.790417  0.002882  0.843750  1790  18654  0.081633  \n\n[9333 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Team1</th>\n      <th>Seed1</th>\n      <th>ORP1</th>\n      <th>SBP1</th>\n      <th>TP1</th>\n      <th>FGP1</th>\n      <th>TPP1</th>\n      <th>FTP1</th>\n      <th>OR1</th>\n      <th>...</th>\n      <th>TPP2</th>\n      <th>FTP2</th>\n      <th>OR2</th>\n      <th>DR2</th>\n      <th>NR2</th>\n      <th>Pace2</th>\n      <th>WP2</th>\n      <th>TM2</th>\n      <th>HM2</th>\n      <th>WP52</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021</td>\n      <td>1328</td>\n      <td>8</td>\n      <td>0.499702</td>\n      <td>0.131068</td>\n      <td>0.127184</td>\n      <td>0.441661</td>\n      <td>0.338409</td>\n      <td>0.744344</td>\n      <td>0.865679</td>\n      <td>...</td>\n      <td>0.350877</td>\n      <td>0.720217</td>\n      <td>0.849164</td>\n      <td>79.491075</td>\n      <td>-78.641911</td>\n      <td>0.004380</td>\n      <td>0.800000</td>\n      <td>1621</td>\n      <td>8781</td>\n      <td>0.206612</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021</td>\n      <td>1393</td>\n      <td>11</td>\n      <td>0.489117</td>\n      <td>0.160151</td>\n      <td>0.124823</td>\n      <td>0.441042</td>\n      <td>0.336650</td>\n      <td>0.784722</td>\n      <td>0.873733</td>\n      <td>...</td>\n      <td>0.354455</td>\n      <td>0.620619</td>\n      <td>0.830533</td>\n      <td>76.715811</td>\n      <td>-75.885278</td>\n      <td>0.003246</td>\n      <td>0.666667</td>\n      <td>714</td>\n      <td>23535</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021</td>\n      <td>1437</td>\n      <td>5</td>\n      <td>0.511923</td>\n      <td>0.087237</td>\n      <td>0.108899</td>\n      <td>0.449690</td>\n      <td>0.352431</td>\n      <td>0.765464</td>\n      <td>0.882634</td>\n      <td>...</td>\n      <td>0.322523</td>\n      <td>0.734615</td>\n      <td>0.862574</td>\n      <td>82.170881</td>\n      <td>-81.308306</td>\n      <td>0.003402</td>\n      <td>0.481481</td>\n      <td>111</td>\n      <td>15598</td>\n      <td>0.049383</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021</td>\n      <td>1457</td>\n      <td>12</td>\n      <td>0.576163</td>\n      <td>0.125322</td>\n      <td>0.169159</td>\n      <td>0.460697</td>\n      <td>0.353043</td>\n      <td>0.685824</td>\n      <td>0.872486</td>\n      <td>...</td>\n      <td>0.323988</td>\n      <td>0.615385</td>\n      <td>0.778947</td>\n      <td>89.531345</td>\n      <td>-88.752398</td>\n      <td>0.002368</td>\n      <td>0.157895</td>\n      <td>-764</td>\n      <td>-5915</td>\n      <td>0.003460</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021</td>\n      <td>1208</td>\n      <td>18</td>\n      <td>0.498188</td>\n      <td>0.123878</td>\n      <td>0.175045</td>\n      <td>0.455311</td>\n      <td>0.323232</td>\n      <td>0.686792</td>\n      <td>0.863921</td>\n      <td>...</td>\n      <td>0.356577</td>\n      <td>0.707602</td>\n      <td>0.871269</td>\n      <td>80.224215</td>\n      <td>-79.352946</td>\n      <td>0.003498</td>\n      <td>0.730769</td>\n      <td>962</td>\n      <td>15796</td>\n      <td>0.284444</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9328</th>\n      <td>2022</td>\n      <td>1393</td>\n      <td>18</td>\n      <td>0.490014</td>\n      <td>0.121733</td>\n      <td>0.117263</td>\n      <td>0.450739</td>\n      <td>0.376412</td>\n      <td>0.736934</td>\n      <td>0.879132</td>\n      <td>...</td>\n      <td>0.323344</td>\n      <td>0.748682</td>\n      <td>0.863403</td>\n      <td>79.932970</td>\n      <td>-79.069567</td>\n      <td>0.002880</td>\n      <td>0.656250</td>\n      <td>815</td>\n      <td>16758</td>\n      <td>0.071111</td>\n    </tr>\n    <tr>\n      <th>9329</th>\n      <td>2022</td>\n      <td>1242</td>\n      <td>1</td>\n      <td>0.538949</td>\n      <td>0.127395</td>\n      <td>0.145138</td>\n      <td>0.480988</td>\n      <td>0.355263</td>\n      <td>0.724398</td>\n      <td>0.911323</td>\n      <td>...</td>\n      <td>0.376412</td>\n      <td>0.736934</td>\n      <td>0.879132</td>\n      <td>85.178817</td>\n      <td>-84.299685</td>\n      <td>0.003302</td>\n      <td>0.484848</td>\n      <td>195</td>\n      <td>21397</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <th>9330</th>\n      <td>2022</td>\n      <td>1197</td>\n      <td>18</td>\n      <td>0.463469</td>\n      <td>0.105462</td>\n      <td>0.140756</td>\n      <td>0.421855</td>\n      <td>0.297362</td>\n      <td>0.705757</td>\n      <td>0.822796</td>\n      <td>...</td>\n      <td>0.381818</td>\n      <td>0.741935</td>\n      <td>0.878939</td>\n      <td>86.018590</td>\n      <td>-85.139652</td>\n      <td>0.002743</td>\n      <td>0.387097</td>\n      <td>-220</td>\n      <td>5613</td>\n      <td>0.018595</td>\n    </tr>\n    <tr>\n      <th>9331</th>\n      <td>2022</td>\n      <td>1356</td>\n      <td>18</td>\n      <td>0.490208</td>\n      <td>0.107700</td>\n      <td>0.150685</td>\n      <td>0.441366</td>\n      <td>0.351310</td>\n      <td>0.690265</td>\n      <td>0.855735</td>\n      <td>...</td>\n      <td>0.308411</td>\n      <td>0.729970</td>\n      <td>0.859013</td>\n      <td>84.223217</td>\n      <td>-83.364204</td>\n      <td>0.003387</td>\n      <td>0.593750</td>\n      <td>599</td>\n      <td>10402</td>\n      <td>0.055363</td>\n    </tr>\n    <tr>\n      <th>9332</th>\n      <td>2022</td>\n      <td>1163</td>\n      <td>5</td>\n      <td>0.561272</td>\n      <td>0.151251</td>\n      <td>0.138362</td>\n      <td>0.435533</td>\n      <td>0.353116</td>\n      <td>0.749585</td>\n      <td>0.855923</td>\n      <td>...</td>\n      <td>0.367666</td>\n      <td>0.746835</td>\n      <td>0.943590</td>\n      <td>81.734007</td>\n      <td>-80.790417</td>\n      <td>0.002882</td>\n      <td>0.843750</td>\n      <td>1790</td>\n      <td>18654</td>\n      <td>0.081633</td>\n    </tr>\n  </tbody>\n</table>\n<p>9333 rows × 33 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/5rcwl4d96zvcq7zxv7fzt3m80000gn/T/ipykernel_46974/818353188.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_team1.rename(columns={\"Seed1\" : \"SDF\",  \"ORP1\": \"ORP\", \"SBP1\": \"SBP\", \"TP1\": \"TP\", \"FGP1\": \"FGP\", \"TPP1\": \"TPP\", \"FTP1\": \"FTP\", \"OR1\": \"OR\", \"DR1\": \"DR\", \"NR1\": \"NR\", \"Pace1\": \"Pace\", \"WP1\": \"WP\", \"TM1\": \"TM\", \"HM1\":\"HM\", \"WP51\": \"WP5\"}, inplace=True)\n",
      "/var/folders/qw/5rcwl4d96zvcq7zxv7fzt3m80000gn/T/ipykernel_46974/818353188.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_team2.rename(columns={\"Seed2\" : \"SDF\",  \"ORP2\": \"ORP\", \"SBP2\": \"SBP\", \"TP2\": \"TP\", \"FGP2\": \"FGP\", \"TPP2\": \"TPP\", \"FTP2\": \"FTP\", \"OR2\": \"OR\", \"DR2\": \"DR\", \"NR2\": \"NR\", \"Pace2\": \"Pace\", \"WP2\": \"WP\", \"TM2\": \"TM\", \"HM2\":\"HM\", \"WP52\": \"WP5\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "x_valid_table\n",
    "valid_team1 = x_valid_table[['Seed1', 'ORP1', 'SBP1', 'TP1', 'FGP1', 'TPP1', 'FTP1', 'OR1', 'DR1', 'NR1', 'Pace1', 'WP1', 'TM1', 'HM1', 'WP51']]\n",
    "valid_team2 = x_valid_table[['Seed2', 'ORP2', 'SBP2', 'TP2', 'FGP2', 'TPP2', 'FTP2', 'OR2', 'DR2', 'NR2', 'Pace2', 'WP2', 'TM2', 'HM2', 'WP52']]\n",
    "valid_team1.rename(columns={\"Seed1\" : \"SDF\",  \"ORP1\": \"ORP\", \"SBP1\": \"SBP\", \"TP1\": \"TP\", \"FGP1\": \"FGP\", \"TPP1\": \"TPP\", \"FTP1\": \"FTP\", \"OR1\": \"OR\", \"DR1\": \"DR\", \"NR1\": \"NR\", \"Pace1\": \"Pace\", \"WP1\": \"WP\", \"TM1\": \"TM\", \"HM1\":\"HM\", \"WP51\": \"WP5\"}, inplace=True)\n",
    "valid_team2.rename(columns={\"Seed2\" : \"SDF\",  \"ORP2\": \"ORP\", \"SBP2\": \"SBP\", \"TP2\": \"TP\", \"FGP2\": \"FGP\", \"TPP2\": \"TPP\", \"FTP2\": \"FTP\", \"OR2\": \"OR\", \"DR2\": \"DR\", \"NR2\": \"NR\", \"Pace2\": \"Pace\", \"WP2\": \"WP\", \"TM2\": \"TM\", \"HM2\":\"HM\", \"WP52\": \"WP5\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "      SDF       ORP       SBP        TP       FGP       TPP       FTP  \\\n0       8  0.499702  0.131068  0.127184  0.441661  0.338409  0.744344   \n1      11  0.489117  0.160151  0.124823  0.441042  0.336650  0.784722   \n2       5  0.511923  0.087237  0.108899  0.449690  0.352431  0.765464   \n3      12  0.576163  0.125322  0.169159  0.460697  0.353043  0.685824   \n4      18  0.498188  0.123878  0.175045  0.455311  0.323232  0.686792   \n...   ...       ...       ...       ...       ...       ...       ...   \n9328   18  0.490014  0.121733  0.117263  0.450739  0.376412  0.736934   \n9329    1  0.538949  0.127395  0.145138  0.480988  0.355263  0.724398   \n9330   18  0.463469  0.105462  0.140756  0.421855  0.297362  0.705757   \n9331   18  0.490208  0.107700  0.150685  0.441366  0.351310  0.690265   \n9332    5  0.561272  0.151251  0.138362  0.435533  0.353116  0.749585   \n\n            OR         DR         NR      Pace        WP    TM     HM  \\\n0     0.865679  84.029126 -83.163448  0.003364  0.600000   539  13794   \n1     0.873733  83.278380 -82.404647  0.003381  0.640000   664  20733   \n2     0.882634  86.475410 -85.592776  0.002934  0.727273   890  22905   \n3     0.872486  82.671480 -81.798994  0.003409  0.958333  1799   9959   \n4     0.863921  88.195691 -87.331770  0.003481  0.520000   132   4705   \n...        ...        ...        ...       ...       ...   ...    ...   \n9328  0.879132  85.178817 -84.299685  0.003302  0.484848   195  21397   \n9329  0.911323  82.221434 -81.310110  0.003334  0.823529  1848  32653   \n9330  0.822796  81.134454 -80.311657  0.002464  0.392857  -179  -8091   \n9331  0.855735  87.246103 -86.390368  0.002538  0.500000   200   7567   \n9332  0.855923  79.037149 -78.181226  0.003196  0.718750  1262  17862   \n\n           WP5  \n0     0.081633  \n1     0.062500  \n2     0.062500  \n3     0.562500  \n4     0.062500  \n...        ...  \n9328  0.022500  \n9329  0.250000  \n9330  0.051653  \n9331  0.062500  \n9332  0.033058  \n\n[9333 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SDF</th>\n      <th>ORP</th>\n      <th>SBP</th>\n      <th>TP</th>\n      <th>FGP</th>\n      <th>TPP</th>\n      <th>FTP</th>\n      <th>OR</th>\n      <th>DR</th>\n      <th>NR</th>\n      <th>Pace</th>\n      <th>WP</th>\n      <th>TM</th>\n      <th>HM</th>\n      <th>WP5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>0.499702</td>\n      <td>0.131068</td>\n      <td>0.127184</td>\n      <td>0.441661</td>\n      <td>0.338409</td>\n      <td>0.744344</td>\n      <td>0.865679</td>\n      <td>84.029126</td>\n      <td>-83.163448</td>\n      <td>0.003364</td>\n      <td>0.600000</td>\n      <td>539</td>\n      <td>13794</td>\n      <td>0.081633</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11</td>\n      <td>0.489117</td>\n      <td>0.160151</td>\n      <td>0.124823</td>\n      <td>0.441042</td>\n      <td>0.336650</td>\n      <td>0.784722</td>\n      <td>0.873733</td>\n      <td>83.278380</td>\n      <td>-82.404647</td>\n      <td>0.003381</td>\n      <td>0.640000</td>\n      <td>664</td>\n      <td>20733</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>0.511923</td>\n      <td>0.087237</td>\n      <td>0.108899</td>\n      <td>0.449690</td>\n      <td>0.352431</td>\n      <td>0.765464</td>\n      <td>0.882634</td>\n      <td>86.475410</td>\n      <td>-85.592776</td>\n      <td>0.002934</td>\n      <td>0.727273</td>\n      <td>890</td>\n      <td>22905</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12</td>\n      <td>0.576163</td>\n      <td>0.125322</td>\n      <td>0.169159</td>\n      <td>0.460697</td>\n      <td>0.353043</td>\n      <td>0.685824</td>\n      <td>0.872486</td>\n      <td>82.671480</td>\n      <td>-81.798994</td>\n      <td>0.003409</td>\n      <td>0.958333</td>\n      <td>1799</td>\n      <td>9959</td>\n      <td>0.562500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18</td>\n      <td>0.498188</td>\n      <td>0.123878</td>\n      <td>0.175045</td>\n      <td>0.455311</td>\n      <td>0.323232</td>\n      <td>0.686792</td>\n      <td>0.863921</td>\n      <td>88.195691</td>\n      <td>-87.331770</td>\n      <td>0.003481</td>\n      <td>0.520000</td>\n      <td>132</td>\n      <td>4705</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9328</th>\n      <td>18</td>\n      <td>0.490014</td>\n      <td>0.121733</td>\n      <td>0.117263</td>\n      <td>0.450739</td>\n      <td>0.376412</td>\n      <td>0.736934</td>\n      <td>0.879132</td>\n      <td>85.178817</td>\n      <td>-84.299685</td>\n      <td>0.003302</td>\n      <td>0.484848</td>\n      <td>195</td>\n      <td>21397</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <th>9329</th>\n      <td>1</td>\n      <td>0.538949</td>\n      <td>0.127395</td>\n      <td>0.145138</td>\n      <td>0.480988</td>\n      <td>0.355263</td>\n      <td>0.724398</td>\n      <td>0.911323</td>\n      <td>82.221434</td>\n      <td>-81.310110</td>\n      <td>0.003334</td>\n      <td>0.823529</td>\n      <td>1848</td>\n      <td>32653</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>9330</th>\n      <td>18</td>\n      <td>0.463469</td>\n      <td>0.105462</td>\n      <td>0.140756</td>\n      <td>0.421855</td>\n      <td>0.297362</td>\n      <td>0.705757</td>\n      <td>0.822796</td>\n      <td>81.134454</td>\n      <td>-80.311657</td>\n      <td>0.002464</td>\n      <td>0.392857</td>\n      <td>-179</td>\n      <td>-8091</td>\n      <td>0.051653</td>\n    </tr>\n    <tr>\n      <th>9331</th>\n      <td>18</td>\n      <td>0.490208</td>\n      <td>0.107700</td>\n      <td>0.150685</td>\n      <td>0.441366</td>\n      <td>0.351310</td>\n      <td>0.690265</td>\n      <td>0.855735</td>\n      <td>87.246103</td>\n      <td>-86.390368</td>\n      <td>0.002538</td>\n      <td>0.500000</td>\n      <td>200</td>\n      <td>7567</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <th>9332</th>\n      <td>5</td>\n      <td>0.561272</td>\n      <td>0.151251</td>\n      <td>0.138362</td>\n      <td>0.435533</td>\n      <td>0.353116</td>\n      <td>0.749585</td>\n      <td>0.855923</td>\n      <td>79.037149</td>\n      <td>-78.181226</td>\n      <td>0.003196</td>\n      <td>0.718750</td>\n      <td>1262</td>\n      <td>17862</td>\n      <td>0.033058</td>\n    </tr>\n  </tbody>\n</table>\n<p>9333 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_team1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "testdifference = valid_team1.subtract(valid_team2, axis=\"columns\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "      SDF       ORP       SBP        TP       FGP       TPP       FTP  \\\n0       6 -0.012745 -0.016672 -0.030430  0.011098 -0.012468  0.024127   \n1      -7 -0.063846 -0.009651 -0.070398  0.000591 -0.017805  0.164104   \n2     -13 -0.007685 -0.016076 -0.050211  0.004273  0.029908  0.030849   \n3      -6  0.078152  0.018810  0.015781  0.061747  0.029056  0.070439   \n4      15 -0.041881  0.002353  0.014507  0.001429 -0.033345 -0.020810   \n...   ...       ...       ...       ...       ...       ...       ...   \n9328   12 -0.025611 -0.027408 -0.028527  0.008442  0.053068 -0.011748   \n9329  -17  0.048935  0.005662  0.027876  0.030249 -0.021148 -0.012536   \n9330    0 -0.017609  0.012898 -0.011064 -0.026001 -0.084456 -0.036179   \n9331   12 -0.033430 -0.021477 -0.004672  0.000629  0.042898 -0.039705   \n9332   -8  0.020689  0.048557  0.012521 -0.058003 -0.014551  0.002750   \n\n            OR        DR        NR      Pace        WP    TM     HM       WP5  \n0     0.016515  4.538051 -4.521537 -0.001016 -0.200000 -1082   5013 -0.124979  \n1     0.043200  6.562569 -6.519369  0.000136 -0.026667   -50  -2802  0.022500  \n2     0.020060  4.304529 -4.284470 -0.000467  0.245791   779   7307  0.013117  \n3     0.093539 -6.859865  6.953404  0.001041  0.800439  2563  15874  0.559040  \n4    -0.007348  7.971476 -7.978824 -0.000017 -0.210769  -830 -11091 -0.221944  \n...        ...       ...       ...       ...       ...   ...    ...       ...  \n9328  0.015729  5.245847 -5.230118  0.000422 -0.171402  -620   4639 -0.048611  \n9329  0.032191 -2.957383  2.989575  0.000032  0.338681  1653  11256  0.227500  \n9330 -0.056142 -4.884136  4.827994 -0.000279  0.005760    41 -13704  0.033058  \n9331 -0.003278  3.022886 -3.026164 -0.000849 -0.093750  -399  -2835  0.007137  \n9332 -0.087667 -2.696857  2.609191  0.000314 -0.125000  -528   -792 -0.048575  \n\n[9333 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SDF</th>\n      <th>ORP</th>\n      <th>SBP</th>\n      <th>TP</th>\n      <th>FGP</th>\n      <th>TPP</th>\n      <th>FTP</th>\n      <th>OR</th>\n      <th>DR</th>\n      <th>NR</th>\n      <th>Pace</th>\n      <th>WP</th>\n      <th>TM</th>\n      <th>HM</th>\n      <th>WP5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>-0.012745</td>\n      <td>-0.016672</td>\n      <td>-0.030430</td>\n      <td>0.011098</td>\n      <td>-0.012468</td>\n      <td>0.024127</td>\n      <td>0.016515</td>\n      <td>4.538051</td>\n      <td>-4.521537</td>\n      <td>-0.001016</td>\n      <td>-0.200000</td>\n      <td>-1082</td>\n      <td>5013</td>\n      <td>-0.124979</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-7</td>\n      <td>-0.063846</td>\n      <td>-0.009651</td>\n      <td>-0.070398</td>\n      <td>0.000591</td>\n      <td>-0.017805</td>\n      <td>0.164104</td>\n      <td>0.043200</td>\n      <td>6.562569</td>\n      <td>-6.519369</td>\n      <td>0.000136</td>\n      <td>-0.026667</td>\n      <td>-50</td>\n      <td>-2802</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-13</td>\n      <td>-0.007685</td>\n      <td>-0.016076</td>\n      <td>-0.050211</td>\n      <td>0.004273</td>\n      <td>0.029908</td>\n      <td>0.030849</td>\n      <td>0.020060</td>\n      <td>4.304529</td>\n      <td>-4.284470</td>\n      <td>-0.000467</td>\n      <td>0.245791</td>\n      <td>779</td>\n      <td>7307</td>\n      <td>0.013117</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6</td>\n      <td>0.078152</td>\n      <td>0.018810</td>\n      <td>0.015781</td>\n      <td>0.061747</td>\n      <td>0.029056</td>\n      <td>0.070439</td>\n      <td>0.093539</td>\n      <td>-6.859865</td>\n      <td>6.953404</td>\n      <td>0.001041</td>\n      <td>0.800439</td>\n      <td>2563</td>\n      <td>15874</td>\n      <td>0.559040</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>-0.041881</td>\n      <td>0.002353</td>\n      <td>0.014507</td>\n      <td>0.001429</td>\n      <td>-0.033345</td>\n      <td>-0.020810</td>\n      <td>-0.007348</td>\n      <td>7.971476</td>\n      <td>-7.978824</td>\n      <td>-0.000017</td>\n      <td>-0.210769</td>\n      <td>-830</td>\n      <td>-11091</td>\n      <td>-0.221944</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9328</th>\n      <td>12</td>\n      <td>-0.025611</td>\n      <td>-0.027408</td>\n      <td>-0.028527</td>\n      <td>0.008442</td>\n      <td>0.053068</td>\n      <td>-0.011748</td>\n      <td>0.015729</td>\n      <td>5.245847</td>\n      <td>-5.230118</td>\n      <td>0.000422</td>\n      <td>-0.171402</td>\n      <td>-620</td>\n      <td>4639</td>\n      <td>-0.048611</td>\n    </tr>\n    <tr>\n      <th>9329</th>\n      <td>-17</td>\n      <td>0.048935</td>\n      <td>0.005662</td>\n      <td>0.027876</td>\n      <td>0.030249</td>\n      <td>-0.021148</td>\n      <td>-0.012536</td>\n      <td>0.032191</td>\n      <td>-2.957383</td>\n      <td>2.989575</td>\n      <td>0.000032</td>\n      <td>0.338681</td>\n      <td>1653</td>\n      <td>11256</td>\n      <td>0.227500</td>\n    </tr>\n    <tr>\n      <th>9330</th>\n      <td>0</td>\n      <td>-0.017609</td>\n      <td>0.012898</td>\n      <td>-0.011064</td>\n      <td>-0.026001</td>\n      <td>-0.084456</td>\n      <td>-0.036179</td>\n      <td>-0.056142</td>\n      <td>-4.884136</td>\n      <td>4.827994</td>\n      <td>-0.000279</td>\n      <td>0.005760</td>\n      <td>41</td>\n      <td>-13704</td>\n      <td>0.033058</td>\n    </tr>\n    <tr>\n      <th>9331</th>\n      <td>12</td>\n      <td>-0.033430</td>\n      <td>-0.021477</td>\n      <td>-0.004672</td>\n      <td>0.000629</td>\n      <td>0.042898</td>\n      <td>-0.039705</td>\n      <td>-0.003278</td>\n      <td>3.022886</td>\n      <td>-3.026164</td>\n      <td>-0.000849</td>\n      <td>-0.093750</td>\n      <td>-399</td>\n      <td>-2835</td>\n      <td>0.007137</td>\n    </tr>\n    <tr>\n      <th>9332</th>\n      <td>-8</td>\n      <td>0.020689</td>\n      <td>0.048557</td>\n      <td>0.012521</td>\n      <td>-0.058003</td>\n      <td>-0.014551</td>\n      <td>0.002750</td>\n      <td>-0.087667</td>\n      <td>-2.696857</td>\n      <td>2.609191</td>\n      <td>0.000314</td>\n      <td>-0.125000</td>\n      <td>-528</td>\n      <td>-792</td>\n      <td>-0.048575</td>\n    </tr>\n  </tbody>\n</table>\n<p>9333 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdifference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "RangeIndex(start=0, stop=88619, step=1)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.index.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "x_train_diff = difference.dropna()\n",
    "y_train_diff = y_train_table.loc[x_train_diff.index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "x_train_diff = x_train_diff.reset_index(drop=True)\n",
    "y_train_diff = y_train_diff.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "x_val = testdifference.dropna()\n",
    "y_val = y_valid_table.loc[x_val.index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "x_val = x_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 18:09:33.943570: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-12 18:09:33.943701: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_train_norm = min_max_scaler.fit_transform(x_train_diff.values)\n",
    "x_train_norm = tf.convert_to_tensor(x_train_norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([74836, 12])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_norm.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "for i in ['ORP', 'SBP', 'TP', 'FGP', 'TPP', 'FTP', 'OR', 'DR', 'NR', 'Pace', 'WP']:\n",
    "    input = tf.keras.Input(shape=(1,), name=i)\n",
    "    encoded = tf.keras.layers.Dense(1, activation='relu')(input)\n",
    "    all_inputs.append(input)\n",
    "    encoded_features.append(encoded)\n",
    "seed_col = tf.keras.Input(shape=(1,), name='SDF', dtype='int64')\n",
    "encoding_layer = tf.keras.layers.Embedding(17, 1, input_length=1)(seed_col)\n",
    "encoding_layer = tf.keras.layers.Flatten()(encoding_layer)\n",
    "all_inputs.append(seed_col)\n",
    "encoded_features.append(encoding_layer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "allfeatures = tf.keras.layers.concatenate(encoded_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(allfeatures),\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(x_train_norm.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 512)               6656      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,583,105\n",
      "Trainable params: 1,583,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "x_valid_table = testdifference.dropna()\n",
    "y_valid_table = y_valid_table.loc[x_valid_table.index]\n",
    "x_valid_table = min_max_scaler.fit_transform(x_valid_table.values)\n",
    "x_valid_table = tf.convert_to_tensor(x_valid_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_diff.shape[0] == x_train_diff.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.6618 - accuracy: 0.5977 - val_loss: 0.6789 - val_accuracy: 0.5610\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6613 - accuracy: 0.5980 - val_loss: 0.6754 - val_accuracy: 0.5701\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6609 - accuracy: 0.5991 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6608 - accuracy: 0.6003 - val_loss: 0.6762 - val_accuracy: 0.5705\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6613 - accuracy: 0.6002 - val_loss: 0.6769 - val_accuracy: 0.5670\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6618 - accuracy: 0.5990 - val_loss: 0.6767 - val_accuracy: 0.5709\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6606 - accuracy: 0.6004 - val_loss: 0.6755 - val_accuracy: 0.5702\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6603 - accuracy: 0.5995 - val_loss: 0.6754 - val_accuracy: 0.5701\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6609 - accuracy: 0.5998 - val_loss: 0.6756 - val_accuracy: 0.5697\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6607 - accuracy: 0.6006 - val_loss: 0.6770 - val_accuracy: 0.5703\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6610 - accuracy: 0.6004 - val_loss: 0.6767 - val_accuracy: 0.5698\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6604 - accuracy: 0.6007 - val_loss: 0.6760 - val_accuracy: 0.5704\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6602 - accuracy: 0.6012 - val_loss: 0.6757 - val_accuracy: 0.5713\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6605 - accuracy: 0.5995 - val_loss: 0.6761 - val_accuracy: 0.5703\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6609 - accuracy: 0.6000 - val_loss: 0.6757 - val_accuracy: 0.5713\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6606 - accuracy: 0.6001 - val_loss: 0.6777 - val_accuracy: 0.5671\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6601 - accuracy: 0.6009 - val_loss: 0.6765 - val_accuracy: 0.5689\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6606 - accuracy: 0.6008 - val_loss: 0.6756 - val_accuracy: 0.5702\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6600 - accuracy: 0.6021 - val_loss: 0.6770 - val_accuracy: 0.5702\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6603 - accuracy: 0.5999 - val_loss: 0.6768 - val_accuracy: 0.5699\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6603 - accuracy: 0.6003 - val_loss: 0.6758 - val_accuracy: 0.5703\n",
      "Epoch 22/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6598 - accuracy: 0.6015 - val_loss: 0.6758 - val_accuracy: 0.5697\n",
      "Epoch 23/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6601 - accuracy: 0.6026 - val_loss: 0.6770 - val_accuracy: 0.5706\n",
      "Epoch 24/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6606 - accuracy: 0.6015 - val_loss: 0.6758 - val_accuracy: 0.5703\n",
      "Epoch 25/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6600 - accuracy: 0.6028 - val_loss: 0.6758 - val_accuracy: 0.5696\n",
      "Epoch 26/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6604 - accuracy: 0.6002 - val_loss: 0.6756 - val_accuracy: 0.5706\n",
      "Epoch 27/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6601 - accuracy: 0.5999 - val_loss: 0.6771 - val_accuracy: 0.5685\n",
      "Epoch 28/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6605 - accuracy: 0.6005 - val_loss: 0.6758 - val_accuracy: 0.5715\n",
      "Epoch 29/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6605 - accuracy: 0.5985 - val_loss: 0.6757 - val_accuracy: 0.5697\n",
      "Epoch 30/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6600 - accuracy: 0.6012 - val_loss: 0.6756 - val_accuracy: 0.5706\n",
      "Epoch 31/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6607 - accuracy: 0.6014 - val_loss: 0.6769 - val_accuracy: 0.5671\n",
      "Epoch 32/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6605 - accuracy: 0.6000 - val_loss: 0.6760 - val_accuracy: 0.5703\n",
      "Epoch 33/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6607 - accuracy: 0.6012 - val_loss: 0.6759 - val_accuracy: 0.5704\n",
      "Epoch 34/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6602 - accuracy: 0.6006 - val_loss: 0.6758 - val_accuracy: 0.5703\n",
      "Epoch 35/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6600 - accuracy: 0.6007 - val_loss: 0.6760 - val_accuracy: 0.5699\n",
      "Epoch 36/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6596 - accuracy: 0.6011 - val_loss: 0.6756 - val_accuracy: 0.5693\n",
      "Epoch 37/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6601 - accuracy: 0.6012 - val_loss: 0.6762 - val_accuracy: 0.5689\n",
      "Epoch 38/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6604 - accuracy: 0.6012 - val_loss: 0.6757 - val_accuracy: 0.5704\n",
      "Epoch 39/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6602 - accuracy: 0.6015 - val_loss: 0.6761 - val_accuracy: 0.5708\n",
      "Epoch 40/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6601 - accuracy: 0.6014 - val_loss: 0.6754 - val_accuracy: 0.5709\n",
      "Epoch 41/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6601 - accuracy: 0.6018 - val_loss: 0.6757 - val_accuracy: 0.5714\n",
      "Epoch 42/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6606 - accuracy: 0.5995 - val_loss: 0.6762 - val_accuracy: 0.5697\n",
      "Epoch 43/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6606 - accuracy: 0.6021 - val_loss: 0.6757 - val_accuracy: 0.5701\n",
      "Epoch 44/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6599 - accuracy: 0.6012 - val_loss: 0.6754 - val_accuracy: 0.5707\n",
      "Epoch 45/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6603 - accuracy: 0.6017 - val_loss: 0.6760 - val_accuracy: 0.5706\n",
      "Epoch 46/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6609 - accuracy: 0.6003 - val_loss: 0.6755 - val_accuracy: 0.5713\n",
      "Epoch 47/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6597 - accuracy: 0.6014 - val_loss: 0.6755 - val_accuracy: 0.5715\n",
      "Epoch 48/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6605 - accuracy: 0.6012 - val_loss: 0.6764 - val_accuracy: 0.5694\n",
      "Epoch 49/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6604 - accuracy: 0.5995 - val_loss: 0.6756 - val_accuracy: 0.5709\n",
      "Epoch 50/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6602 - accuracy: 0.6009 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "Epoch 51/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6606 - accuracy: 0.6008 - val_loss: 0.6757 - val_accuracy: 0.5710\n",
      "Epoch 52/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6600 - accuracy: 0.6011 - val_loss: 0.6754 - val_accuracy: 0.5713\n",
      "Epoch 53/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6600 - accuracy: 0.6021 - val_loss: 0.6765 - val_accuracy: 0.5702\n",
      "Epoch 54/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6601 - accuracy: 0.6012 - val_loss: 0.6761 - val_accuracy: 0.5698\n",
      "Epoch 55/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6601 - accuracy: 0.6012 - val_loss: 0.6761 - val_accuracy: 0.5699\n",
      "Epoch 56/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6603 - accuracy: 0.6012 - val_loss: 0.6755 - val_accuracy: 0.5705\n",
      "Epoch 57/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6604 - accuracy: 0.6007 - val_loss: 0.6759 - val_accuracy: 0.5702\n",
      "Epoch 58/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6600 - accuracy: 0.6009 - val_loss: 0.6757 - val_accuracy: 0.5718\n",
      "Epoch 59/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6600 - accuracy: 0.6026 - val_loss: 0.6762 - val_accuracy: 0.5707\n",
      "Epoch 60/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6599 - accuracy: 0.6010 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "Epoch 61/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6605 - accuracy: 0.5999 - val_loss: 0.6762 - val_accuracy: 0.5705\n",
      "Epoch 62/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6603 - accuracy: 0.5997 - val_loss: 0.6759 - val_accuracy: 0.5694\n",
      "Epoch 63/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6601 - accuracy: 0.6012 - val_loss: 0.6755 - val_accuracy: 0.5698\n",
      "Epoch 64/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6599 - accuracy: 0.6008 - val_loss: 0.6756 - val_accuracy: 0.5705\n",
      "Epoch 65/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6601 - accuracy: 0.6012 - val_loss: 0.6761 - val_accuracy: 0.5699\n",
      "Epoch 66/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6603 - accuracy: 0.6022 - val_loss: 0.6760 - val_accuracy: 0.5719\n",
      "Epoch 67/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6605 - accuracy: 0.6002 - val_loss: 0.6760 - val_accuracy: 0.5716\n",
      "Epoch 68/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.6602 - accuracy: 0.6009 - val_loss: 0.6777 - val_accuracy: 0.5687\n",
      "Epoch 69/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6599 - accuracy: 0.6002 - val_loss: 0.6755 - val_accuracy: 0.5714\n",
      "Epoch 70/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6603 - accuracy: 0.6011 - val_loss: 0.6756 - val_accuracy: 0.5710\n",
      "Epoch 71/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6596 - accuracy: 0.6026 - val_loss: 0.6757 - val_accuracy: 0.5701\n",
      "Epoch 72/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6602 - accuracy: 0.6000 - val_loss: 0.6771 - val_accuracy: 0.5702\n",
      "Epoch 73/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6604 - accuracy: 0.6005 - val_loss: 0.6760 - val_accuracy: 0.5699\n",
      "Epoch 74/1000\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.6596 - accuracy: 0.6009 - val_loss: 0.6758 - val_accuracy: 0.5716\n",
      "Epoch 75/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6599 - accuracy: 0.6013 - val_loss: 0.6766 - val_accuracy: 0.5698\n",
      "Epoch 76/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.6601 - accuracy: 0.6008 - val_loss: 0.6763 - val_accuracy: 0.5704\n",
      "Epoch 77/1000\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.6597 - accuracy: 0.6008 - val_loss: 0.6755 - val_accuracy: 0.5706\n",
      "Epoch 78/1000\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 0.6601 - accuracy: 0.6017 - val_loss: 0.6758 - val_accuracy: 0.5708\n",
      "Epoch 79/1000\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 0.6599 - accuracy: 0.6013 - val_loss: 0.6755 - val_accuracy: 0.5713\n",
      "Epoch 80/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6598 - accuracy: 0.6011 - val_loss: 0.6756 - val_accuracy: 0.5717\n",
      "Epoch 81/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6604 - accuracy: 0.6004 - val_loss: 0.6769 - val_accuracy: 0.5688\n",
      "Epoch 82/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6603 - accuracy: 0.6006 - val_loss: 0.6756 - val_accuracy: 0.5719\n",
      "Epoch 83/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6605 - accuracy: 0.5996 - val_loss: 0.6761 - val_accuracy: 0.5707\n",
      "Epoch 84/1000\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.6598 - accuracy: 0.6010 - val_loss: 0.6762 - val_accuracy: 0.5699\n",
      "Epoch 85/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6605 - accuracy: 0.5999 - val_loss: 0.6766 - val_accuracy: 0.5698\n",
      "Epoch 86/1000\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.6597 - accuracy: 0.6015 - val_loss: 0.6758 - val_accuracy: 0.5707\n",
      "Epoch 87/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6601 - accuracy: 0.6009 - val_loss: 0.6755 - val_accuracy: 0.5721\n",
      "Epoch 88/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6598 - accuracy: 0.6003 - val_loss: 0.6757 - val_accuracy: 0.5705\n",
      "Epoch 89/1000\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.6599 - accuracy: 0.6017 - val_loss: 0.6759 - val_accuracy: 0.5713\n",
      "Epoch 90/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6597 - accuracy: 0.6029 - val_loss: 0.6758 - val_accuracy: 0.5712\n",
      "Epoch 91/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6599 - accuracy: 0.6009 - val_loss: 0.6759 - val_accuracy: 0.5695\n",
      "Epoch 92/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6597 - accuracy: 0.6018 - val_loss: 0.6764 - val_accuracy: 0.5699\n",
      "Epoch 93/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6597 - accuracy: 0.6022 - val_loss: 0.6762 - val_accuracy: 0.5710\n",
      "Epoch 94/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6598 - accuracy: 0.6014 - val_loss: 0.6755 - val_accuracy: 0.5698\n",
      "Epoch 95/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6610 - accuracy: 0.5983 - val_loss: 0.6756 - val_accuracy: 0.5714\n",
      "Epoch 96/1000\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.6608 - accuracy: 0.5988 - val_loss: 0.6756 - val_accuracy: 0.5710\n",
      "Epoch 97/1000\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.6600 - accuracy: 0.6014 - val_loss: 0.6757 - val_accuracy: 0.5699\n",
      "Epoch 98/1000\n",
      "38/47 [=======================>......] - ETA: 7:20 - loss: 0.6596 - accuracy: 0.6007"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_norm, y_train_diff, epochs=1000, batch_size=16000, validation_data=(x_valid_table, y_valid_table))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "(74836, 1)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_diff.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:47:10.384158: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-13 23:47:10.384337: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=16000, labeler=y_train_diff):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = labeler\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "train_ds = df_to_dataset(x_train_diff, labeler=y_train_diff)\n",
    "[(train_feature_batch, label_batch)] = train_ds.take(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:47:12.092872: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-13 23:47:12.116444: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:12.491008: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:12.501432: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:12.879057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:12.889239: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:13.259566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:13.269757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:13.639621: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:13.650609: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:13.982624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:13.992897: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:14.343172: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:14.354010: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:14.696001: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:14.706562: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:15.043844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:15.054071: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:15.393299: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:15.403606: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:15.751251: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:15.762077: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:16.115595: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:16.126027: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:16.471048: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:16.481008: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:16.846725: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:16.857149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:17.221907: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-13 23:47:17.232440: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = tf.keras.layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "all_inputs = []\n",
    "encoded_features = []\n",
    "for i in [\"SDF\", \"ORP\", \"SBP\", \"TP\", \"FGP\", \"TPP\", \"FTP\", \"OR\", \"DR\", \"NR\", \"Pace\", \"WP\", \"TM\", \"HM\", \"WP5\"]:\n",
    "    if i == \"SDF\":\n",
    "        categorical_encoding_layer = tf.keras.Input(shape=(1,), name=i, dtype='int64')\n",
    "        encoding_layer = get_category_encoding_layer(i, train_ds, dtype='int64', max_tokens=16)\n",
    "        encoded_categorical_col = encoding_layer(categorical_encoding_layer)\n",
    "        all_inputs.append(categorical_encoding_layer)\n",
    "        encoded_features.append(encoded_categorical_col)\n",
    "    else:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=i)\n",
    "        normalization_layer = get_normalization_layer(i, train_ds)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "val_ds = df_to_dataset(x_val, shuffle=False, labeler=y_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset element_spec=({'SDF': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'ORP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'SBP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'TP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'FGP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'TPP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'FTP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'OR': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'DR': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'NR': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'Pace': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'WP': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'TM': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'HM': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'WP5': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None, 1), dtype=tf.int64, name=None))>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "allfeatures = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(allfeatures)\n",
    "x = tf.keras.layers.Dropout(0.8)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.8)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='swish')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "column_model = tf.keras.Model(inputs=all_inputs, outputs=x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " SDF (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " integer_lookup (IntegerLookup)  (None, 1)           0           ['SDF[0][0]']                    \n",
      "                                                                                                  \n",
      " ORP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " SBP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " TP (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " FGP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " TPP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " FTP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " OR (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " DR (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " NR (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Pace (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " WP (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " TM (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " HM (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " WP5 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " category_encoding (CategoryEnc  (None, 16)          0           ['integer_lookup[0][0]']         \n",
      " oding)                                                                                           \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 1)            3           ['ORP[0][0]']                    \n",
      "                                                                                                  \n",
      " normalization_1 (Normalization  (None, 1)           3           ['SBP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_2 (Normalization  (None, 1)           3           ['TP[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_3 (Normalization  (None, 1)           3           ['FGP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_4 (Normalization  (None, 1)           3           ['TPP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_5 (Normalization  (None, 1)           3           ['FTP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_6 (Normalization  (None, 1)           3           ['OR[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_7 (Normalization  (None, 1)           3           ['DR[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_8 (Normalization  (None, 1)           3           ['NR[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_9 (Normalization  (None, 1)           3           ['Pace[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_10 (Normalizatio  (None, 1)           3           ['WP[0][0]']                     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " normalization_11 (Normalizatio  (None, 1)           3           ['TM[0][0]']                     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " normalization_12 (Normalizatio  (None, 1)           3           ['HM[0][0]']                     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " normalization_13 (Normalizatio  (None, 1)           3           ['WP5[0][0]']                    \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 30)           0           ['category_encoding[0][0]',      \n",
      "                                                                  'normalization[0][0]',          \n",
      "                                                                  'normalization_1[0][0]',        \n",
      "                                                                  'normalization_2[0][0]',        \n",
      "                                                                  'normalization_3[0][0]',        \n",
      "                                                                  'normalization_4[0][0]',        \n",
      "                                                                  'normalization_5[0][0]',        \n",
      "                                                                  'normalization_6[0][0]',        \n",
      "                                                                  'normalization_7[0][0]',        \n",
      "                                                                  'normalization_8[0][0]',        \n",
      "                                                                  'normalization_9[0][0]',        \n",
      "                                                                  'normalization_10[0][0]',       \n",
      "                                                                  'normalization_11[0][0]',       \n",
      "                                                                  'normalization_12[0][0]',       \n",
      "                                                                  'normalization_13[0][0]']       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         31744       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          262656      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         525312      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 1024)         0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          524800      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1024)         525312      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 1024)         0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1024)         1049600     ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 1024)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 512)          524800      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 512)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          262656      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 512)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1024)         525312      ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 1024)         0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1024)         1049600     ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 1024)         0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 512)          524800      ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 512)          0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1024)         525312      ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 1024)         0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 512)          524800      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 512)          0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 512)          262656      ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 512)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 512)          262656      ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 512)          0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1)            513         ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,907,371\n",
      "Trainable params: 7,907,329\n",
      "Non-trainable params: 42\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=1e-04>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "column_model.summary()\n",
    "column_model.optimizer.lr.assign(0.0001)\n",
    "# column_model.fit(train_ds, epochs=100, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:47:31.914818: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - ETA: 0s - loss: 4.2945 - accuracy: 0.5012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:47:34.572759: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 361ms/step - loss: 4.2945 - accuracy: 0.5012 - val_loss: 4.2306 - val_accuracy: 0.5019\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 4.1923 - accuracy: 0.5011 - val_loss: 4.1292 - val_accuracy: 0.5019\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 4.0916 - accuracy: 0.5028 - val_loss: 4.0298 - val_accuracy: 0.5019\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 3.9929 - accuracy: 0.5042 - val_loss: 3.9326 - val_accuracy: 0.5019\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 3.8965 - accuracy: 0.5030 - val_loss: 3.8377 - val_accuracy: 0.5019\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 3.8025 - accuracy: 0.5041 - val_loss: 3.7453 - val_accuracy: 0.4981\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 3.7109 - accuracy: 0.5039 - val_loss: 3.6554 - val_accuracy: 0.4981\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 3.6218 - accuracy: 0.5040 - val_loss: 3.5679 - val_accuracy: 0.4981\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 3.5350 - accuracy: 0.5034 - val_loss: 3.4828 - val_accuracy: 0.4981\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 3.4504 - accuracy: 0.5048 - val_loss: 3.4001 - val_accuracy: 0.4981\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 3.3673 - accuracy: 0.5093 - val_loss: 3.3197 - val_accuracy: 0.4981\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 3.2843 - accuracy: 0.5118 - val_loss: 3.2412 - val_accuracy: 0.5083\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 3.2032 - accuracy: 0.5259 - val_loss: 3.1658 - val_accuracy: 0.5019\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 3.1244 - accuracy: 0.5864 - val_loss: 3.0925 - val_accuracy: 0.5017\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 3.0485 - accuracy: 0.6018 - val_loss: 3.0196 - val_accuracy: 0.5178\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 2.9732 - accuracy: 0.6099 - val_loss: 2.9493 - val_accuracy: 0.5242\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 2.9001 - accuracy: 0.6175 - val_loss: 2.8758 - val_accuracy: 0.5542\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 2.8298 - accuracy: 0.6218 - val_loss: 2.8077 - val_accuracy: 0.5611\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 2.7617 - accuracy: 0.6255 - val_loss: 2.7391 - val_accuracy: 0.5793\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 2.6949 - accuracy: 0.6301 - val_loss: 2.6715 - val_accuracy: 0.5910\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 2.6303 - accuracy: 0.6336 - val_loss: 2.6067 - val_accuracy: 0.6017\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 2.5675 - accuracy: 0.6371 - val_loss: 2.5422 - val_accuracy: 0.6156\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 2.5057 - accuracy: 0.6409 - val_loss: 2.4824 - val_accuracy: 0.6209\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 2.4460 - accuracy: 0.6454 - val_loss: 2.4245 - val_accuracy: 0.6310\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 2.3882 - accuracy: 0.6475 - val_loss: 2.3714 - val_accuracy: 0.6376\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 2.3341 - accuracy: 0.6497 - val_loss: 2.3213 - val_accuracy: 0.6402\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 2.2816 - accuracy: 0.6482 - val_loss: 2.2715 - val_accuracy: 0.6383\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 2.2299 - accuracy: 0.6509 - val_loss: 2.2214 - val_accuracy: 0.6388\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 2.1809 - accuracy: 0.6514 - val_loss: 2.1746 - val_accuracy: 0.6395\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 2.1326 - accuracy: 0.6526 - val_loss: 2.1249 - val_accuracy: 0.6392\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 2.0881 - accuracy: 0.6521 - val_loss: 2.0807 - val_accuracy: 0.6395\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 2.0420 - accuracy: 0.6529 - val_loss: 2.0350 - val_accuracy: 0.6394\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 1.9985 - accuracy: 0.6540 - val_loss: 1.9930 - val_accuracy: 0.6395\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 1.9561 - accuracy: 0.6532 - val_loss: 1.9506 - val_accuracy: 0.6395\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 1.9150 - accuracy: 0.6560 - val_loss: 1.9107 - val_accuracy: 0.6393\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 1.8748 - accuracy: 0.6552 - val_loss: 1.8718 - val_accuracy: 0.6396\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 1.8364 - accuracy: 0.6551 - val_loss: 1.8334 - val_accuracy: 0.6412\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 1.7976 - accuracy: 0.6575 - val_loss: 1.7963 - val_accuracy: 0.6411\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 1.7614 - accuracy: 0.6574 - val_loss: 1.7605 - val_accuracy: 0.6406\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 1.7261 - accuracy: 0.6568 - val_loss: 1.7262 - val_accuracy: 0.6410\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.6918 - accuracy: 0.6572 - val_loss: 1.6922 - val_accuracy: 0.6409\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 1.6578 - accuracy: 0.6584 - val_loss: 1.6590 - val_accuracy: 0.6402\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 1.6253 - accuracy: 0.6578 - val_loss: 1.6278 - val_accuracy: 0.6409\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 1.5943 - accuracy: 0.6574 - val_loss: 1.5952 - val_accuracy: 0.6397\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 1.5627 - accuracy: 0.6574 - val_loss: 1.5663 - val_accuracy: 0.6397\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 1.5341 - accuracy: 0.6582 - val_loss: 1.5369 - val_accuracy: 0.6405\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 1.5043 - accuracy: 0.6583 - val_loss: 1.5084 - val_accuracy: 0.6409\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 1.4765 - accuracy: 0.6592 - val_loss: 1.4806 - val_accuracy: 0.6410\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 1.4490 - accuracy: 0.6590 - val_loss: 1.4539 - val_accuracy: 0.6409\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 1.4224 - accuracy: 0.6610 - val_loss: 1.4285 - val_accuracy: 0.6411\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.3972 - accuracy: 0.6590 - val_loss: 1.4035 - val_accuracy: 0.6410\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 1.3718 - accuracy: 0.6596 - val_loss: 1.3781 - val_accuracy: 0.6423\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 1.3484 - accuracy: 0.6587 - val_loss: 1.3557 - val_accuracy: 0.6407\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.3248 - accuracy: 0.6601 - val_loss: 1.3317 - val_accuracy: 0.6415\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 1.3024 - accuracy: 0.6587 - val_loss: 1.3103 - val_accuracy: 0.6416\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.2801 - accuracy: 0.6603 - val_loss: 1.2883 - val_accuracy: 0.6428\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 1.2578 - accuracy: 0.6616 - val_loss: 1.2669 - val_accuracy: 0.6424\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 1.2374 - accuracy: 0.6599 - val_loss: 1.2471 - val_accuracy: 0.6426\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 1.2173 - accuracy: 0.6606 - val_loss: 1.2265 - val_accuracy: 0.6426\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 1.1982 - accuracy: 0.6611 - val_loss: 1.2076 - val_accuracy: 0.6422\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 1.1791 - accuracy: 0.6615 - val_loss: 1.1895 - val_accuracy: 0.6425\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 1.1607 - accuracy: 0.6622 - val_loss: 1.1703 - val_accuracy: 0.6428\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 1.1429 - accuracy: 0.6611 - val_loss: 1.1536 - val_accuracy: 0.6430\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 1.1253 - accuracy: 0.6618 - val_loss: 1.1376 - val_accuracy: 0.6430\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 1.1094 - accuracy: 0.6620 - val_loss: 1.1201 - val_accuracy: 0.6423\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 1.0936 - accuracy: 0.6603 - val_loss: 1.1036 - val_accuracy: 0.6422\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 1.0774 - accuracy: 0.6631 - val_loss: 1.0890 - val_accuracy: 0.6426\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.0617 - accuracy: 0.6634 - val_loss: 1.0740 - val_accuracy: 0.6419\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.0478 - accuracy: 0.6608 - val_loss: 1.0596 - val_accuracy: 0.6426\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 1.0330 - accuracy: 0.6638 - val_loss: 1.0461 - val_accuracy: 0.6429\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 1.0195 - accuracy: 0.6644 - val_loss: 1.0325 - val_accuracy: 0.6428\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 1.0064 - accuracy: 0.6624 - val_loss: 1.0160 - val_accuracy: 0.6429\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.9927 - accuracy: 0.6632 - val_loss: 1.0070 - val_accuracy: 0.6423\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.9804 - accuracy: 0.6647 - val_loss: 0.9925 - val_accuracy: 0.6431\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.9681 - accuracy: 0.6634 - val_loss: 0.9818 - val_accuracy: 0.6425\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.9559 - accuracy: 0.6636 - val_loss: 0.9694 - val_accuracy: 0.6430\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.9455 - accuracy: 0.6639 - val_loss: 0.9566 - val_accuracy: 0.6433\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.9335 - accuracy: 0.6640 - val_loss: 0.9484 - val_accuracy: 0.6438\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.9235 - accuracy: 0.6638 - val_loss: 0.9354 - val_accuracy: 0.6430\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.9133 - accuracy: 0.6645 - val_loss: 0.9270 - val_accuracy: 0.6430\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.9039 - accuracy: 0.6639 - val_loss: 0.9152 - val_accuracy: 0.6429\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.8938 - accuracy: 0.6636 - val_loss: 0.9100 - val_accuracy: 0.6434\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.8842 - accuracy: 0.6648 - val_loss: 0.8958 - val_accuracy: 0.6433\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.8750 - accuracy: 0.6644 - val_loss: 0.8914 - val_accuracy: 0.6428\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.8663 - accuracy: 0.6653 - val_loss: 0.8791 - val_accuracy: 0.6438\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.8587 - accuracy: 0.6638 - val_loss: 0.8734 - val_accuracy: 0.6431\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.8507 - accuracy: 0.6652 - val_loss: 0.8627 - val_accuracy: 0.6432\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.8426 - accuracy: 0.6653 - val_loss: 0.8573 - val_accuracy: 0.6433\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.8340 - accuracy: 0.6654 - val_loss: 0.8482 - val_accuracy: 0.6433\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.8272 - accuracy: 0.6641 - val_loss: 0.8411 - val_accuracy: 0.6437\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.8201 - accuracy: 0.6668 - val_loss: 0.8331 - val_accuracy: 0.6433\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.8128 - accuracy: 0.6654 - val_loss: 0.8281 - val_accuracy: 0.6438\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.8058 - accuracy: 0.6664 - val_loss: 0.8196 - val_accuracy: 0.6439\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.7998 - accuracy: 0.6654 - val_loss: 0.8149 - val_accuracy: 0.6435\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.7935 - accuracy: 0.6664 - val_loss: 0.8083 - val_accuracy: 0.6435\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.7868 - accuracy: 0.6674 - val_loss: 0.8023 - val_accuracy: 0.6439\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.7816 - accuracy: 0.6657 - val_loss: 0.7958 - val_accuracy: 0.6437\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.7759 - accuracy: 0.6662 - val_loss: 0.7924 - val_accuracy: 0.6435\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.7708 - accuracy: 0.6655 - val_loss: 0.7849 - val_accuracy: 0.6438\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.7652 - accuracy: 0.6666 - val_loss: 0.7821 - val_accuracy: 0.6440\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.7603 - accuracy: 0.6659 - val_loss: 0.7746 - val_accuracy: 0.6440\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.7555 - accuracy: 0.6668 - val_loss: 0.7710 - val_accuracy: 0.6438\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.7515 - accuracy: 0.6649 - val_loss: 0.7665 - val_accuracy: 0.6438\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.7464 - accuracy: 0.6661 - val_loss: 0.7609 - val_accuracy: 0.6444\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.7417 - accuracy: 0.6669 - val_loss: 0.7592 - val_accuracy: 0.6441\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.7376 - accuracy: 0.6672 - val_loss: 0.7531 - val_accuracy: 0.6440\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.7333 - accuracy: 0.6674 - val_loss: 0.7509 - val_accuracy: 0.6439\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.7294 - accuracy: 0.6680 - val_loss: 0.7451 - val_accuracy: 0.6440\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.7265 - accuracy: 0.6670 - val_loss: 0.7429 - val_accuracy: 0.6442\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.7219 - accuracy: 0.6678 - val_loss: 0.7387 - val_accuracy: 0.6439\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.7182 - accuracy: 0.6670 - val_loss: 0.7359 - val_accuracy: 0.6439\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.7143 - accuracy: 0.6683 - val_loss: 0.7325 - val_accuracy: 0.6441\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.7120 - accuracy: 0.6679 - val_loss: 0.7284 - val_accuracy: 0.6444\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.7085 - accuracy: 0.6681 - val_loss: 0.7262 - val_accuracy: 0.6438\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.7051 - accuracy: 0.6681 - val_loss: 0.7215 - val_accuracy: 0.6441\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.7025 - accuracy: 0.6684 - val_loss: 0.7202 - val_accuracy: 0.6440\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.7002 - accuracy: 0.6667 - val_loss: 0.7162 - val_accuracy: 0.6440\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6968 - accuracy: 0.6676 - val_loss: 0.7123 - val_accuracy: 0.6441\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6940 - accuracy: 0.6685 - val_loss: 0.7136 - val_accuracy: 0.6448\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6924 - accuracy: 0.6668 - val_loss: 0.7078 - val_accuracy: 0.6446\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6889 - accuracy: 0.6682 - val_loss: 0.7083 - val_accuracy: 0.6446\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6865 - accuracy: 0.6687 - val_loss: 0.7043 - val_accuracy: 0.6450\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6844 - accuracy: 0.6685 - val_loss: 0.7020 - val_accuracy: 0.6446\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6823 - accuracy: 0.6679 - val_loss: 0.7016 - val_accuracy: 0.6446\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6807 - accuracy: 0.6672 - val_loss: 0.6981 - val_accuracy: 0.6447\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6780 - accuracy: 0.6680 - val_loss: 0.6970 - val_accuracy: 0.6447\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6759 - accuracy: 0.6684 - val_loss: 0.6954 - val_accuracy: 0.6443\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6734 - accuracy: 0.6693 - val_loss: 0.6933 - val_accuracy: 0.6448\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6727 - accuracy: 0.6678 - val_loss: 0.6913 - val_accuracy: 0.6443\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6699 - accuracy: 0.6689 - val_loss: 0.6902 - val_accuracy: 0.6446\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6693 - accuracy: 0.6683 - val_loss: 0.6866 - val_accuracy: 0.6446\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6675 - accuracy: 0.6691 - val_loss: 0.6884 - val_accuracy: 0.6446\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6660 - accuracy: 0.6678 - val_loss: 0.6828 - val_accuracy: 0.6449\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6643 - accuracy: 0.6693 - val_loss: 0.6839 - val_accuracy: 0.6446\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6623 - accuracy: 0.6690 - val_loss: 0.6809 - val_accuracy: 0.6447\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6608 - accuracy: 0.6694 - val_loss: 0.6805 - val_accuracy: 0.6452\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6596 - accuracy: 0.6681 - val_loss: 0.6798 - val_accuracy: 0.6449\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6585 - accuracy: 0.6689 - val_loss: 0.6767 - val_accuracy: 0.6451\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6571 - accuracy: 0.6685 - val_loss: 0.6768 - val_accuracy: 0.6451\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6557 - accuracy: 0.6681 - val_loss: 0.6760 - val_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6543 - accuracy: 0.6687 - val_loss: 0.6743 - val_accuracy: 0.6449\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6531 - accuracy: 0.6697 - val_loss: 0.6745 - val_accuracy: 0.6449\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6518 - accuracy: 0.6684 - val_loss: 0.6715 - val_accuracy: 0.6447\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6515 - accuracy: 0.6694 - val_loss: 0.6735 - val_accuracy: 0.6455\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6502 - accuracy: 0.6690 - val_loss: 0.6705 - val_accuracy: 0.6449\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6490 - accuracy: 0.6696 - val_loss: 0.6713 - val_accuracy: 0.6447\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6479 - accuracy: 0.6694 - val_loss: 0.6692 - val_accuracy: 0.6446\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6471 - accuracy: 0.6693 - val_loss: 0.6677 - val_accuracy: 0.6449\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6462 - accuracy: 0.6693 - val_loss: 0.6677 - val_accuracy: 0.6449\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6457 - accuracy: 0.6698 - val_loss: 0.6674 - val_accuracy: 0.6451\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6449 - accuracy: 0.6702 - val_loss: 0.6668 - val_accuracy: 0.6448\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6435 - accuracy: 0.6691 - val_loss: 0.6647 - val_accuracy: 0.6453\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6428 - accuracy: 0.6691 - val_loss: 0.6645 - val_accuracy: 0.6447\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6416 - accuracy: 0.6702 - val_loss: 0.6642 - val_accuracy: 0.6452\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6417 - accuracy: 0.6690 - val_loss: 0.6642 - val_accuracy: 0.6452\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6406 - accuracy: 0.6698 - val_loss: 0.6622 - val_accuracy: 0.6450\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6399 - accuracy: 0.6705 - val_loss: 0.6664 - val_accuracy: 0.6456\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6392 - accuracy: 0.6696 - val_loss: 0.6606 - val_accuracy: 0.6453\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6385 - accuracy: 0.6694 - val_loss: 0.6625 - val_accuracy: 0.6456\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6381 - accuracy: 0.6703 - val_loss: 0.6598 - val_accuracy: 0.6453\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6375 - accuracy: 0.6708 - val_loss: 0.6606 - val_accuracy: 0.6455\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6367 - accuracy: 0.6707 - val_loss: 0.6601 - val_accuracy: 0.6457\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6361 - accuracy: 0.6706 - val_loss: 0.6589 - val_accuracy: 0.6453\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6359 - accuracy: 0.6706 - val_loss: 0.6598 - val_accuracy: 0.6446\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6354 - accuracy: 0.6705 - val_loss: 0.6580 - val_accuracy: 0.6459\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6353 - accuracy: 0.6705 - val_loss: 0.6585 - val_accuracy: 0.6451\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6346 - accuracy: 0.6713 - val_loss: 0.6570 - val_accuracy: 0.6449\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6337 - accuracy: 0.6708 - val_loss: 0.6599 - val_accuracy: 0.6448\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6336 - accuracy: 0.6708 - val_loss: 0.6557 - val_accuracy: 0.6450\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6326 - accuracy: 0.6716 - val_loss: 0.6582 - val_accuracy: 0.6441\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6323 - accuracy: 0.6714 - val_loss: 0.6565 - val_accuracy: 0.6449\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6328 - accuracy: 0.6701 - val_loss: 0.6561 - val_accuracy: 0.6451\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6318 - accuracy: 0.6711 - val_loss: 0.6554 - val_accuracy: 0.6443\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6320 - accuracy: 0.6703 - val_loss: 0.6560 - val_accuracy: 0.6460\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6309 - accuracy: 0.6709 - val_loss: 0.6559 - val_accuracy: 0.6461\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6305 - accuracy: 0.6715 - val_loss: 0.6499 - val_accuracy: 0.6444\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6310 - accuracy: 0.6702 - val_loss: 0.6591 - val_accuracy: 0.6458\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6296 - accuracy: 0.6719 - val_loss: 0.6526 - val_accuracy: 0.6455\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 219ms/step - loss: 0.6300 - accuracy: 0.6715 - val_loss: 0.6559 - val_accuracy: 0.6447\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6292 - accuracy: 0.6719 - val_loss: 0.6526 - val_accuracy: 0.6447\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6294 - accuracy: 0.6708 - val_loss: 0.6550 - val_accuracy: 0.6452\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6289 - accuracy: 0.6706 - val_loss: 0.6531 - val_accuracy: 0.6460\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6287 - accuracy: 0.6697 - val_loss: 0.6518 - val_accuracy: 0.6440\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6281 - accuracy: 0.6707 - val_loss: 0.6547 - val_accuracy: 0.6458\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6288 - accuracy: 0.6708 - val_loss: 0.6515 - val_accuracy: 0.6452\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6282 - accuracy: 0.6702 - val_loss: 0.6516 - val_accuracy: 0.6440\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6278 - accuracy: 0.6718 - val_loss: 0.6534 - val_accuracy: 0.6452\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6274 - accuracy: 0.6714 - val_loss: 0.6496 - val_accuracy: 0.6459\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6273 - accuracy: 0.6722 - val_loss: 0.6541 - val_accuracy: 0.6456\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6271 - accuracy: 0.6711 - val_loss: 0.6515 - val_accuracy: 0.6459\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6262 - accuracy: 0.6714 - val_loss: 0.6549 - val_accuracy: 0.6452\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6263 - accuracy: 0.6718 - val_loss: 0.6500 - val_accuracy: 0.6453\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6267 - accuracy: 0.6714 - val_loss: 0.6524 - val_accuracy: 0.6449\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6259 - accuracy: 0.6721 - val_loss: 0.6507 - val_accuracy: 0.6443\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6255 - accuracy: 0.6725 - val_loss: 0.6519 - val_accuracy: 0.6458\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6256 - accuracy: 0.6721 - val_loss: 0.6527 - val_accuracy: 0.6450\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6257 - accuracy: 0.6716 - val_loss: 0.6492 - val_accuracy: 0.6452\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6245 - accuracy: 0.6739 - val_loss: 0.6530 - val_accuracy: 0.6451\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6252 - accuracy: 0.6720 - val_loss: 0.6507 - val_accuracy: 0.6447\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6245 - accuracy: 0.6726 - val_loss: 0.6512 - val_accuracy: 0.6453\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6249 - accuracy: 0.6714 - val_loss: 0.6527 - val_accuracy: 0.6456\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6256 - accuracy: 0.6715 - val_loss: 0.6498 - val_accuracy: 0.6453\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6242 - accuracy: 0.6725 - val_loss: 0.6516 - val_accuracy: 0.6451\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6247 - accuracy: 0.6705 - val_loss: 0.6507 - val_accuracy: 0.6450\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 219ms/step - loss: 0.6244 - accuracy: 0.6725 - val_loss: 0.6500 - val_accuracy: 0.6453\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6246 - accuracy: 0.6718 - val_loss: 0.6524 - val_accuracy: 0.6451\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6242 - accuracy: 0.6726 - val_loss: 0.6492 - val_accuracy: 0.6455\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6241 - accuracy: 0.6733 - val_loss: 0.6485 - val_accuracy: 0.6452\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6238 - accuracy: 0.6723 - val_loss: 0.6533 - val_accuracy: 0.6457\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6237 - accuracy: 0.6726 - val_loss: 0.6492 - val_accuracy: 0.6458\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6238 - accuracy: 0.6723 - val_loss: 0.6494 - val_accuracy: 0.6453\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6236 - accuracy: 0.6723 - val_loss: 0.6485 - val_accuracy: 0.6448\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6232 - accuracy: 0.6729 - val_loss: 0.6510 - val_accuracy: 0.6455\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6239 - accuracy: 0.6719 - val_loss: 0.6509 - val_accuracy: 0.6450\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6232 - accuracy: 0.6727 - val_loss: 0.6495 - val_accuracy: 0.6444\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.6229 - accuracy: 0.6732 - val_loss: 0.6503 - val_accuracy: 0.6446\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6231 - accuracy: 0.6721 - val_loss: 0.6491 - val_accuracy: 0.6456\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6227 - accuracy: 0.6726 - val_loss: 0.6509 - val_accuracy: 0.6446\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6229 - accuracy: 0.6728 - val_loss: 0.6459 - val_accuracy: 0.6452\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6227 - accuracy: 0.6726 - val_loss: 0.6537 - val_accuracy: 0.6446\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6233 - accuracy: 0.6723 - val_loss: 0.6470 - val_accuracy: 0.6458\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6232 - accuracy: 0.6730 - val_loss: 0.6510 - val_accuracy: 0.6451\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6224 - accuracy: 0.6730 - val_loss: 0.6509 - val_accuracy: 0.6452\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6223 - accuracy: 0.6722 - val_loss: 0.6485 - val_accuracy: 0.6449\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6217 - accuracy: 0.6727 - val_loss: 0.6488 - val_accuracy: 0.6451\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6223 - accuracy: 0.6727 - val_loss: 0.6480 - val_accuracy: 0.6449\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6221 - accuracy: 0.6717 - val_loss: 0.6535 - val_accuracy: 0.6448\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6225 - accuracy: 0.6725 - val_loss: 0.6474 - val_accuracy: 0.6455\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6215 - accuracy: 0.6738 - val_loss: 0.6523 - val_accuracy: 0.6446\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6220 - accuracy: 0.6728 - val_loss: 0.6475 - val_accuracy: 0.6450\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6222 - accuracy: 0.6728 - val_loss: 0.6520 - val_accuracy: 0.6458\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6222 - accuracy: 0.6732 - val_loss: 0.6474 - val_accuracy: 0.6449\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6221 - accuracy: 0.6721 - val_loss: 0.6465 - val_accuracy: 0.6450\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6218 - accuracy: 0.6726 - val_loss: 0.6516 - val_accuracy: 0.6453\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6217 - accuracy: 0.6727 - val_loss: 0.6483 - val_accuracy: 0.6453\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6213 - accuracy: 0.6737 - val_loss: 0.6476 - val_accuracy: 0.6452\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6213 - accuracy: 0.6730 - val_loss: 0.6504 - val_accuracy: 0.6457\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.6211 - accuracy: 0.6725 - val_loss: 0.6494 - val_accuracy: 0.6459\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6207 - accuracy: 0.6738 - val_loss: 0.6476 - val_accuracy: 0.6460\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6204 - accuracy: 0.6742 - val_loss: 0.6483 - val_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6214 - accuracy: 0.6735 - val_loss: 0.6496 - val_accuracy: 0.6455\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6206 - accuracy: 0.6738 - val_loss: 0.6494 - val_accuracy: 0.6457\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6208 - accuracy: 0.6736 - val_loss: 0.6511 - val_accuracy: 0.6451\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6210 - accuracy: 0.6738 - val_loss: 0.6458 - val_accuracy: 0.6449\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6211 - accuracy: 0.6728 - val_loss: 0.6492 - val_accuracy: 0.6455\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.6211 - accuracy: 0.6740 - val_loss: 0.6506 - val_accuracy: 0.6457\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6208 - accuracy: 0.6741 - val_loss: 0.6472 - val_accuracy: 0.6455\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6211 - accuracy: 0.6727 - val_loss: 0.6491 - val_accuracy: 0.6455\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6206 - accuracy: 0.6735 - val_loss: 0.6494 - val_accuracy: 0.6457\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6204 - accuracy: 0.6733 - val_loss: 0.6515 - val_accuracy: 0.6453\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6203 - accuracy: 0.6741 - val_loss: 0.6453 - val_accuracy: 0.6448\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6205 - accuracy: 0.6732 - val_loss: 0.6513 - val_accuracy: 0.6448\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6208 - accuracy: 0.6732 - val_loss: 0.6483 - val_accuracy: 0.6452\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6207 - accuracy: 0.6728 - val_loss: 0.6500 - val_accuracy: 0.6456\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6206 - accuracy: 0.6731 - val_loss: 0.6496 - val_accuracy: 0.6451\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6199 - accuracy: 0.6731 - val_loss: 0.6481 - val_accuracy: 0.6452\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6198 - accuracy: 0.6749 - val_loss: 0.6527 - val_accuracy: 0.6449\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6197 - accuracy: 0.6736 - val_loss: 0.6432 - val_accuracy: 0.6449\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6204 - accuracy: 0.6739 - val_loss: 0.6518 - val_accuracy: 0.6452\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6201 - accuracy: 0.6738 - val_loss: 0.6492 - val_accuracy: 0.6450\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6206 - accuracy: 0.6732 - val_loss: 0.6526 - val_accuracy: 0.6452\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6206 - accuracy: 0.6728 - val_loss: 0.6454 - val_accuracy: 0.6437\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6205 - accuracy: 0.6727 - val_loss: 0.6509 - val_accuracy: 0.6443\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6201 - accuracy: 0.6740 - val_loss: 0.6495 - val_accuracy: 0.6450\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6198 - accuracy: 0.6735 - val_loss: 0.6471 - val_accuracy: 0.6447\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6198 - accuracy: 0.6741 - val_loss: 0.6478 - val_accuracy: 0.6451\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6196 - accuracy: 0.6736 - val_loss: 0.6509 - val_accuracy: 0.6447\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6195 - accuracy: 0.6736 - val_loss: 0.6493 - val_accuracy: 0.6442\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6197 - accuracy: 0.6738 - val_loss: 0.6496 - val_accuracy: 0.6447\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6196 - accuracy: 0.6736 - val_loss: 0.6490 - val_accuracy: 0.6443\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6194 - accuracy: 0.6745 - val_loss: 0.6446 - val_accuracy: 0.6457\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6197 - accuracy: 0.6742 - val_loss: 0.6537 - val_accuracy: 0.6446\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.6193 - accuracy: 0.6747 - val_loss: 0.6486 - val_accuracy: 0.6440\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6193 - accuracy: 0.6744 - val_loss: 0.6472 - val_accuracy: 0.6447\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6193 - accuracy: 0.6738 - val_loss: 0.6499 - val_accuracy: 0.6448\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6196 - accuracy: 0.6743 - val_loss: 0.6490 - val_accuracy: 0.6450\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6195 - accuracy: 0.6735 - val_loss: 0.6493 - val_accuracy: 0.6451\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6197 - accuracy: 0.6743 - val_loss: 0.6477 - val_accuracy: 0.6448\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6191 - accuracy: 0.6747 - val_loss: 0.6470 - val_accuracy: 0.6455\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6187 - accuracy: 0.6753 - val_loss: 0.6520 - val_accuracy: 0.6451\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6197 - accuracy: 0.6733 - val_loss: 0.6477 - val_accuracy: 0.6456\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6186 - accuracy: 0.6745 - val_loss: 0.6506 - val_accuracy: 0.6451\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6191 - accuracy: 0.6742 - val_loss: 0.6524 - val_accuracy: 0.6453\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6194 - accuracy: 0.6743 - val_loss: 0.6444 - val_accuracy: 0.6453\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6188 - accuracy: 0.6742 - val_loss: 0.6540 - val_accuracy: 0.6450\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6191 - accuracy: 0.6745 - val_loss: 0.6489 - val_accuracy: 0.6449\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6187 - accuracy: 0.6745 - val_loss: 0.6463 - val_accuracy: 0.6449\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6193 - accuracy: 0.6747 - val_loss: 0.6503 - val_accuracy: 0.6450\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6191 - accuracy: 0.6743 - val_loss: 0.6500 - val_accuracy: 0.6448\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6193 - accuracy: 0.6734 - val_loss: 0.6506 - val_accuracy: 0.6451\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6193 - accuracy: 0.6752 - val_loss: 0.6443 - val_accuracy: 0.6447\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6193 - accuracy: 0.6744 - val_loss: 0.6494 - val_accuracy: 0.6450\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6189 - accuracy: 0.6747 - val_loss: 0.6528 - val_accuracy: 0.6453\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6188 - accuracy: 0.6741 - val_loss: 0.6440 - val_accuracy: 0.6447\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6187 - accuracy: 0.6749 - val_loss: 0.6484 - val_accuracy: 0.6448\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6186 - accuracy: 0.6745 - val_loss: 0.6484 - val_accuracy: 0.6455\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6185 - accuracy: 0.6746 - val_loss: 0.6479 - val_accuracy: 0.6450\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6184 - accuracy: 0.6752 - val_loss: 0.6503 - val_accuracy: 0.6448\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6187 - accuracy: 0.6751 - val_loss: 0.6432 - val_accuracy: 0.6449\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6185 - accuracy: 0.6751 - val_loss: 0.6498 - val_accuracy: 0.6444\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6187 - accuracy: 0.6741 - val_loss: 0.6496 - val_accuracy: 0.6458\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6185 - accuracy: 0.6754 - val_loss: 0.6497 - val_accuracy: 0.6456\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6186 - accuracy: 0.6750 - val_loss: 0.6481 - val_accuracy: 0.6452\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6182 - accuracy: 0.6754 - val_loss: 0.6477 - val_accuracy: 0.6444\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6186 - accuracy: 0.6749 - val_loss: 0.6512 - val_accuracy: 0.6446\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6185 - accuracy: 0.6748 - val_loss: 0.6484 - val_accuracy: 0.6455\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6187 - accuracy: 0.6736 - val_loss: 0.6491 - val_accuracy: 0.6448\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6185 - accuracy: 0.6740 - val_loss: 0.6473 - val_accuracy: 0.6452\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6180 - accuracy: 0.6750 - val_loss: 0.6496 - val_accuracy: 0.6449\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6179 - accuracy: 0.6746 - val_loss: 0.6496 - val_accuracy: 0.6451\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6180 - accuracy: 0.6743 - val_loss: 0.6477 - val_accuracy: 0.6447\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6184 - accuracy: 0.6749 - val_loss: 0.6520 - val_accuracy: 0.6446\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6181 - accuracy: 0.6744 - val_loss: 0.6453 - val_accuracy: 0.6449\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6183 - accuracy: 0.6744 - val_loss: 0.6476 - val_accuracy: 0.6448\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6184 - accuracy: 0.6746 - val_loss: 0.6486 - val_accuracy: 0.6447\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6177 - accuracy: 0.6754 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6183 - accuracy: 0.6745 - val_loss: 0.6467 - val_accuracy: 0.6449\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6182 - accuracy: 0.6751 - val_loss: 0.6484 - val_accuracy: 0.6451\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6179 - accuracy: 0.6753 - val_loss: 0.6494 - val_accuracy: 0.6451\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6182 - accuracy: 0.6743 - val_loss: 0.6520 - val_accuracy: 0.6447\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6177 - accuracy: 0.6762 - val_loss: 0.6483 - val_accuracy: 0.6442\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6180 - accuracy: 0.6745 - val_loss: 0.6505 - val_accuracy: 0.6448\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6184 - accuracy: 0.6745 - val_loss: 0.6449 - val_accuracy: 0.6446\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6181 - accuracy: 0.6748 - val_loss: 0.6465 - val_accuracy: 0.6449\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6181 - accuracy: 0.6740 - val_loss: 0.6561 - val_accuracy: 0.6450\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6185 - accuracy: 0.6749 - val_loss: 0.6463 - val_accuracy: 0.6450\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6179 - accuracy: 0.6750 - val_loss: 0.6455 - val_accuracy: 0.6452\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6178 - accuracy: 0.6747 - val_loss: 0.6529 - val_accuracy: 0.6452\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6175 - accuracy: 0.6758 - val_loss: 0.6479 - val_accuracy: 0.6457\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6174 - accuracy: 0.6752 - val_loss: 0.6491 - val_accuracy: 0.6448\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6179 - accuracy: 0.6757 - val_loss: 0.6509 - val_accuracy: 0.6451\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6169 - accuracy: 0.6764 - val_loss: 0.6458 - val_accuracy: 0.6450\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6173 - accuracy: 0.6760 - val_loss: 0.6512 - val_accuracy: 0.6450\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6171 - accuracy: 0.6756 - val_loss: 0.6471 - val_accuracy: 0.6448\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6176 - accuracy: 0.6744 - val_loss: 0.6522 - val_accuracy: 0.6447\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6172 - accuracy: 0.6761 - val_loss: 0.6493 - val_accuracy: 0.6451\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.6179 - accuracy: 0.6749 - val_loss: 0.6464 - val_accuracy: 0.6444\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6181 - accuracy: 0.6739 - val_loss: 0.6460 - val_accuracy: 0.6447\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6176 - accuracy: 0.6757 - val_loss: 0.6498 - val_accuracy: 0.6448\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6172 - accuracy: 0.6754 - val_loss: 0.6465 - val_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6172 - accuracy: 0.6760 - val_loss: 0.6512 - val_accuracy: 0.6452\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6173 - accuracy: 0.6761 - val_loss: 0.6496 - val_accuracy: 0.6451\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6174 - accuracy: 0.6755 - val_loss: 0.6503 - val_accuracy: 0.6449\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6176 - accuracy: 0.6741 - val_loss: 0.6478 - val_accuracy: 0.6449\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6167 - accuracy: 0.6766 - val_loss: 0.6483 - val_accuracy: 0.6446\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6169 - accuracy: 0.6752 - val_loss: 0.6527 - val_accuracy: 0.6451\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6175 - accuracy: 0.6753 - val_loss: 0.6467 - val_accuracy: 0.6451\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 0.6170 - accuracy: 0.6760 - val_loss: 0.6531 - val_accuracy: 0.6448\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6168 - accuracy: 0.6763 - val_loss: 0.6504 - val_accuracy: 0.6450\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6170 - accuracy: 0.6756 - val_loss: 0.6467 - val_accuracy: 0.6451\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6169 - accuracy: 0.6763 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6170 - accuracy: 0.6759 - val_loss: 0.6482 - val_accuracy: 0.6450\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6168 - accuracy: 0.6763 - val_loss: 0.6529 - val_accuracy: 0.6446\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6173 - accuracy: 0.6753 - val_loss: 0.6524 - val_accuracy: 0.6456\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6171 - accuracy: 0.6755 - val_loss: 0.6461 - val_accuracy: 0.6443\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6167 - accuracy: 0.6762 - val_loss: 0.6495 - val_accuracy: 0.6449\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6170 - accuracy: 0.6748 - val_loss: 0.6508 - val_accuracy: 0.6457\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6169 - accuracy: 0.6762 - val_loss: 0.6529 - val_accuracy: 0.6452\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6168 - accuracy: 0.6770 - val_loss: 0.6480 - val_accuracy: 0.6446\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6168 - accuracy: 0.6767 - val_loss: 0.6476 - val_accuracy: 0.6448\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6167 - accuracy: 0.6751 - val_loss: 0.6545 - val_accuracy: 0.6444\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6170 - accuracy: 0.6758 - val_loss: 0.6479 - val_accuracy: 0.6447\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6171 - accuracy: 0.6749 - val_loss: 0.6488 - val_accuracy: 0.6452\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6171 - accuracy: 0.6754 - val_loss: 0.6492 - val_accuracy: 0.6455\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6167 - accuracy: 0.6757 - val_loss: 0.6483 - val_accuracy: 0.6450\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6165 - accuracy: 0.6759 - val_loss: 0.6516 - val_accuracy: 0.6452\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6168 - accuracy: 0.6768 - val_loss: 0.6442 - val_accuracy: 0.6448\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6171 - accuracy: 0.6754 - val_loss: 0.6523 - val_accuracy: 0.6443\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6168 - accuracy: 0.6759 - val_loss: 0.6517 - val_accuracy: 0.6448\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6162 - accuracy: 0.6766 - val_loss: 0.6463 - val_accuracy: 0.6444\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6166 - accuracy: 0.6762 - val_loss: 0.6522 - val_accuracy: 0.6456\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6162 - accuracy: 0.6759 - val_loss: 0.6482 - val_accuracy: 0.6443\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6161 - accuracy: 0.6765 - val_loss: 0.6487 - val_accuracy: 0.6448\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6160 - accuracy: 0.6775 - val_loss: 0.6491 - val_accuracy: 0.6446\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6166 - accuracy: 0.6759 - val_loss: 0.6486 - val_accuracy: 0.6448\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6169 - accuracy: 0.6749 - val_loss: 0.6522 - val_accuracy: 0.6450\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6163 - accuracy: 0.6763 - val_loss: 0.6497 - val_accuracy: 0.6451\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6161 - accuracy: 0.6762 - val_loss: 0.6533 - val_accuracy: 0.6450\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6164 - accuracy: 0.6766 - val_loss: 0.6492 - val_accuracy: 0.6459\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6167 - accuracy: 0.6763 - val_loss: 0.6494 - val_accuracy: 0.6447\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6162 - accuracy: 0.6766 - val_loss: 0.6469 - val_accuracy: 0.6447\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6163 - accuracy: 0.6761 - val_loss: 0.6516 - val_accuracy: 0.6447\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6165 - accuracy: 0.6768 - val_loss: 0.6487 - val_accuracy: 0.6451\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6161 - accuracy: 0.6765 - val_loss: 0.6496 - val_accuracy: 0.6456\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6165 - accuracy: 0.6762 - val_loss: 0.6442 - val_accuracy: 0.6451\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6162 - accuracy: 0.6764 - val_loss: 0.6517 - val_accuracy: 0.6450\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6163 - accuracy: 0.6758 - val_loss: 0.6459 - val_accuracy: 0.6453\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6161 - accuracy: 0.6764 - val_loss: 0.6462 - val_accuracy: 0.6450\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6163 - accuracy: 0.6770 - val_loss: 0.6483 - val_accuracy: 0.6451\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6166 - accuracy: 0.6760 - val_loss: 0.6492 - val_accuracy: 0.6458\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6160 - accuracy: 0.6766 - val_loss: 0.6518 - val_accuracy: 0.6456\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6160 - accuracy: 0.6770 - val_loss: 0.6442 - val_accuracy: 0.6444\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6161 - accuracy: 0.6763 - val_loss: 0.6491 - val_accuracy: 0.6446\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6165 - accuracy: 0.6761 - val_loss: 0.6538 - val_accuracy: 0.6449\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6161 - accuracy: 0.6761 - val_loss: 0.6503 - val_accuracy: 0.6451\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6163 - accuracy: 0.6759 - val_loss: 0.6437 - val_accuracy: 0.6459\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6166 - accuracy: 0.6760 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6162 - accuracy: 0.6754 - val_loss: 0.6492 - val_accuracy: 0.6452\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 0.6158 - accuracy: 0.6772 - val_loss: 0.6506 - val_accuracy: 0.6459\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6157 - accuracy: 0.6770 - val_loss: 0.6486 - val_accuracy: 0.6459\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6162 - accuracy: 0.6762 - val_loss: 0.6484 - val_accuracy: 0.6450\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6158 - accuracy: 0.6762 - val_loss: 0.6491 - val_accuracy: 0.6444\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6166 - accuracy: 0.6758 - val_loss: 0.6545 - val_accuracy: 0.6447\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6162 - accuracy: 0.6761 - val_loss: 0.6474 - val_accuracy: 0.6452\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6161 - accuracy: 0.6765 - val_loss: 0.6475 - val_accuracy: 0.6451\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6161 - accuracy: 0.6766 - val_loss: 0.6548 - val_accuracy: 0.6452\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6157 - accuracy: 0.6768 - val_loss: 0.6494 - val_accuracy: 0.6451\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6153 - accuracy: 0.6768 - val_loss: 0.6488 - val_accuracy: 0.6457\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6156 - accuracy: 0.6776 - val_loss: 0.6474 - val_accuracy: 0.6455\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6155 - accuracy: 0.6771 - val_loss: 0.6497 - val_accuracy: 0.6453\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6157 - accuracy: 0.6767 - val_loss: 0.6494 - val_accuracy: 0.6456\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6156 - accuracy: 0.6771 - val_loss: 0.6517 - val_accuracy: 0.6448\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6156 - accuracy: 0.6765 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6156 - accuracy: 0.6764 - val_loss: 0.6462 - val_accuracy: 0.6449\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6153 - accuracy: 0.6779 - val_loss: 0.6509 - val_accuracy: 0.6453\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6152 - accuracy: 0.6770 - val_loss: 0.6526 - val_accuracy: 0.6448\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6157 - accuracy: 0.6765 - val_loss: 0.6481 - val_accuracy: 0.6453\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6158 - accuracy: 0.6769 - val_loss: 0.6479 - val_accuracy: 0.6452\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6156 - accuracy: 0.6759 - val_loss: 0.6448 - val_accuracy: 0.6452\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6159 - accuracy: 0.6776 - val_loss: 0.6548 - val_accuracy: 0.6455\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6157 - accuracy: 0.6772 - val_loss: 0.6520 - val_accuracy: 0.6447\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6165 - accuracy: 0.6765 - val_loss: 0.6540 - val_accuracy: 0.6448\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6155 - accuracy: 0.6764 - val_loss: 0.6480 - val_accuracy: 0.6453\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6163 - accuracy: 0.6775 - val_loss: 0.6424 - val_accuracy: 0.6456\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6160 - accuracy: 0.6777 - val_loss: 0.6527 - val_accuracy: 0.6456\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6152 - accuracy: 0.6775 - val_loss: 0.6513 - val_accuracy: 0.6450\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6149 - accuracy: 0.6777 - val_loss: 0.6509 - val_accuracy: 0.6452\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6154 - accuracy: 0.6764 - val_loss: 0.6486 - val_accuracy: 0.6452\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6149 - accuracy: 0.6776 - val_loss: 0.6522 - val_accuracy: 0.6451\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6148 - accuracy: 0.6786 - val_loss: 0.6489 - val_accuracy: 0.6452\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6149 - accuracy: 0.6787 - val_loss: 0.6502 - val_accuracy: 0.6453\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6151 - accuracy: 0.6781 - val_loss: 0.6494 - val_accuracy: 0.6452\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6159 - accuracy: 0.6765 - val_loss: 0.6519 - val_accuracy: 0.6455\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6150 - accuracy: 0.6764 - val_loss: 0.6497 - val_accuracy: 0.6456\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6153 - accuracy: 0.6770 - val_loss: 0.6511 - val_accuracy: 0.6451\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6155 - accuracy: 0.6771 - val_loss: 0.6460 - val_accuracy: 0.6453\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6151 - accuracy: 0.6778 - val_loss: 0.6519 - val_accuracy: 0.6459\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6157 - accuracy: 0.6769 - val_loss: 0.6533 - val_accuracy: 0.6459\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6154 - accuracy: 0.6780 - val_loss: 0.6540 - val_accuracy: 0.6446\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6154 - accuracy: 0.6766 - val_loss: 0.6468 - val_accuracy: 0.6451\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6150 - accuracy: 0.6776 - val_loss: 0.6505 - val_accuracy: 0.6451\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6150 - accuracy: 0.6777 - val_loss: 0.6520 - val_accuracy: 0.6458\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6149 - accuracy: 0.6774 - val_loss: 0.6492 - val_accuracy: 0.6455\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6150 - accuracy: 0.6771 - val_loss: 0.6501 - val_accuracy: 0.6456\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6143 - accuracy: 0.6781 - val_loss: 0.6498 - val_accuracy: 0.6448\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6151 - accuracy: 0.6788 - val_loss: 0.6487 - val_accuracy: 0.6448\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.6144 - accuracy: 0.6782 - val_loss: 0.6523 - val_accuracy: 0.6452\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6146 - accuracy: 0.6783 - val_loss: 0.6447 - val_accuracy: 0.6448\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6148 - accuracy: 0.6784 - val_loss: 0.6519 - val_accuracy: 0.6451\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6150 - accuracy: 0.6767 - val_loss: 0.6508 - val_accuracy: 0.6451\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6146 - accuracy: 0.6766 - val_loss: 0.6514 - val_accuracy: 0.6455\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6150 - accuracy: 0.6776 - val_loss: 0.6490 - val_accuracy: 0.6453\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6143 - accuracy: 0.6779 - val_loss: 0.6513 - val_accuracy: 0.6451\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6145 - accuracy: 0.6769 - val_loss: 0.6541 - val_accuracy: 0.6452\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6149 - accuracy: 0.6785 - val_loss: 0.6510 - val_accuracy: 0.6451\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6143 - accuracy: 0.6770 - val_loss: 0.6494 - val_accuracy: 0.6462\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6152 - accuracy: 0.6766 - val_loss: 0.6518 - val_accuracy: 0.6450\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6148 - accuracy: 0.6773 - val_loss: 0.6476 - val_accuracy: 0.6449\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6152 - accuracy: 0.6781 - val_loss: 0.6556 - val_accuracy: 0.6452\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6150 - accuracy: 0.6772 - val_loss: 0.6429 - val_accuracy: 0.6455\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6153 - accuracy: 0.6780 - val_loss: 0.6478 - val_accuracy: 0.6456\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6148 - accuracy: 0.6770 - val_loss: 0.6538 - val_accuracy: 0.6455\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6151 - accuracy: 0.6777 - val_loss: 0.6570 - val_accuracy: 0.6453\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6156 - accuracy: 0.6780 - val_loss: 0.6434 - val_accuracy: 0.6455\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6148 - accuracy: 0.6776 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6152 - accuracy: 0.6772 - val_loss: 0.6502 - val_accuracy: 0.6461\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6146 - accuracy: 0.6777 - val_loss: 0.6541 - val_accuracy: 0.6458\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6147 - accuracy: 0.6780 - val_loss: 0.6487 - val_accuracy: 0.6458\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6149 - accuracy: 0.6772 - val_loss: 0.6489 - val_accuracy: 0.6449\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6145 - accuracy: 0.6788 - val_loss: 0.6544 - val_accuracy: 0.6448\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6144 - accuracy: 0.6773 - val_loss: 0.6468 - val_accuracy: 0.6460\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6140 - accuracy: 0.6781 - val_loss: 0.6500 - val_accuracy: 0.6458\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.6146 - accuracy: 0.6768 - val_loss: 0.6474 - val_accuracy: 0.6457\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6147 - accuracy: 0.6777 - val_loss: 0.6498 - val_accuracy: 0.6456\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6146 - accuracy: 0.6776 - val_loss: 0.6523 - val_accuracy: 0.6452\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6147 - accuracy: 0.6783 - val_loss: 0.6516 - val_accuracy: 0.6452\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6138 - accuracy: 0.6787 - val_loss: 0.6488 - val_accuracy: 0.6457\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6146 - accuracy: 0.6777 - val_loss: 0.6465 - val_accuracy: 0.6457\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6144 - accuracy: 0.6779 - val_loss: 0.6565 - val_accuracy: 0.6455\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6141 - accuracy: 0.6784 - val_loss: 0.6487 - val_accuracy: 0.6451\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6143 - accuracy: 0.6777 - val_loss: 0.6497 - val_accuracy: 0.6457\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.6147 - accuracy: 0.6778 - val_loss: 0.6477 - val_accuracy: 0.6449\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6145 - accuracy: 0.6782 - val_loss: 0.6534 - val_accuracy: 0.6450\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6141 - accuracy: 0.6783 - val_loss: 0.6500 - val_accuracy: 0.6451\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.6142 - accuracy: 0.6780 - val_loss: 0.6486 - val_accuracy: 0.6450\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6144 - accuracy: 0.6767 - val_loss: 0.6467 - val_accuracy: 0.6452\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6137 - accuracy: 0.6784 - val_loss: 0.6534 - val_accuracy: 0.6453\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6144 - accuracy: 0.6782 - val_loss: 0.6523 - val_accuracy: 0.6461\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 0.6150 - accuracy: 0.6773 - val_loss: 0.6434 - val_accuracy: 0.6461\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6144 - accuracy: 0.6781 - val_loss: 0.6497 - val_accuracy: 0.6457\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6138 - accuracy: 0.6788 - val_loss: 0.6486 - val_accuracy: 0.6456\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6140 - accuracy: 0.6784 - val_loss: 0.6538 - val_accuracy: 0.6461\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6141 - accuracy: 0.6791 - val_loss: 0.6482 - val_accuracy: 0.6460\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6140 - accuracy: 0.6785 - val_loss: 0.6505 - val_accuracy: 0.6458\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 0.6139 - accuracy: 0.6791 - val_loss: 0.6465 - val_accuracy: 0.6452\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.6143 - accuracy: 0.6777 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6138 - accuracy: 0.6782 - val_loss: 0.6521 - val_accuracy: 0.6444\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 0.6145 - accuracy: 0.6785 - val_loss: 0.6479 - val_accuracy: 0.6450\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 0.6141 - accuracy: 0.6781 - val_loss: 0.6518 - val_accuracy: 0.6448\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.6136 - accuracy: 0.6788 - val_loss: 0.6531 - val_accuracy: 0.6462\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    column_model.fit(train_ds, epochs=100, validation_data=val_ds)\n",
    "    column_model.optimizer.lr.assign(column_model.optimizer.lr * 0.99)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.7136 - accuracy: 0.6637 - val_loss: 0.7291 - val_accuracy: 0.6434\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.7107 - accuracy: 0.6645 - val_loss: 0.7252 - val_accuracy: 0.6437\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.7083 - accuracy: 0.6636 - val_loss: 0.7253 - val_accuracy: 0.6438\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.7058 - accuracy: 0.6647 - val_loss: 0.7207 - val_accuracy: 0.6440\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.7032 - accuracy: 0.6649 - val_loss: 0.7200 - val_accuracy: 0.6437\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.7007 - accuracy: 0.6658 - val_loss: 0.7182 - val_accuracy: 0.6437\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.6985 - accuracy: 0.6652 - val_loss: 0.7146 - val_accuracy: 0.6435\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6963 - accuracy: 0.6652 - val_loss: 0.7130 - val_accuracy: 0.6437\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6947 - accuracy: 0.6662 - val_loss: 0.7113 - val_accuracy: 0.6433\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6923 - accuracy: 0.6648 - val_loss: 0.7090 - val_accuracy: 0.6435\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6910 - accuracy: 0.6651 - val_loss: 0.7073 - val_accuracy: 0.6433\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6888 - accuracy: 0.6643 - val_loss: 0.7051 - val_accuracy: 0.6431\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6869 - accuracy: 0.6656 - val_loss: 0.7033 - val_accuracy: 0.6432\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6847 - accuracy: 0.6648 - val_loss: 0.7019 - val_accuracy: 0.6433\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6832 - accuracy: 0.6664 - val_loss: 0.6997 - val_accuracy: 0.6434\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 0.6811 - accuracy: 0.6659 - val_loss: 0.6986 - val_accuracy: 0.6438\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6795 - accuracy: 0.6661 - val_loss: 0.6969 - val_accuracy: 0.6438\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6781 - accuracy: 0.6661 - val_loss: 0.6954 - val_accuracy: 0.6433\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6769 - accuracy: 0.6671 - val_loss: 0.6938 - val_accuracy: 0.6439\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6745 - accuracy: 0.6664 - val_loss: 0.6929 - val_accuracy: 0.6433\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6736 - accuracy: 0.6656 - val_loss: 0.6917 - val_accuracy: 0.6433\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6722 - accuracy: 0.6662 - val_loss: 0.6892 - val_accuracy: 0.6439\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6699 - accuracy: 0.6667 - val_loss: 0.6888 - val_accuracy: 0.6435\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6690 - accuracy: 0.6661 - val_loss: 0.6870 - val_accuracy: 0.6441\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.6681 - accuracy: 0.6657 - val_loss: 0.6858 - val_accuracy: 0.6439\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6668 - accuracy: 0.6660 - val_loss: 0.6840 - val_accuracy: 0.6441\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6656 - accuracy: 0.6666 - val_loss: 0.6831 - val_accuracy: 0.6437\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6635 - accuracy: 0.6672 - val_loss: 0.6834 - val_accuracy: 0.6440\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6625 - accuracy: 0.6674 - val_loss: 0.6807 - val_accuracy: 0.6438\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6612 - accuracy: 0.6679 - val_loss: 0.6811 - val_accuracy: 0.6440\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6606 - accuracy: 0.6667 - val_loss: 0.6783 - val_accuracy: 0.6441\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6597 - accuracy: 0.6669 - val_loss: 0.6791 - val_accuracy: 0.6442\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6582 - accuracy: 0.6668 - val_loss: 0.6763 - val_accuracy: 0.6442\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6573 - accuracy: 0.6668 - val_loss: 0.6761 - val_accuracy: 0.6442\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6566 - accuracy: 0.6667 - val_loss: 0.6750 - val_accuracy: 0.6446\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6554 - accuracy: 0.6674 - val_loss: 0.6737 - val_accuracy: 0.6446\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6550 - accuracy: 0.6672 - val_loss: 0.6723 - val_accuracy: 0.6441\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6535 - accuracy: 0.6666 - val_loss: 0.6734 - val_accuracy: 0.6441\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6537 - accuracy: 0.6660 - val_loss: 0.6708 - val_accuracy: 0.6442\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6521 - accuracy: 0.6668 - val_loss: 0.6713 - val_accuracy: 0.6446\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6517 - accuracy: 0.6673 - val_loss: 0.6698 - val_accuracy: 0.6448\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6508 - accuracy: 0.6676 - val_loss: 0.6690 - val_accuracy: 0.6447\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6501 - accuracy: 0.6668 - val_loss: 0.6696 - val_accuracy: 0.6443\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6487 - accuracy: 0.6677 - val_loss: 0.6684 - val_accuracy: 0.6446\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6474 - accuracy: 0.6681 - val_loss: 0.6681 - val_accuracy: 0.6450\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6469 - accuracy: 0.6684 - val_loss: 0.6677 - val_accuracy: 0.6448\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6468 - accuracy: 0.6668 - val_loss: 0.6663 - val_accuracy: 0.6450\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6458 - accuracy: 0.6679 - val_loss: 0.6658 - val_accuracy: 0.6449\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6454 - accuracy: 0.6669 - val_loss: 0.6649 - val_accuracy: 0.6449\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6449 - accuracy: 0.6675 - val_loss: 0.6647 - val_accuracy: 0.6449\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6445 - accuracy: 0.6678 - val_loss: 0.6630 - val_accuracy: 0.6443\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6439 - accuracy: 0.6670 - val_loss: 0.6636 - val_accuracy: 0.6455\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6433 - accuracy: 0.6677 - val_loss: 0.6611 - val_accuracy: 0.6456\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.6417 - accuracy: 0.6684 - val_loss: 0.6650 - val_accuracy: 0.6447\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6421 - accuracy: 0.6682 - val_loss: 0.6606 - val_accuracy: 0.6446\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6408 - accuracy: 0.6684 - val_loss: 0.6625 - val_accuracy: 0.6450\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6409 - accuracy: 0.6685 - val_loss: 0.6602 - val_accuracy: 0.6453\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6409 - accuracy: 0.6685 - val_loss: 0.6596 - val_accuracy: 0.6457\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6398 - accuracy: 0.6683 - val_loss: 0.6611 - val_accuracy: 0.6452\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6395 - accuracy: 0.6683 - val_loss: 0.6586 - val_accuracy: 0.6447\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6384 - accuracy: 0.6681 - val_loss: 0.6608 - val_accuracy: 0.6446\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6384 - accuracy: 0.6697 - val_loss: 0.6581 - val_accuracy: 0.6455\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6383 - accuracy: 0.6688 - val_loss: 0.6580 - val_accuracy: 0.6453\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6375 - accuracy: 0.6695 - val_loss: 0.6583 - val_accuracy: 0.6452\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.6374 - accuracy: 0.6686 - val_loss: 0.6583 - val_accuracy: 0.6441\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6366 - accuracy: 0.6683 - val_loss: 0.6567 - val_accuracy: 0.6443\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6359 - accuracy: 0.6684 - val_loss: 0.6568 - val_accuracy: 0.6450\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6361 - accuracy: 0.6682 - val_loss: 0.6562 - val_accuracy: 0.6451\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6354 - accuracy: 0.6696 - val_loss: 0.6562 - val_accuracy: 0.6458\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6352 - accuracy: 0.6696 - val_loss: 0.6561 - val_accuracy: 0.6452\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6357 - accuracy: 0.6690 - val_loss: 0.6543 - val_accuracy: 0.6455\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6352 - accuracy: 0.6686 - val_loss: 0.6565 - val_accuracy: 0.6451\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6337 - accuracy: 0.6684 - val_loss: 0.6556 - val_accuracy: 0.6451\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6341 - accuracy: 0.6700 - val_loss: 0.6542 - val_accuracy: 0.6450\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6340 - accuracy: 0.6695 - val_loss: 0.6545 - val_accuracy: 0.6452\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6340 - accuracy: 0.6689 - val_loss: 0.6541 - val_accuracy: 0.6448\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6338 - accuracy: 0.6683 - val_loss: 0.6547 - val_accuracy: 0.6448\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6326 - accuracy: 0.6691 - val_loss: 0.6543 - val_accuracy: 0.6448\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6329 - accuracy: 0.6692 - val_loss: 0.6526 - val_accuracy: 0.6456\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6329 - accuracy: 0.6688 - val_loss: 0.6548 - val_accuracy: 0.6449\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6321 - accuracy: 0.6695 - val_loss: 0.6524 - val_accuracy: 0.6455\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6320 - accuracy: 0.6688 - val_loss: 0.6538 - val_accuracy: 0.6448\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6319 - accuracy: 0.6686 - val_loss: 0.6527 - val_accuracy: 0.6447\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6312 - accuracy: 0.6685 - val_loss: 0.6536 - val_accuracy: 0.6450\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6310 - accuracy: 0.6702 - val_loss: 0.6517 - val_accuracy: 0.6455\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6306 - accuracy: 0.6700 - val_loss: 0.6531 - val_accuracy: 0.6447\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6310 - accuracy: 0.6703 - val_loss: 0.6521 - val_accuracy: 0.6452\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6307 - accuracy: 0.6690 - val_loss: 0.6523 - val_accuracy: 0.6452\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6305 - accuracy: 0.6701 - val_loss: 0.6512 - val_accuracy: 0.6450\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6300 - accuracy: 0.6694 - val_loss: 0.6525 - val_accuracy: 0.6446\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6302 - accuracy: 0.6691 - val_loss: 0.6507 - val_accuracy: 0.6452\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6295 - accuracy: 0.6692 - val_loss: 0.6525 - val_accuracy: 0.6451\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6297 - accuracy: 0.6695 - val_loss: 0.6504 - val_accuracy: 0.6451\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6290 - accuracy: 0.6699 - val_loss: 0.6517 - val_accuracy: 0.6453\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6290 - accuracy: 0.6703 - val_loss: 0.6505 - val_accuracy: 0.6451\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6298 - accuracy: 0.6694 - val_loss: 0.6500 - val_accuracy: 0.6449\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6294 - accuracy: 0.6698 - val_loss: 0.6517 - val_accuracy: 0.6451\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6283 - accuracy: 0.6706 - val_loss: 0.6505 - val_accuracy: 0.6457\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6282 - accuracy: 0.6708 - val_loss: 0.6514 - val_accuracy: 0.6453\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6289 - accuracy: 0.6698 - val_loss: 0.6490 - val_accuracy: 0.6455\n",
      "Epoch:  1\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6281 - accuracy: 0.6710 - val_loss: 0.6512 - val_accuracy: 0.6451\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6278 - accuracy: 0.6698 - val_loss: 0.6490 - val_accuracy: 0.6455\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6279 - accuracy: 0.6706 - val_loss: 0.6503 - val_accuracy: 0.6455\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6277 - accuracy: 0.6700 - val_loss: 0.6505 - val_accuracy: 0.6452\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6278 - accuracy: 0.6715 - val_loss: 0.6498 - val_accuracy: 0.6452\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6281 - accuracy: 0.6699 - val_loss: 0.6486 - val_accuracy: 0.6456\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6270 - accuracy: 0.6713 - val_loss: 0.6506 - val_accuracy: 0.6452\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6270 - accuracy: 0.6700 - val_loss: 0.6486 - val_accuracy: 0.6452\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6268 - accuracy: 0.6710 - val_loss: 0.6507 - val_accuracy: 0.6451\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6276 - accuracy: 0.6704 - val_loss: 0.6471 - val_accuracy: 0.6451\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6268 - accuracy: 0.6714 - val_loss: 0.6505 - val_accuracy: 0.6450\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6268 - accuracy: 0.6696 - val_loss: 0.6488 - val_accuracy: 0.6451\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6266 - accuracy: 0.6710 - val_loss: 0.6488 - val_accuracy: 0.6449\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6270 - accuracy: 0.6708 - val_loss: 0.6486 - val_accuracy: 0.6450\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6265 - accuracy: 0.6700 - val_loss: 0.6490 - val_accuracy: 0.6450\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6265 - accuracy: 0.6710 - val_loss: 0.6489 - val_accuracy: 0.6449\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6260 - accuracy: 0.6708 - val_loss: 0.6500 - val_accuracy: 0.6449\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6262 - accuracy: 0.6707 - val_loss: 0.6478 - val_accuracy: 0.6450\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6261 - accuracy: 0.6714 - val_loss: 0.6492 - val_accuracy: 0.6456\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6261 - accuracy: 0.6716 - val_loss: 0.6491 - val_accuracy: 0.6448\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6261 - accuracy: 0.6708 - val_loss: 0.6489 - val_accuracy: 0.6446\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6255 - accuracy: 0.6710 - val_loss: 0.6486 - val_accuracy: 0.6451\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6261 - accuracy: 0.6705 - val_loss: 0.6476 - val_accuracy: 0.6450\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6257 - accuracy: 0.6711 - val_loss: 0.6477 - val_accuracy: 0.6451\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6263 - accuracy: 0.6712 - val_loss: 0.6473 - val_accuracy: 0.6449\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6253 - accuracy: 0.6711 - val_loss: 0.6494 - val_accuracy: 0.6449\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6252 - accuracy: 0.6713 - val_loss: 0.6471 - val_accuracy: 0.6451\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6253 - accuracy: 0.6713 - val_loss: 0.6488 - val_accuracy: 0.6453\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6250 - accuracy: 0.6716 - val_loss: 0.6471 - val_accuracy: 0.6449\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6256 - accuracy: 0.6722 - val_loss: 0.6488 - val_accuracy: 0.6450\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6253 - accuracy: 0.6702 - val_loss: 0.6465 - val_accuracy: 0.6450\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6254 - accuracy: 0.6720 - val_loss: 0.6493 - val_accuracy: 0.6450\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6250 - accuracy: 0.6706 - val_loss: 0.6472 - val_accuracy: 0.6446\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6248 - accuracy: 0.6717 - val_loss: 0.6487 - val_accuracy: 0.6446\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6245 - accuracy: 0.6712 - val_loss: 0.6472 - val_accuracy: 0.6451\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6239 - accuracy: 0.6726 - val_loss: 0.6496 - val_accuracy: 0.6450\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6248 - accuracy: 0.6708 - val_loss: 0.6464 - val_accuracy: 0.6450\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6242 - accuracy: 0.6719 - val_loss: 0.6478 - val_accuracy: 0.6443\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6246 - accuracy: 0.6712 - val_loss: 0.6464 - val_accuracy: 0.6450\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6243 - accuracy: 0.6713 - val_loss: 0.6479 - val_accuracy: 0.6452\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6247 - accuracy: 0.6709 - val_loss: 0.6472 - val_accuracy: 0.6447\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6241 - accuracy: 0.6712 - val_loss: 0.6471 - val_accuracy: 0.6448\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6244 - accuracy: 0.6722 - val_loss: 0.6470 - val_accuracy: 0.6453\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6236 - accuracy: 0.6722 - val_loss: 0.6488 - val_accuracy: 0.6447\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6245 - accuracy: 0.6714 - val_loss: 0.6480 - val_accuracy: 0.6442\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6238 - accuracy: 0.6715 - val_loss: 0.6495 - val_accuracy: 0.6453\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6240 - accuracy: 0.6710 - val_loss: 0.6467 - val_accuracy: 0.6453\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6244 - accuracy: 0.6715 - val_loss: 0.6478 - val_accuracy: 0.6447\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6232 - accuracy: 0.6728 - val_loss: 0.6474 - val_accuracy: 0.6455\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6235 - accuracy: 0.6730 - val_loss: 0.6476 - val_accuracy: 0.6453\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6234 - accuracy: 0.6721 - val_loss: 0.6465 - val_accuracy: 0.6452\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6232 - accuracy: 0.6718 - val_loss: 0.6485 - val_accuracy: 0.6449\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6242 - accuracy: 0.6714 - val_loss: 0.6460 - val_accuracy: 0.6450\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6240 - accuracy: 0.6724 - val_loss: 0.6481 - val_accuracy: 0.6450\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6236 - accuracy: 0.6721 - val_loss: 0.6483 - val_accuracy: 0.6450\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6239 - accuracy: 0.6718 - val_loss: 0.6462 - val_accuracy: 0.6448\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6229 - accuracy: 0.6724 - val_loss: 0.6490 - val_accuracy: 0.6452\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6235 - accuracy: 0.6708 - val_loss: 0.6453 - val_accuracy: 0.6447\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6235 - accuracy: 0.6712 - val_loss: 0.6490 - val_accuracy: 0.6447\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6233 - accuracy: 0.6727 - val_loss: 0.6468 - val_accuracy: 0.6448\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6229 - accuracy: 0.6716 - val_loss: 0.6485 - val_accuracy: 0.6446\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6230 - accuracy: 0.6715 - val_loss: 0.6455 - val_accuracy: 0.6448\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6230 - accuracy: 0.6718 - val_loss: 0.6471 - val_accuracy: 0.6450\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6227 - accuracy: 0.6712 - val_loss: 0.6480 - val_accuracy: 0.6450\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6222 - accuracy: 0.6724 - val_loss: 0.6474 - val_accuracy: 0.6451\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6229 - accuracy: 0.6724 - val_loss: 0.6474 - val_accuracy: 0.6448\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6228 - accuracy: 0.6721 - val_loss: 0.6480 - val_accuracy: 0.6442\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6230 - accuracy: 0.6724 - val_loss: 0.6456 - val_accuracy: 0.6449\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6230 - accuracy: 0.6714 - val_loss: 0.6490 - val_accuracy: 0.6450\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6221 - accuracy: 0.6727 - val_loss: 0.6459 - val_accuracy: 0.6448\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6228 - accuracy: 0.6716 - val_loss: 0.6475 - val_accuracy: 0.6448\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6220 - accuracy: 0.6727 - val_loss: 0.6477 - val_accuracy: 0.6452\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6223 - accuracy: 0.6725 - val_loss: 0.6457 - val_accuracy: 0.6446\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6219 - accuracy: 0.6727 - val_loss: 0.6490 - val_accuracy: 0.6452\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6222 - accuracy: 0.6723 - val_loss: 0.6446 - val_accuracy: 0.6449\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6218 - accuracy: 0.6716 - val_loss: 0.6502 - val_accuracy: 0.6443\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6222 - accuracy: 0.6726 - val_loss: 0.6461 - val_accuracy: 0.6453\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6224 - accuracy: 0.6724 - val_loss: 0.6478 - val_accuracy: 0.6447\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6218 - accuracy: 0.6730 - val_loss: 0.6482 - val_accuracy: 0.6448\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6226 - accuracy: 0.6726 - val_loss: 0.6449 - val_accuracy: 0.6451\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6220 - accuracy: 0.6725 - val_loss: 0.6486 - val_accuracy: 0.6451\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6224 - accuracy: 0.6730 - val_loss: 0.6443 - val_accuracy: 0.6452\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6224 - accuracy: 0.6726 - val_loss: 0.6485 - val_accuracy: 0.6450\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6220 - accuracy: 0.6723 - val_loss: 0.6459 - val_accuracy: 0.6451\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6223 - accuracy: 0.6717 - val_loss: 0.6467 - val_accuracy: 0.6442\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6218 - accuracy: 0.6725 - val_loss: 0.6461 - val_accuracy: 0.6448\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6212 - accuracy: 0.6737 - val_loss: 0.6477 - val_accuracy: 0.6449\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6216 - accuracy: 0.6733 - val_loss: 0.6461 - val_accuracy: 0.6452\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6218 - accuracy: 0.6718 - val_loss: 0.6465 - val_accuracy: 0.6456\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6217 - accuracy: 0.6725 - val_loss: 0.6469 - val_accuracy: 0.6446\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6217 - accuracy: 0.6729 - val_loss: 0.6455 - val_accuracy: 0.6448\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6215 - accuracy: 0.6726 - val_loss: 0.6474 - val_accuracy: 0.6452\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6214 - accuracy: 0.6728 - val_loss: 0.6470 - val_accuracy: 0.6451\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6218 - accuracy: 0.6723 - val_loss: 0.6472 - val_accuracy: 0.6444\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6213 - accuracy: 0.6731 - val_loss: 0.6467 - val_accuracy: 0.6449\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6216 - accuracy: 0.6735 - val_loss: 0.6474 - val_accuracy: 0.6448\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6211 - accuracy: 0.6733 - val_loss: 0.6461 - val_accuracy: 0.6453\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6205 - accuracy: 0.6736 - val_loss: 0.6491 - val_accuracy: 0.6452\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6212 - accuracy: 0.6730 - val_loss: 0.6449 - val_accuracy: 0.6449\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6214 - accuracy: 0.6732 - val_loss: 0.6469 - val_accuracy: 0.6446\n",
      "Epoch:  2\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6210 - accuracy: 0.6733 - val_loss: 0.6482 - val_accuracy: 0.6449\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6216 - accuracy: 0.6720 - val_loss: 0.6457 - val_accuracy: 0.6453\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6210 - accuracy: 0.6728 - val_loss: 0.6470 - val_accuracy: 0.6453\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6211 - accuracy: 0.6733 - val_loss: 0.6464 - val_accuracy: 0.6451\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6205 - accuracy: 0.6739 - val_loss: 0.6463 - val_accuracy: 0.6449\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6212 - accuracy: 0.6730 - val_loss: 0.6469 - val_accuracy: 0.6444\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6209 - accuracy: 0.6733 - val_loss: 0.6451 - val_accuracy: 0.6450\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6203 - accuracy: 0.6734 - val_loss: 0.6499 - val_accuracy: 0.6446\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6209 - accuracy: 0.6731 - val_loss: 0.6461 - val_accuracy: 0.6449\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6209 - accuracy: 0.6732 - val_loss: 0.6486 - val_accuracy: 0.6450\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6210 - accuracy: 0.6724 - val_loss: 0.6460 - val_accuracy: 0.6446\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6207 - accuracy: 0.6730 - val_loss: 0.6472 - val_accuracy: 0.6446\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6207 - accuracy: 0.6731 - val_loss: 0.6464 - val_accuracy: 0.6452\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6210 - accuracy: 0.6732 - val_loss: 0.6470 - val_accuracy: 0.6443\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6201 - accuracy: 0.6740 - val_loss: 0.6485 - val_accuracy: 0.6444\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6208 - accuracy: 0.6737 - val_loss: 0.6464 - val_accuracy: 0.6443\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6205 - accuracy: 0.6733 - val_loss: 0.6469 - val_accuracy: 0.6442\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6208 - accuracy: 0.6733 - val_loss: 0.6459 - val_accuracy: 0.6452\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6205 - accuracy: 0.6741 - val_loss: 0.6468 - val_accuracy: 0.6444\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6204 - accuracy: 0.6737 - val_loss: 0.6466 - val_accuracy: 0.6449\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6206 - accuracy: 0.6730 - val_loss: 0.6471 - val_accuracy: 0.6452\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6203 - accuracy: 0.6737 - val_loss: 0.6483 - val_accuracy: 0.6443\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6202 - accuracy: 0.6734 - val_loss: 0.6442 - val_accuracy: 0.6452\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6202 - accuracy: 0.6735 - val_loss: 0.6481 - val_accuracy: 0.6444\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6202 - accuracy: 0.6734 - val_loss: 0.6465 - val_accuracy: 0.6449\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6199 - accuracy: 0.6737 - val_loss: 0.6476 - val_accuracy: 0.6450\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6205 - accuracy: 0.6735 - val_loss: 0.6453 - val_accuracy: 0.6441\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6198 - accuracy: 0.6749 - val_loss: 0.6487 - val_accuracy: 0.6447\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6202 - accuracy: 0.6734 - val_loss: 0.6445 - val_accuracy: 0.6449\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6199 - accuracy: 0.6726 - val_loss: 0.6472 - val_accuracy: 0.6440\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6199 - accuracy: 0.6740 - val_loss: 0.6486 - val_accuracy: 0.6449\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6197 - accuracy: 0.6741 - val_loss: 0.6451 - val_accuracy: 0.6447\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6201 - accuracy: 0.6733 - val_loss: 0.6472 - val_accuracy: 0.6437\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.6196 - accuracy: 0.6732 - val_loss: 0.6464 - val_accuracy: 0.6440\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 0.6202 - accuracy: 0.6723 - val_loss: 0.6468 - val_accuracy: 0.6447\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6200 - accuracy: 0.6736 - val_loss: 0.6457 - val_accuracy: 0.6440\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6200 - accuracy: 0.6740 - val_loss: 0.6489 - val_accuracy: 0.6438\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6201 - accuracy: 0.6732 - val_loss: 0.6435 - val_accuracy: 0.6453\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6202 - accuracy: 0.6737 - val_loss: 0.6489 - val_accuracy: 0.6444\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6196 - accuracy: 0.6734 - val_loss: 0.6452 - val_accuracy: 0.6443\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6198 - accuracy: 0.6730 - val_loss: 0.6484 - val_accuracy: 0.6442\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6196 - accuracy: 0.6742 - val_loss: 0.6452 - val_accuracy: 0.6452\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6196 - accuracy: 0.6742 - val_loss: 0.6468 - val_accuracy: 0.6437\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6196 - accuracy: 0.6738 - val_loss: 0.6478 - val_accuracy: 0.6447\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6191 - accuracy: 0.6740 - val_loss: 0.6460 - val_accuracy: 0.6451\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6194 - accuracy: 0.6743 - val_loss: 0.6456 - val_accuracy: 0.6440\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6199 - accuracy: 0.6736 - val_loss: 0.6477 - val_accuracy: 0.6446\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6196 - accuracy: 0.6743 - val_loss: 0.6451 - val_accuracy: 0.6439\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6191 - accuracy: 0.6738 - val_loss: 0.6480 - val_accuracy: 0.6447\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6193 - accuracy: 0.6751 - val_loss: 0.6439 - val_accuracy: 0.6447\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6186 - accuracy: 0.6750 - val_loss: 0.6488 - val_accuracy: 0.6437\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6195 - accuracy: 0.6733 - val_loss: 0.6443 - val_accuracy: 0.6443\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6195 - accuracy: 0.6738 - val_loss: 0.6493 - val_accuracy: 0.6442\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6187 - accuracy: 0.6743 - val_loss: 0.6457 - val_accuracy: 0.6449\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6193 - accuracy: 0.6735 - val_loss: 0.6472 - val_accuracy: 0.6442\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6193 - accuracy: 0.6742 - val_loss: 0.6463 - val_accuracy: 0.6452\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.6190 - accuracy: 0.6736 - val_loss: 0.6469 - val_accuracy: 0.6444\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6191 - accuracy: 0.6743 - val_loss: 0.6471 - val_accuracy: 0.6441\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6189 - accuracy: 0.6744 - val_loss: 0.6464 - val_accuracy: 0.6440\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6191 - accuracy: 0.6744 - val_loss: 0.6462 - val_accuracy: 0.6450\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6188 - accuracy: 0.6755 - val_loss: 0.6466 - val_accuracy: 0.6442\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6192 - accuracy: 0.6741 - val_loss: 0.6470 - val_accuracy: 0.6446\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6191 - accuracy: 0.6742 - val_loss: 0.6453 - val_accuracy: 0.6440\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6185 - accuracy: 0.6750 - val_loss: 0.6475 - val_accuracy: 0.6446\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6187 - accuracy: 0.6740 - val_loss: 0.6450 - val_accuracy: 0.6446\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6191 - accuracy: 0.6742 - val_loss: 0.6475 - val_accuracy: 0.6451\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6186 - accuracy: 0.6740 - val_loss: 0.6454 - val_accuracy: 0.6448\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6184 - accuracy: 0.6752 - val_loss: 0.6475 - val_accuracy: 0.6440\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6184 - accuracy: 0.6746 - val_loss: 0.6462 - val_accuracy: 0.6451\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6188 - accuracy: 0.6738 - val_loss: 0.6461 - val_accuracy: 0.6442\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6185 - accuracy: 0.6755 - val_loss: 0.6458 - val_accuracy: 0.6446\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6188 - accuracy: 0.6746 - val_loss: 0.6467 - val_accuracy: 0.6451\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6181 - accuracy: 0.6743 - val_loss: 0.6461 - val_accuracy: 0.6446\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6186 - accuracy: 0.6746 - val_loss: 0.6463 - val_accuracy: 0.6444\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6183 - accuracy: 0.6746 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6183 - accuracy: 0.6742 - val_loss: 0.6463 - val_accuracy: 0.6443\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6183 - accuracy: 0.6749 - val_loss: 0.6460 - val_accuracy: 0.6449\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6183 - accuracy: 0.6747 - val_loss: 0.6456 - val_accuracy: 0.6448\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6183 - accuracy: 0.6749 - val_loss: 0.6489 - val_accuracy: 0.6448\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6179 - accuracy: 0.6745 - val_loss: 0.6465 - val_accuracy: 0.6447\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6184 - accuracy: 0.6751 - val_loss: 0.6465 - val_accuracy: 0.6447\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6186 - accuracy: 0.6736 - val_loss: 0.6467 - val_accuracy: 0.6442\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6186 - accuracy: 0.6747 - val_loss: 0.6443 - val_accuracy: 0.6446\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.6178 - accuracy: 0.6749 - val_loss: 0.6503 - val_accuracy: 0.6449\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6180 - accuracy: 0.6743 - val_loss: 0.6438 - val_accuracy: 0.6444\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6181 - accuracy: 0.6748 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6179 - accuracy: 0.6752 - val_loss: 0.6421 - val_accuracy: 0.6447\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6187 - accuracy: 0.6740 - val_loss: 0.6495 - val_accuracy: 0.6446\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6184 - accuracy: 0.6746 - val_loss: 0.6440 - val_accuracy: 0.6450\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6182 - accuracy: 0.6747 - val_loss: 0.6483 - val_accuracy: 0.6452\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6183 - accuracy: 0.6742 - val_loss: 0.6446 - val_accuracy: 0.6443\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6177 - accuracy: 0.6752 - val_loss: 0.6488 - val_accuracy: 0.6446\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6182 - accuracy: 0.6749 - val_loss: 0.6447 - val_accuracy: 0.6449\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6177 - accuracy: 0.6750 - val_loss: 0.6479 - val_accuracy: 0.6455\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6174 - accuracy: 0.6751 - val_loss: 0.6430 - val_accuracy: 0.6451\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6180 - accuracy: 0.6754 - val_loss: 0.6486 - val_accuracy: 0.6449\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6178 - accuracy: 0.6760 - val_loss: 0.6455 - val_accuracy: 0.6444\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6179 - accuracy: 0.6751 - val_loss: 0.6459 - val_accuracy: 0.6446\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6174 - accuracy: 0.6757 - val_loss: 0.6468 - val_accuracy: 0.6444\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6177 - accuracy: 0.6755 - val_loss: 0.6468 - val_accuracy: 0.6452\n",
      "Epoch:  3\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6176 - accuracy: 0.6750 - val_loss: 0.6466 - val_accuracy: 0.6448\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6176 - accuracy: 0.6752 - val_loss: 0.6475 - val_accuracy: 0.6449\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6174 - accuracy: 0.6749 - val_loss: 0.6472 - val_accuracy: 0.6448\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6177 - accuracy: 0.6750 - val_loss: 0.6458 - val_accuracy: 0.6448\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6174 - accuracy: 0.6754 - val_loss: 0.6466 - val_accuracy: 0.6447\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6177 - accuracy: 0.6761 - val_loss: 0.6464 - val_accuracy: 0.6448\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6176 - accuracy: 0.6756 - val_loss: 0.6483 - val_accuracy: 0.6444\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6178 - accuracy: 0.6753 - val_loss: 0.6445 - val_accuracy: 0.6444\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6175 - accuracy: 0.6751 - val_loss: 0.6491 - val_accuracy: 0.6452\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6170 - accuracy: 0.6758 - val_loss: 0.6464 - val_accuracy: 0.6453\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6175 - accuracy: 0.6748 - val_loss: 0.6462 - val_accuracy: 0.6447\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6170 - accuracy: 0.6754 - val_loss: 0.6459 - val_accuracy: 0.6448\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6173 - accuracy: 0.6756 - val_loss: 0.6471 - val_accuracy: 0.6452\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6175 - accuracy: 0.6758 - val_loss: 0.6465 - val_accuracy: 0.6449\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6171 - accuracy: 0.6760 - val_loss: 0.6457 - val_accuracy: 0.6447\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6172 - accuracy: 0.6754 - val_loss: 0.6478 - val_accuracy: 0.6443\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6173 - accuracy: 0.6754 - val_loss: 0.6463 - val_accuracy: 0.6450\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6170 - accuracy: 0.6751 - val_loss: 0.6489 - val_accuracy: 0.6449\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6174 - accuracy: 0.6746 - val_loss: 0.6443 - val_accuracy: 0.6444\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6176 - accuracy: 0.6746 - val_loss: 0.6489 - val_accuracy: 0.6447\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6174 - accuracy: 0.6751 - val_loss: 0.6451 - val_accuracy: 0.6450\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6174 - accuracy: 0.6752 - val_loss: 0.6485 - val_accuracy: 0.6450\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6175 - accuracy: 0.6758 - val_loss: 0.6442 - val_accuracy: 0.6453\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6169 - accuracy: 0.6754 - val_loss: 0.6504 - val_accuracy: 0.6453\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6172 - accuracy: 0.6744 - val_loss: 0.6437 - val_accuracy: 0.6453\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6169 - accuracy: 0.6762 - val_loss: 0.6495 - val_accuracy: 0.6452\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6170 - accuracy: 0.6757 - val_loss: 0.6443 - val_accuracy: 0.6451\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6168 - accuracy: 0.6754 - val_loss: 0.6485 - val_accuracy: 0.6444\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6169 - accuracy: 0.6758 - val_loss: 0.6473 - val_accuracy: 0.6451\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6169 - accuracy: 0.6753 - val_loss: 0.6474 - val_accuracy: 0.6451\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6165 - accuracy: 0.6753 - val_loss: 0.6478 - val_accuracy: 0.6458\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6167 - accuracy: 0.6761 - val_loss: 0.6455 - val_accuracy: 0.6453\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6172 - accuracy: 0.6749 - val_loss: 0.6468 - val_accuracy: 0.6446\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6169 - accuracy: 0.6756 - val_loss: 0.6464 - val_accuracy: 0.6453\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6169 - accuracy: 0.6760 - val_loss: 0.6477 - val_accuracy: 0.6450\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6173 - accuracy: 0.6747 - val_loss: 0.6469 - val_accuracy: 0.6455\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6166 - accuracy: 0.6756 - val_loss: 0.6449 - val_accuracy: 0.6460\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6168 - accuracy: 0.6758 - val_loss: 0.6491 - val_accuracy: 0.6458\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6168 - accuracy: 0.6754 - val_loss: 0.6447 - val_accuracy: 0.6457\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6169 - accuracy: 0.6757 - val_loss: 0.6491 - val_accuracy: 0.6446\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6170 - accuracy: 0.6759 - val_loss: 0.6450 - val_accuracy: 0.6460\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6169 - accuracy: 0.6756 - val_loss: 0.6504 - val_accuracy: 0.6453\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6168 - accuracy: 0.6755 - val_loss: 0.6454 - val_accuracy: 0.6461\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6167 - accuracy: 0.6753 - val_loss: 0.6491 - val_accuracy: 0.6458\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6161 - accuracy: 0.6758 - val_loss: 0.6468 - val_accuracy: 0.6457\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6166 - accuracy: 0.6752 - val_loss: 0.6469 - val_accuracy: 0.6461\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6165 - accuracy: 0.6762 - val_loss: 0.6478 - val_accuracy: 0.6460\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6167 - accuracy: 0.6757 - val_loss: 0.6447 - val_accuracy: 0.6456\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6165 - accuracy: 0.6759 - val_loss: 0.6480 - val_accuracy: 0.6457\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6165 - accuracy: 0.6765 - val_loss: 0.6464 - val_accuracy: 0.6458\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6162 - accuracy: 0.6759 - val_loss: 0.6475 - val_accuracy: 0.6457\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6161 - accuracy: 0.6757 - val_loss: 0.6463 - val_accuracy: 0.6458\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6163 - accuracy: 0.6755 - val_loss: 0.6475 - val_accuracy: 0.6459\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6164 - accuracy: 0.6757 - val_loss: 0.6449 - val_accuracy: 0.6458\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6162 - accuracy: 0.6763 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6160 - accuracy: 0.6764 - val_loss: 0.6436 - val_accuracy: 0.6457\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6164 - accuracy: 0.6763 - val_loss: 0.6495 - val_accuracy: 0.6452\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6159 - accuracy: 0.6760 - val_loss: 0.6445 - val_accuracy: 0.6460\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6165 - accuracy: 0.6759 - val_loss: 0.6481 - val_accuracy: 0.6451\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6163 - accuracy: 0.6755 - val_loss: 0.6466 - val_accuracy: 0.6460\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6163 - accuracy: 0.6760 - val_loss: 0.6468 - val_accuracy: 0.6452\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6166 - accuracy: 0.6765 - val_loss: 0.6469 - val_accuracy: 0.6458\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6163 - accuracy: 0.6760 - val_loss: 0.6456 - val_accuracy: 0.6460\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6169 - accuracy: 0.6755 - val_loss: 0.6477 - val_accuracy: 0.6457\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6157 - accuracy: 0.6766 - val_loss: 0.6487 - val_accuracy: 0.6459\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6157 - accuracy: 0.6756 - val_loss: 0.6459 - val_accuracy: 0.6461\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6163 - accuracy: 0.6763 - val_loss: 0.6480 - val_accuracy: 0.6457\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6157 - accuracy: 0.6754 - val_loss: 0.6479 - val_accuracy: 0.6458\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.6160 - accuracy: 0.6758 - val_loss: 0.6467 - val_accuracy: 0.6459\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6157 - accuracy: 0.6763 - val_loss: 0.6461 - val_accuracy: 0.6463\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6163 - accuracy: 0.6756 - val_loss: 0.6486 - val_accuracy: 0.6461\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6158 - accuracy: 0.6768 - val_loss: 0.6461 - val_accuracy: 0.6463\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6160 - accuracy: 0.6763 - val_loss: 0.6488 - val_accuracy: 0.6462\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6161 - accuracy: 0.6764 - val_loss: 0.6462 - val_accuracy: 0.6460\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6160 - accuracy: 0.6754 - val_loss: 0.6495 - val_accuracy: 0.6460\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.6159 - accuracy: 0.6758 - val_loss: 0.6471 - val_accuracy: 0.6459\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6161 - accuracy: 0.6762 - val_loss: 0.6441 - val_accuracy: 0.6461\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6156 - accuracy: 0.6761 - val_loss: 0.6511 - val_accuracy: 0.6460\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6155 - accuracy: 0.6764 - val_loss: 0.6436 - val_accuracy: 0.6455\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6160 - accuracy: 0.6759 - val_loss: 0.6500 - val_accuracy: 0.6453\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6155 - accuracy: 0.6762 - val_loss: 0.6477 - val_accuracy: 0.6453\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6160 - accuracy: 0.6749 - val_loss: 0.6435 - val_accuracy: 0.6457\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6159 - accuracy: 0.6756 - val_loss: 0.6501 - val_accuracy: 0.6453\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6156 - accuracy: 0.6761 - val_loss: 0.6453 - val_accuracy: 0.6459\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6165 - accuracy: 0.6767 - val_loss: 0.6459 - val_accuracy: 0.6460\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6159 - accuracy: 0.6761 - val_loss: 0.6469 - val_accuracy: 0.6462\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6158 - accuracy: 0.6754 - val_loss: 0.6485 - val_accuracy: 0.6462\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6160 - accuracy: 0.6765 - val_loss: 0.6451 - val_accuracy: 0.6461\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6160 - accuracy: 0.6760 - val_loss: 0.6506 - val_accuracy: 0.6460\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6160 - accuracy: 0.6761 - val_loss: 0.6439 - val_accuracy: 0.6458\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6153 - accuracy: 0.6771 - val_loss: 0.6510 - val_accuracy: 0.6455\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6153 - accuracy: 0.6763 - val_loss: 0.6440 - val_accuracy: 0.6461\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6155 - accuracy: 0.6759 - val_loss: 0.6493 - val_accuracy: 0.6460\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6154 - accuracy: 0.6764 - val_loss: 0.6468 - val_accuracy: 0.6461\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6153 - accuracy: 0.6763 - val_loss: 0.6464 - val_accuracy: 0.6460\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6154 - accuracy: 0.6759 - val_loss: 0.6472 - val_accuracy: 0.6457\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6153 - accuracy: 0.6763 - val_loss: 0.6479 - val_accuracy: 0.6458\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6158 - accuracy: 0.6766 - val_loss: 0.6469 - val_accuracy: 0.6456\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6156 - accuracy: 0.6757 - val_loss: 0.6476 - val_accuracy: 0.6452\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6157 - accuracy: 0.6763 - val_loss: 0.6460 - val_accuracy: 0.6451\n",
      "Epoch:  4\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6158 - accuracy: 0.6763 - val_loss: 0.6480 - val_accuracy: 0.6449\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6155 - accuracy: 0.6767 - val_loss: 0.6483 - val_accuracy: 0.6456\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6152 - accuracy: 0.6765 - val_loss: 0.6468 - val_accuracy: 0.6456\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6150 - accuracy: 0.6764 - val_loss: 0.6490 - val_accuracy: 0.6455\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6151 - accuracy: 0.6765 - val_loss: 0.6464 - val_accuracy: 0.6456\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6149 - accuracy: 0.6768 - val_loss: 0.6498 - val_accuracy: 0.6455\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6154 - accuracy: 0.6775 - val_loss: 0.6480 - val_accuracy: 0.6456\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6151 - accuracy: 0.6759 - val_loss: 0.6478 - val_accuracy: 0.6453\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6151 - accuracy: 0.6772 - val_loss: 0.6490 - val_accuracy: 0.6458\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6152 - accuracy: 0.6776 - val_loss: 0.6453 - val_accuracy: 0.6459\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6155 - accuracy: 0.6766 - val_loss: 0.6473 - val_accuracy: 0.6459\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6158 - accuracy: 0.6765 - val_loss: 0.6483 - val_accuracy: 0.6463\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6155 - accuracy: 0.6765 - val_loss: 0.6460 - val_accuracy: 0.6463\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6148 - accuracy: 0.6771 - val_loss: 0.6466 - val_accuracy: 0.6463\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6150 - accuracy: 0.6769 - val_loss: 0.6471 - val_accuracy: 0.6460\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6151 - accuracy: 0.6771 - val_loss: 0.6462 - val_accuracy: 0.6462\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6155 - accuracy: 0.6766 - val_loss: 0.6477 - val_accuracy: 0.6456\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6144 - accuracy: 0.6765 - val_loss: 0.6480 - val_accuracy: 0.6458\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6156 - accuracy: 0.6763 - val_loss: 0.6454 - val_accuracy: 0.6453\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.6149 - accuracy: 0.6768 - val_loss: 0.6509 - val_accuracy: 0.6452\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6151 - accuracy: 0.6776 - val_loss: 0.6448 - val_accuracy: 0.6461\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6150 - accuracy: 0.6769 - val_loss: 0.6501 - val_accuracy: 0.6452\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6146 - accuracy: 0.6767 - val_loss: 0.6456 - val_accuracy: 0.6455\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6150 - accuracy: 0.6762 - val_loss: 0.6471 - val_accuracy: 0.6457\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6146 - accuracy: 0.6780 - val_loss: 0.6469 - val_accuracy: 0.6459\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6149 - accuracy: 0.6775 - val_loss: 0.6451 - val_accuracy: 0.6463\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6152 - accuracy: 0.6768 - val_loss: 0.6493 - val_accuracy: 0.6460\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6153 - accuracy: 0.6775 - val_loss: 0.6432 - val_accuracy: 0.6467\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6153 - accuracy: 0.6774 - val_loss: 0.6513 - val_accuracy: 0.6448\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6149 - accuracy: 0.6772 - val_loss: 0.6444 - val_accuracy: 0.6462\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6147 - accuracy: 0.6773 - val_loss: 0.6493 - val_accuracy: 0.6456\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6152 - accuracy: 0.6761 - val_loss: 0.6471 - val_accuracy: 0.6458\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6146 - accuracy: 0.6775 - val_loss: 0.6479 - val_accuracy: 0.6456\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6152 - accuracy: 0.6765 - val_loss: 0.6457 - val_accuracy: 0.6455\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6146 - accuracy: 0.6766 - val_loss: 0.6461 - val_accuracy: 0.6456\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6151 - accuracy: 0.6775 - val_loss: 0.6486 - val_accuracy: 0.6457\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6147 - accuracy: 0.6767 - val_loss: 0.6461 - val_accuracy: 0.6469\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6148 - accuracy: 0.6771 - val_loss: 0.6484 - val_accuracy: 0.6456\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6151 - accuracy: 0.6765 - val_loss: 0.6470 - val_accuracy: 0.6457\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6149 - accuracy: 0.6764 - val_loss: 0.6485 - val_accuracy: 0.6456\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6150 - accuracy: 0.6775 - val_loss: 0.6458 - val_accuracy: 0.6458\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6146 - accuracy: 0.6769 - val_loss: 0.6485 - val_accuracy: 0.6459\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6149 - accuracy: 0.6767 - val_loss: 0.6464 - val_accuracy: 0.6456\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6151 - accuracy: 0.6774 - val_loss: 0.6475 - val_accuracy: 0.6455\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6152 - accuracy: 0.6763 - val_loss: 0.6474 - val_accuracy: 0.6452\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6146 - accuracy: 0.6763 - val_loss: 0.6468 - val_accuracy: 0.6455\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6145 - accuracy: 0.6781 - val_loss: 0.6475 - val_accuracy: 0.6455\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6149 - accuracy: 0.6771 - val_loss: 0.6465 - val_accuracy: 0.6451\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6147 - accuracy: 0.6774 - val_loss: 0.6474 - val_accuracy: 0.6455\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6144 - accuracy: 0.6773 - val_loss: 0.6472 - val_accuracy: 0.6449\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6142 - accuracy: 0.6775 - val_loss: 0.6468 - val_accuracy: 0.6452\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6149 - accuracy: 0.6774 - val_loss: 0.6484 - val_accuracy: 0.6450\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6141 - accuracy: 0.6779 - val_loss: 0.6479 - val_accuracy: 0.6458\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6150 - accuracy: 0.6770 - val_loss: 0.6475 - val_accuracy: 0.6452\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6144 - accuracy: 0.6774 - val_loss: 0.6510 - val_accuracy: 0.6447\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6143 - accuracy: 0.6783 - val_loss: 0.6462 - val_accuracy: 0.6452\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6148 - accuracy: 0.6770 - val_loss: 0.6482 - val_accuracy: 0.6456\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6148 - accuracy: 0.6763 - val_loss: 0.6469 - val_accuracy: 0.6456\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6147 - accuracy: 0.6767 - val_loss: 0.6483 - val_accuracy: 0.6444\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6141 - accuracy: 0.6775 - val_loss: 0.6469 - val_accuracy: 0.6452\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6144 - accuracy: 0.6765 - val_loss: 0.6457 - val_accuracy: 0.6452\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6143 - accuracy: 0.6770 - val_loss: 0.6520 - val_accuracy: 0.6452\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6147 - accuracy: 0.6773 - val_loss: 0.6452 - val_accuracy: 0.6451\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6154 - accuracy: 0.6762 - val_loss: 0.6487 - val_accuracy: 0.6458\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6146 - accuracy: 0.6779 - val_loss: 0.6497 - val_accuracy: 0.6457\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6144 - accuracy: 0.6768 - val_loss: 0.6463 - val_accuracy: 0.6462\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6148 - accuracy: 0.6758 - val_loss: 0.6504 - val_accuracy: 0.6460\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6140 - accuracy: 0.6768 - val_loss: 0.6448 - val_accuracy: 0.6458\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6145 - accuracy: 0.6770 - val_loss: 0.6504 - val_accuracy: 0.6460\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6148 - accuracy: 0.6774 - val_loss: 0.6440 - val_accuracy: 0.6456\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6140 - accuracy: 0.6775 - val_loss: 0.6504 - val_accuracy: 0.6460\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6141 - accuracy: 0.6770 - val_loss: 0.6464 - val_accuracy: 0.6451\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6147 - accuracy: 0.6760 - val_loss: 0.6492 - val_accuracy: 0.6467\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6149 - accuracy: 0.6774 - val_loss: 0.6473 - val_accuracy: 0.6452\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6144 - accuracy: 0.6776 - val_loss: 0.6459 - val_accuracy: 0.6460\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6139 - accuracy: 0.6764 - val_loss: 0.6511 - val_accuracy: 0.6461\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6146 - accuracy: 0.6765 - val_loss: 0.6448 - val_accuracy: 0.6459\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6141 - accuracy: 0.6774 - val_loss: 0.6496 - val_accuracy: 0.6460\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6137 - accuracy: 0.6778 - val_loss: 0.6482 - val_accuracy: 0.6455\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6138 - accuracy: 0.6784 - val_loss: 0.6482 - val_accuracy: 0.6460\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6140 - accuracy: 0.6776 - val_loss: 0.6475 - val_accuracy: 0.6465\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6143 - accuracy: 0.6770 - val_loss: 0.6472 - val_accuracy: 0.6466\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6144 - accuracy: 0.6771 - val_loss: 0.6513 - val_accuracy: 0.6462\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6142 - accuracy: 0.6771 - val_loss: 0.6441 - val_accuracy: 0.6451\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6145 - accuracy: 0.6762 - val_loss: 0.6509 - val_accuracy: 0.6458\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6143 - accuracy: 0.6777 - val_loss: 0.6455 - val_accuracy: 0.6459\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6140 - accuracy: 0.6775 - val_loss: 0.6481 - val_accuracy: 0.6461\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6133 - accuracy: 0.6781 - val_loss: 0.6470 - val_accuracy: 0.6458\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6138 - accuracy: 0.6777 - val_loss: 0.6468 - val_accuracy: 0.6461\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6139 - accuracy: 0.6776 - val_loss: 0.6482 - val_accuracy: 0.6460\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6136 - accuracy: 0.6780 - val_loss: 0.6490 - val_accuracy: 0.6458\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6139 - accuracy: 0.6782 - val_loss: 0.6501 - val_accuracy: 0.6456\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6139 - accuracy: 0.6775 - val_loss: 0.6435 - val_accuracy: 0.6452\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6140 - accuracy: 0.6772 - val_loss: 0.6537 - val_accuracy: 0.6460\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6144 - accuracy: 0.6769 - val_loss: 0.6428 - val_accuracy: 0.6456\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.6143 - accuracy: 0.6778 - val_loss: 0.6527 - val_accuracy: 0.6467\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6132 - accuracy: 0.6775 - val_loss: 0.6464 - val_accuracy: 0.6460\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6134 - accuracy: 0.6789 - val_loss: 0.6484 - val_accuracy: 0.6466\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6139 - accuracy: 0.6777 - val_loss: 0.6463 - val_accuracy: 0.6467\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6136 - accuracy: 0.6783 - val_loss: 0.6470 - val_accuracy: 0.6459\n",
      "Epoch:  5\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6136 - accuracy: 0.6776 - val_loss: 0.6492 - val_accuracy: 0.6453\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6141 - accuracy: 0.6761 - val_loss: 0.6482 - val_accuracy: 0.6457\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6135 - accuracy: 0.6775 - val_loss: 0.6488 - val_accuracy: 0.6457\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6142 - accuracy: 0.6774 - val_loss: 0.6470 - val_accuracy: 0.6462\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6137 - accuracy: 0.6774 - val_loss: 0.6501 - val_accuracy: 0.6463\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6132 - accuracy: 0.6783 - val_loss: 0.6466 - val_accuracy: 0.6460\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6132 - accuracy: 0.6767 - val_loss: 0.6505 - val_accuracy: 0.6457\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6140 - accuracy: 0.6778 - val_loss: 0.6470 - val_accuracy: 0.6453\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6132 - accuracy: 0.6772 - val_loss: 0.6495 - val_accuracy: 0.6458\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6136 - accuracy: 0.6775 - val_loss: 0.6477 - val_accuracy: 0.6459\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6131 - accuracy: 0.6781 - val_loss: 0.6501 - val_accuracy: 0.6455\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6140 - accuracy: 0.6776 - val_loss: 0.6496 - val_accuracy: 0.6453\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6135 - accuracy: 0.6779 - val_loss: 0.6472 - val_accuracy: 0.6453\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6134 - accuracy: 0.6772 - val_loss: 0.6502 - val_accuracy: 0.6455\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6135 - accuracy: 0.6774 - val_loss: 0.6469 - val_accuracy: 0.6448\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6135 - accuracy: 0.6771 - val_loss: 0.6487 - val_accuracy: 0.6456\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6136 - accuracy: 0.6779 - val_loss: 0.6461 - val_accuracy: 0.6455\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6133 - accuracy: 0.6779 - val_loss: 0.6509 - val_accuracy: 0.6452\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6135 - accuracy: 0.6770 - val_loss: 0.6476 - val_accuracy: 0.6449\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6134 - accuracy: 0.6777 - val_loss: 0.6484 - val_accuracy: 0.6451\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6132 - accuracy: 0.6771 - val_loss: 0.6493 - val_accuracy: 0.6456\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6132 - accuracy: 0.6770 - val_loss: 0.6487 - val_accuracy: 0.6451\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6133 - accuracy: 0.6768 - val_loss: 0.6471 - val_accuracy: 0.6455\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6136 - accuracy: 0.6778 - val_loss: 0.6495 - val_accuracy: 0.6449\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6132 - accuracy: 0.6783 - val_loss: 0.6485 - val_accuracy: 0.6449\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6131 - accuracy: 0.6776 - val_loss: 0.6479 - val_accuracy: 0.6455\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6133 - accuracy: 0.6781 - val_loss: 0.6487 - val_accuracy: 0.6455\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6126 - accuracy: 0.6784 - val_loss: 0.6470 - val_accuracy: 0.6451\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6133 - accuracy: 0.6768 - val_loss: 0.6466 - val_accuracy: 0.6449\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6131 - accuracy: 0.6786 - val_loss: 0.6471 - val_accuracy: 0.6458\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6131 - accuracy: 0.6779 - val_loss: 0.6502 - val_accuracy: 0.6451\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6134 - accuracy: 0.6780 - val_loss: 0.6457 - val_accuracy: 0.6449\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6131 - accuracy: 0.6778 - val_loss: 0.6501 - val_accuracy: 0.6449\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6136 - accuracy: 0.6776 - val_loss: 0.6463 - val_accuracy: 0.6452\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6129 - accuracy: 0.6786 - val_loss: 0.6501 - val_accuracy: 0.6447\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6135 - accuracy: 0.6775 - val_loss: 0.6504 - val_accuracy: 0.6462\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6137 - accuracy: 0.6774 - val_loss: 0.6441 - val_accuracy: 0.6446\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6137 - accuracy: 0.6772 - val_loss: 0.6492 - val_accuracy: 0.6443\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6129 - accuracy: 0.6781 - val_loss: 0.6471 - val_accuracy: 0.6459\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6130 - accuracy: 0.6774 - val_loss: 0.6481 - val_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6133 - accuracy: 0.6777 - val_loss: 0.6476 - val_accuracy: 0.6459\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6129 - accuracy: 0.6785 - val_loss: 0.6455 - val_accuracy: 0.6451\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6131 - accuracy: 0.6776 - val_loss: 0.6495 - val_accuracy: 0.6455\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6130 - accuracy: 0.6783 - val_loss: 0.6468 - val_accuracy: 0.6451\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6136 - accuracy: 0.6770 - val_loss: 0.6490 - val_accuracy: 0.6456\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6131 - accuracy: 0.6781 - val_loss: 0.6501 - val_accuracy: 0.6444\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6126 - accuracy: 0.6786 - val_loss: 0.6490 - val_accuracy: 0.6462\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6130 - accuracy: 0.6779 - val_loss: 0.6489 - val_accuracy: 0.6447\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6127 - accuracy: 0.6780 - val_loss: 0.6483 - val_accuracy: 0.6458\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6131 - accuracy: 0.6772 - val_loss: 0.6506 - val_accuracy: 0.6460\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6132 - accuracy: 0.6779 - val_loss: 0.6469 - val_accuracy: 0.6450\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6135 - accuracy: 0.6777 - val_loss: 0.6504 - val_accuracy: 0.6461\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6130 - accuracy: 0.6772 - val_loss: 0.6470 - val_accuracy: 0.6453\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6129 - accuracy: 0.6784 - val_loss: 0.6498 - val_accuracy: 0.6458\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6125 - accuracy: 0.6786 - val_loss: 0.6474 - val_accuracy: 0.6452\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6128 - accuracy: 0.6781 - val_loss: 0.6497 - val_accuracy: 0.6452\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6130 - accuracy: 0.6779 - val_loss: 0.6470 - val_accuracy: 0.6449\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6124 - accuracy: 0.6786 - val_loss: 0.6467 - val_accuracy: 0.6447\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6130 - accuracy: 0.6779 - val_loss: 0.6512 - val_accuracy: 0.6456\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6131 - accuracy: 0.6780 - val_loss: 0.6455 - val_accuracy: 0.6458\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6131 - accuracy: 0.6779 - val_loss: 0.6505 - val_accuracy: 0.6463\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6132 - accuracy: 0.6781 - val_loss: 0.6463 - val_accuracy: 0.6458\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6129 - accuracy: 0.6781 - val_loss: 0.6502 - val_accuracy: 0.6447\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6125 - accuracy: 0.6792 - val_loss: 0.6501 - val_accuracy: 0.6460\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6126 - accuracy: 0.6786 - val_loss: 0.6476 - val_accuracy: 0.6453\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6134 - accuracy: 0.6769 - val_loss: 0.6529 - val_accuracy: 0.6452\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6134 - accuracy: 0.6780 - val_loss: 0.6431 - val_accuracy: 0.6449\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6126 - accuracy: 0.6788 - val_loss: 0.6511 - val_accuracy: 0.6459\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6132 - accuracy: 0.6776 - val_loss: 0.6486 - val_accuracy: 0.6459\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6129 - accuracy: 0.6780 - val_loss: 0.6482 - val_accuracy: 0.6451\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6122 - accuracy: 0.6780 - val_loss: 0.6515 - val_accuracy: 0.6461\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6127 - accuracy: 0.6778 - val_loss: 0.6479 - val_accuracy: 0.6455\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6122 - accuracy: 0.6789 - val_loss: 0.6495 - val_accuracy: 0.6457\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6129 - accuracy: 0.6768 - val_loss: 0.6510 - val_accuracy: 0.6453\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6124 - accuracy: 0.6779 - val_loss: 0.6483 - val_accuracy: 0.6450\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6127 - accuracy: 0.6773 - val_loss: 0.6506 - val_accuracy: 0.6453\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6123 - accuracy: 0.6783 - val_loss: 0.6496 - val_accuracy: 0.6451\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6124 - accuracy: 0.6789 - val_loss: 0.6463 - val_accuracy: 0.6453\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6124 - accuracy: 0.6790 - val_loss: 0.6502 - val_accuracy: 0.6449\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6129 - accuracy: 0.6788 - val_loss: 0.6495 - val_accuracy: 0.6455\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6124 - accuracy: 0.6781 - val_loss: 0.6458 - val_accuracy: 0.6451\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6129 - accuracy: 0.6780 - val_loss: 0.6510 - val_accuracy: 0.6457\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6130 - accuracy: 0.6783 - val_loss: 0.6492 - val_accuracy: 0.6452\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6131 - accuracy: 0.6779 - val_loss: 0.6480 - val_accuracy: 0.6455\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6126 - accuracy: 0.6785 - val_loss: 0.6496 - val_accuracy: 0.6455\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6123 - accuracy: 0.6794 - val_loss: 0.6477 - val_accuracy: 0.6455\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6122 - accuracy: 0.6790 - val_loss: 0.6488 - val_accuracy: 0.6452\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6124 - accuracy: 0.6775 - val_loss: 0.6510 - val_accuracy: 0.6451\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6123 - accuracy: 0.6789 - val_loss: 0.6471 - val_accuracy: 0.6448\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6119 - accuracy: 0.6783 - val_loss: 0.6517 - val_accuracy: 0.6452\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6122 - accuracy: 0.6786 - val_loss: 0.6472 - val_accuracy: 0.6450\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6119 - accuracy: 0.6794 - val_loss: 0.6501 - val_accuracy: 0.6452\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6128 - accuracy: 0.6781 - val_loss: 0.6436 - val_accuracy: 0.6455\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6128 - accuracy: 0.6782 - val_loss: 0.6535 - val_accuracy: 0.6443\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6129 - accuracy: 0.6774 - val_loss: 0.6483 - val_accuracy: 0.6455\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6129 - accuracy: 0.6778 - val_loss: 0.6505 - val_accuracy: 0.6453\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6122 - accuracy: 0.6785 - val_loss: 0.6490 - val_accuracy: 0.6459\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6120 - accuracy: 0.6777 - val_loss: 0.6481 - val_accuracy: 0.6450\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6115 - accuracy: 0.6780 - val_loss: 0.6507 - val_accuracy: 0.6457\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6120 - accuracy: 0.6783 - val_loss: 0.6472 - val_accuracy: 0.6450\n",
      "Epoch:  6\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6121 - accuracy: 0.6781 - val_loss: 0.6516 - val_accuracy: 0.6458\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6124 - accuracy: 0.6777 - val_loss: 0.6471 - val_accuracy: 0.6453\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6125 - accuracy: 0.6776 - val_loss: 0.6508 - val_accuracy: 0.6452\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6125 - accuracy: 0.6777 - val_loss: 0.6455 - val_accuracy: 0.6456\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6125 - accuracy: 0.6781 - val_loss: 0.6504 - val_accuracy: 0.6455\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6121 - accuracy: 0.6777 - val_loss: 0.6470 - val_accuracy: 0.6448\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6127 - accuracy: 0.6781 - val_loss: 0.6486 - val_accuracy: 0.6456\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6126 - accuracy: 0.6791 - val_loss: 0.6508 - val_accuracy: 0.6451\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6127 - accuracy: 0.6787 - val_loss: 0.6513 - val_accuracy: 0.6455\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6125 - accuracy: 0.6782 - val_loss: 0.6455 - val_accuracy: 0.6450\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6127 - accuracy: 0.6776 - val_loss: 0.6531 - val_accuracy: 0.6456\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6117 - accuracy: 0.6788 - val_loss: 0.6511 - val_accuracy: 0.6456\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6122 - accuracy: 0.6782 - val_loss: 0.6465 - val_accuracy: 0.6456\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6122 - accuracy: 0.6779 - val_loss: 0.6494 - val_accuracy: 0.6444\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6125 - accuracy: 0.6782 - val_loss: 0.6489 - val_accuracy: 0.6453\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6121 - accuracy: 0.6773 - val_loss: 0.6471 - val_accuracy: 0.6441\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6120 - accuracy: 0.6784 - val_loss: 0.6504 - val_accuracy: 0.6450\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6119 - accuracy: 0.6779 - val_loss: 0.6472 - val_accuracy: 0.6448\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6117 - accuracy: 0.6796 - val_loss: 0.6516 - val_accuracy: 0.6447\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6122 - accuracy: 0.6782 - val_loss: 0.6475 - val_accuracy: 0.6446\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6120 - accuracy: 0.6786 - val_loss: 0.6525 - val_accuracy: 0.6448\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6117 - accuracy: 0.6787 - val_loss: 0.6441 - val_accuracy: 0.6450\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6124 - accuracy: 0.6783 - val_loss: 0.6556 - val_accuracy: 0.6450\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6121 - accuracy: 0.6784 - val_loss: 0.6480 - val_accuracy: 0.6451\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6121 - accuracy: 0.6776 - val_loss: 0.6484 - val_accuracy: 0.6449\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6118 - accuracy: 0.6782 - val_loss: 0.6486 - val_accuracy: 0.6453\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6118 - accuracy: 0.6786 - val_loss: 0.6450 - val_accuracy: 0.6449\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6114 - accuracy: 0.6791 - val_loss: 0.6564 - val_accuracy: 0.6452\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6123 - accuracy: 0.6794 - val_loss: 0.6439 - val_accuracy: 0.6449\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6120 - accuracy: 0.6787 - val_loss: 0.6513 - val_accuracy: 0.6452\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6115 - accuracy: 0.6788 - val_loss: 0.6481 - val_accuracy: 0.6456\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6112 - accuracy: 0.6790 - val_loss: 0.6507 - val_accuracy: 0.6460\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6118 - accuracy: 0.6786 - val_loss: 0.6482 - val_accuracy: 0.6450\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6117 - accuracy: 0.6789 - val_loss: 0.6465 - val_accuracy: 0.6456\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6117 - accuracy: 0.6789 - val_loss: 0.6547 - val_accuracy: 0.6453\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6113 - accuracy: 0.6790 - val_loss: 0.6445 - val_accuracy: 0.6456\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6124 - accuracy: 0.6788 - val_loss: 0.6521 - val_accuracy: 0.6446\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6119 - accuracy: 0.6784 - val_loss: 0.6480 - val_accuracy: 0.6450\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6124 - accuracy: 0.6794 - val_loss: 0.6556 - val_accuracy: 0.6450\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6117 - accuracy: 0.6784 - val_loss: 0.6442 - val_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6121 - accuracy: 0.6789 - val_loss: 0.6570 - val_accuracy: 0.6456\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6119 - accuracy: 0.6796 - val_loss: 0.6459 - val_accuracy: 0.6451\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6116 - accuracy: 0.6784 - val_loss: 0.6513 - val_accuracy: 0.6459\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6111 - accuracy: 0.6792 - val_loss: 0.6487 - val_accuracy: 0.6451\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6114 - accuracy: 0.6792 - val_loss: 0.6510 - val_accuracy: 0.6450\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6123 - accuracy: 0.6790 - val_loss: 0.6476 - val_accuracy: 0.6448\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6119 - accuracy: 0.6785 - val_loss: 0.6470 - val_accuracy: 0.6449\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6122 - accuracy: 0.6781 - val_loss: 0.6534 - val_accuracy: 0.6450\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6114 - accuracy: 0.6796 - val_loss: 0.6453 - val_accuracy: 0.6450\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6118 - accuracy: 0.6794 - val_loss: 0.6542 - val_accuracy: 0.6450\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6119 - accuracy: 0.6783 - val_loss: 0.6475 - val_accuracy: 0.6447\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6113 - accuracy: 0.6790 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6126 - accuracy: 0.6785 - val_loss: 0.6475 - val_accuracy: 0.6451\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6113 - accuracy: 0.6791 - val_loss: 0.6547 - val_accuracy: 0.6456\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6120 - accuracy: 0.6799 - val_loss: 0.6447 - val_accuracy: 0.6458\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6113 - accuracy: 0.6789 - val_loss: 0.6540 - val_accuracy: 0.6451\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6118 - accuracy: 0.6791 - val_loss: 0.6473 - val_accuracy: 0.6449\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6118 - accuracy: 0.6792 - val_loss: 0.6524 - val_accuracy: 0.6448\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6110 - accuracy: 0.6790 - val_loss: 0.6472 - val_accuracy: 0.6450\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6114 - accuracy: 0.6792 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6117 - accuracy: 0.6791 - val_loss: 0.6502 - val_accuracy: 0.6453\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6112 - accuracy: 0.6790 - val_loss: 0.6484 - val_accuracy: 0.6452\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6113 - accuracy: 0.6796 - val_loss: 0.6518 - val_accuracy: 0.6450\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6113 - accuracy: 0.6781 - val_loss: 0.6501 - val_accuracy: 0.6453\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6114 - accuracy: 0.6792 - val_loss: 0.6489 - val_accuracy: 0.6448\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6117 - accuracy: 0.6784 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6115 - accuracy: 0.6789 - val_loss: 0.6494 - val_accuracy: 0.6452\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6114 - accuracy: 0.6786 - val_loss: 0.6473 - val_accuracy: 0.6450\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6117 - accuracy: 0.6784 - val_loss: 0.6526 - val_accuracy: 0.6457\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6112 - accuracy: 0.6783 - val_loss: 0.6471 - val_accuracy: 0.6452\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6114 - accuracy: 0.6790 - val_loss: 0.6509 - val_accuracy: 0.6450\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6111 - accuracy: 0.6796 - val_loss: 0.6493 - val_accuracy: 0.6452\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6107 - accuracy: 0.6788 - val_loss: 0.6501 - val_accuracy: 0.6456\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6117 - accuracy: 0.6784 - val_loss: 0.6481 - val_accuracy: 0.6453\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6116 - accuracy: 0.6783 - val_loss: 0.6495 - val_accuracy: 0.6448\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6112 - accuracy: 0.6795 - val_loss: 0.6508 - val_accuracy: 0.6450\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6116 - accuracy: 0.6796 - val_loss: 0.6477 - val_accuracy: 0.6453\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6114 - accuracy: 0.6795 - val_loss: 0.6508 - val_accuracy: 0.6447\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6113 - accuracy: 0.6792 - val_loss: 0.6488 - val_accuracy: 0.6447\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6111 - accuracy: 0.6797 - val_loss: 0.6501 - val_accuracy: 0.6444\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6107 - accuracy: 0.6797 - val_loss: 0.6518 - val_accuracy: 0.6446\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6111 - accuracy: 0.6796 - val_loss: 0.6483 - val_accuracy: 0.6446\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6115 - accuracy: 0.6785 - val_loss: 0.6511 - val_accuracy: 0.6447\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6106 - accuracy: 0.6798 - val_loss: 0.6509 - val_accuracy: 0.6450\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6118 - accuracy: 0.6781 - val_loss: 0.6479 - val_accuracy: 0.6444\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6117 - accuracy: 0.6783 - val_loss: 0.6487 - val_accuracy: 0.6447\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6111 - accuracy: 0.6791 - val_loss: 0.6524 - val_accuracy: 0.6450\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6110 - accuracy: 0.6789 - val_loss: 0.6485 - val_accuracy: 0.6451\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6105 - accuracy: 0.6809 - val_loss: 0.6500 - val_accuracy: 0.6456\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6110 - accuracy: 0.6795 - val_loss: 0.6508 - val_accuracy: 0.6450\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6112 - accuracy: 0.6795 - val_loss: 0.6471 - val_accuracy: 0.6456\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6113 - accuracy: 0.6796 - val_loss: 0.6504 - val_accuracy: 0.6448\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6107 - accuracy: 0.6796 - val_loss: 0.6483 - val_accuracy: 0.6453\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6109 - accuracy: 0.6793 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6109 - accuracy: 0.6800 - val_loss: 0.6480 - val_accuracy: 0.6450\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6107 - accuracy: 0.6795 - val_loss: 0.6510 - val_accuracy: 0.6447\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6110 - accuracy: 0.6795 - val_loss: 0.6492 - val_accuracy: 0.6446\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6109 - accuracy: 0.6799 - val_loss: 0.6449 - val_accuracy: 0.6444\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6113 - accuracy: 0.6795 - val_loss: 0.6558 - val_accuracy: 0.6451\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6106 - accuracy: 0.6799 - val_loss: 0.6467 - val_accuracy: 0.6451\n",
      "Epoch:  7\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6114 - accuracy: 0.6791 - val_loss: 0.6509 - val_accuracy: 0.6448\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6112 - accuracy: 0.6792 - val_loss: 0.6521 - val_accuracy: 0.6452\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6112 - accuracy: 0.6782 - val_loss: 0.6453 - val_accuracy: 0.6451\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6111 - accuracy: 0.6788 - val_loss: 0.6541 - val_accuracy: 0.6452\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6111 - accuracy: 0.6786 - val_loss: 0.6467 - val_accuracy: 0.6449\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6115 - accuracy: 0.6791 - val_loss: 0.6515 - val_accuracy: 0.6448\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6110 - accuracy: 0.6790 - val_loss: 0.6498 - val_accuracy: 0.6450\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6113 - accuracy: 0.6792 - val_loss: 0.6485 - val_accuracy: 0.6453\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6110 - accuracy: 0.6797 - val_loss: 0.6515 - val_accuracy: 0.6456\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6114 - accuracy: 0.6789 - val_loss: 0.6471 - val_accuracy: 0.6446\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6108 - accuracy: 0.6799 - val_loss: 0.6501 - val_accuracy: 0.6448\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6112 - accuracy: 0.6803 - val_loss: 0.6483 - val_accuracy: 0.6459\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6102 - accuracy: 0.6799 - val_loss: 0.6529 - val_accuracy: 0.6449\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6110 - accuracy: 0.6785 - val_loss: 0.6454 - val_accuracy: 0.6448\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6109 - accuracy: 0.6794 - val_loss: 0.6530 - val_accuracy: 0.6447\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6109 - accuracy: 0.6799 - val_loss: 0.6474 - val_accuracy: 0.6451\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6106 - accuracy: 0.6794 - val_loss: 0.6515 - val_accuracy: 0.6452\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6111 - accuracy: 0.6792 - val_loss: 0.6478 - val_accuracy: 0.6450\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6103 - accuracy: 0.6796 - val_loss: 0.6536 - val_accuracy: 0.6453\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6108 - accuracy: 0.6792 - val_loss: 0.6473 - val_accuracy: 0.6450\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6115 - accuracy: 0.6786 - val_loss: 0.6512 - val_accuracy: 0.6448\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6108 - accuracy: 0.6789 - val_loss: 0.6506 - val_accuracy: 0.6460\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6106 - accuracy: 0.6800 - val_loss: 0.6498 - val_accuracy: 0.6450\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6106 - accuracy: 0.6793 - val_loss: 0.6493 - val_accuracy: 0.6455\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6103 - accuracy: 0.6802 - val_loss: 0.6498 - val_accuracy: 0.6450\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6106 - accuracy: 0.6796 - val_loss: 0.6506 - val_accuracy: 0.6455\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6109 - accuracy: 0.6796 - val_loss: 0.6467 - val_accuracy: 0.6455\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6107 - accuracy: 0.6790 - val_loss: 0.6533 - val_accuracy: 0.6452\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6109 - accuracy: 0.6793 - val_loss: 0.6463 - val_accuracy: 0.6452\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6112 - accuracy: 0.6792 - val_loss: 0.6509 - val_accuracy: 0.6450\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6104 - accuracy: 0.6801 - val_loss: 0.6478 - val_accuracy: 0.6455\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6106 - accuracy: 0.6799 - val_loss: 0.6530 - val_accuracy: 0.6449\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6113 - accuracy: 0.6789 - val_loss: 0.6484 - val_accuracy: 0.6453\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6107 - accuracy: 0.6802 - val_loss: 0.6506 - val_accuracy: 0.6453\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6107 - accuracy: 0.6801 - val_loss: 0.6488 - val_accuracy: 0.6451\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6111 - accuracy: 0.6797 - val_loss: 0.6477 - val_accuracy: 0.6456\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6104 - accuracy: 0.6792 - val_loss: 0.6514 - val_accuracy: 0.6448\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6103 - accuracy: 0.6799 - val_loss: 0.6498 - val_accuracy: 0.6452\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6110 - accuracy: 0.6794 - val_loss: 0.6490 - val_accuracy: 0.6443\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6111 - accuracy: 0.6793 - val_loss: 0.6512 - val_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6110 - accuracy: 0.6795 - val_loss: 0.6460 - val_accuracy: 0.6448\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6106 - accuracy: 0.6788 - val_loss: 0.6530 - val_accuracy: 0.6451\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6103 - accuracy: 0.6791 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6110 - accuracy: 0.6802 - val_loss: 0.6456 - val_accuracy: 0.6449\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6113 - accuracy: 0.6791 - val_loss: 0.6547 - val_accuracy: 0.6460\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6106 - accuracy: 0.6800 - val_loss: 0.6475 - val_accuracy: 0.6452\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6101 - accuracy: 0.6795 - val_loss: 0.6527 - val_accuracy: 0.6446\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6107 - accuracy: 0.6789 - val_loss: 0.6487 - val_accuracy: 0.6447\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6108 - accuracy: 0.6795 - val_loss: 0.6517 - val_accuracy: 0.6453\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6108 - accuracy: 0.6784 - val_loss: 0.6485 - val_accuracy: 0.6453\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6100 - accuracy: 0.6794 - val_loss: 0.6511 - val_accuracy: 0.6451\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6099 - accuracy: 0.6798 - val_loss: 0.6524 - val_accuracy: 0.6455\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6106 - accuracy: 0.6792 - val_loss: 0.6463 - val_accuracy: 0.6452\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6103 - accuracy: 0.6802 - val_loss: 0.6520 - val_accuracy: 0.6456\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6106 - accuracy: 0.6801 - val_loss: 0.6468 - val_accuracy: 0.6452\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6109 - accuracy: 0.6795 - val_loss: 0.6509 - val_accuracy: 0.6452\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6104 - accuracy: 0.6792 - val_loss: 0.6522 - val_accuracy: 0.6450\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6102 - accuracy: 0.6800 - val_loss: 0.6489 - val_accuracy: 0.6452\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6108 - accuracy: 0.6801 - val_loss: 0.6490 - val_accuracy: 0.6452\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6107 - accuracy: 0.6794 - val_loss: 0.6521 - val_accuracy: 0.6452\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6103 - accuracy: 0.6800 - val_loss: 0.6461 - val_accuracy: 0.6449\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6108 - accuracy: 0.6790 - val_loss: 0.6565 - val_accuracy: 0.6449\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6108 - accuracy: 0.6796 - val_loss: 0.6461 - val_accuracy: 0.6449\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6106 - accuracy: 0.6792 - val_loss: 0.6525 - val_accuracy: 0.6443\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6105 - accuracy: 0.6796 - val_loss: 0.6519 - val_accuracy: 0.6447\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6103 - accuracy: 0.6802 - val_loss: 0.6490 - val_accuracy: 0.6450\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6103 - accuracy: 0.6798 - val_loss: 0.6540 - val_accuracy: 0.6452\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6101 - accuracy: 0.6805 - val_loss: 0.6461 - val_accuracy: 0.6451\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6100 - accuracy: 0.6797 - val_loss: 0.6549 - val_accuracy: 0.6451\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6103 - accuracy: 0.6799 - val_loss: 0.6488 - val_accuracy: 0.6453\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6100 - accuracy: 0.6806 - val_loss: 0.6526 - val_accuracy: 0.6450\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6107 - accuracy: 0.6795 - val_loss: 0.6507 - val_accuracy: 0.6456\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6100 - accuracy: 0.6805 - val_loss: 0.6477 - val_accuracy: 0.6453\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6106 - accuracy: 0.6782 - val_loss: 0.6515 - val_accuracy: 0.6457\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6100 - accuracy: 0.6798 - val_loss: 0.6542 - val_accuracy: 0.6450\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6103 - accuracy: 0.6798 - val_loss: 0.6477 - val_accuracy: 0.6450\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6104 - accuracy: 0.6790 - val_loss: 0.6553 - val_accuracy: 0.6442\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6111 - accuracy: 0.6801 - val_loss: 0.6471 - val_accuracy: 0.6459\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6105 - accuracy: 0.6794 - val_loss: 0.6523 - val_accuracy: 0.6447\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6104 - accuracy: 0.6793 - val_loss: 0.6495 - val_accuracy: 0.6451\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6103 - accuracy: 0.6789 - val_loss: 0.6511 - val_accuracy: 0.6448\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6097 - accuracy: 0.6802 - val_loss: 0.6537 - val_accuracy: 0.6450\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6096 - accuracy: 0.6806 - val_loss: 0.6469 - val_accuracy: 0.6458\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6100 - accuracy: 0.6797 - val_loss: 0.6523 - val_accuracy: 0.6452\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6102 - accuracy: 0.6793 - val_loss: 0.6507 - val_accuracy: 0.6453\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6097 - accuracy: 0.6795 - val_loss: 0.6492 - val_accuracy: 0.6447\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6098 - accuracy: 0.6797 - val_loss: 0.6531 - val_accuracy: 0.6453\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6103 - accuracy: 0.6801 - val_loss: 0.6476 - val_accuracy: 0.6466\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6107 - accuracy: 0.6788 - val_loss: 0.6493 - val_accuracy: 0.6456\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6104 - accuracy: 0.6791 - val_loss: 0.6528 - val_accuracy: 0.6449\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6100 - accuracy: 0.6804 - val_loss: 0.6470 - val_accuracy: 0.6449\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6109 - accuracy: 0.6796 - val_loss: 0.6529 - val_accuracy: 0.6448\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6100 - accuracy: 0.6809 - val_loss: 0.6487 - val_accuracy: 0.6447\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6105 - accuracy: 0.6797 - val_loss: 0.6516 - val_accuracy: 0.6446\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6103 - accuracy: 0.6796 - val_loss: 0.6516 - val_accuracy: 0.6450\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6106 - accuracy: 0.6792 - val_loss: 0.6525 - val_accuracy: 0.6451\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6097 - accuracy: 0.6799 - val_loss: 0.6486 - val_accuracy: 0.6446\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6093 - accuracy: 0.6801 - val_loss: 0.6546 - val_accuracy: 0.6446\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6101 - accuracy: 0.6794 - val_loss: 0.6480 - val_accuracy: 0.6450\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6101 - accuracy: 0.6796 - val_loss: 0.6526 - val_accuracy: 0.6456\n",
      "Epoch:  8\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6106 - accuracy: 0.6785 - val_loss: 0.6482 - val_accuracy: 0.6455\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6102 - accuracy: 0.6807 - val_loss: 0.6575 - val_accuracy: 0.6455\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6104 - accuracy: 0.6798 - val_loss: 0.6476 - val_accuracy: 0.6443\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6100 - accuracy: 0.6791 - val_loss: 0.6528 - val_accuracy: 0.6448\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6099 - accuracy: 0.6798 - val_loss: 0.6516 - val_accuracy: 0.6450\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6098 - accuracy: 0.6798 - val_loss: 0.6489 - val_accuracy: 0.6448\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6102 - accuracy: 0.6804 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6098 - accuracy: 0.6799 - val_loss: 0.6490 - val_accuracy: 0.6450\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6094 - accuracy: 0.6805 - val_loss: 0.6517 - val_accuracy: 0.6450\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6101 - accuracy: 0.6791 - val_loss: 0.6530 - val_accuracy: 0.6449\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6100 - accuracy: 0.6795 - val_loss: 0.6499 - val_accuracy: 0.6451\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6101 - accuracy: 0.6793 - val_loss: 0.6520 - val_accuracy: 0.6453\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6098 - accuracy: 0.6808 - val_loss: 0.6494 - val_accuracy: 0.6450\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6099 - accuracy: 0.6803 - val_loss: 0.6549 - val_accuracy: 0.6453\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6101 - accuracy: 0.6804 - val_loss: 0.6476 - val_accuracy: 0.6450\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6097 - accuracy: 0.6797 - val_loss: 0.6513 - val_accuracy: 0.6450\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6094 - accuracy: 0.6809 - val_loss: 0.6518 - val_accuracy: 0.6450\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6101 - accuracy: 0.6799 - val_loss: 0.6469 - val_accuracy: 0.6455\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6105 - accuracy: 0.6789 - val_loss: 0.6535 - val_accuracy: 0.6451\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6100 - accuracy: 0.6794 - val_loss: 0.6472 - val_accuracy: 0.6453\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6095 - accuracy: 0.6800 - val_loss: 0.6530 - val_accuracy: 0.6447\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6094 - accuracy: 0.6804 - val_loss: 0.6487 - val_accuracy: 0.6451\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6096 - accuracy: 0.6805 - val_loss: 0.6532 - val_accuracy: 0.6447\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6099 - accuracy: 0.6789 - val_loss: 0.6502 - val_accuracy: 0.6449\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6097 - accuracy: 0.6805 - val_loss: 0.6531 - val_accuracy: 0.6446\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6100 - accuracy: 0.6792 - val_loss: 0.6465 - val_accuracy: 0.6447\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6099 - accuracy: 0.6798 - val_loss: 0.6537 - val_accuracy: 0.6443\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6105 - accuracy: 0.6791 - val_loss: 0.6484 - val_accuracy: 0.6443\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6102 - accuracy: 0.6800 - val_loss: 0.6516 - val_accuracy: 0.6448\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6102 - accuracy: 0.6792 - val_loss: 0.6475 - val_accuracy: 0.6443\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6097 - accuracy: 0.6806 - val_loss: 0.6503 - val_accuracy: 0.6450\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6104 - accuracy: 0.6806 - val_loss: 0.6501 - val_accuracy: 0.6448\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6094 - accuracy: 0.6811 - val_loss: 0.6494 - val_accuracy: 0.6449\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6098 - accuracy: 0.6803 - val_loss: 0.6525 - val_accuracy: 0.6443\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6099 - accuracy: 0.6808 - val_loss: 0.6488 - val_accuracy: 0.6453\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6096 - accuracy: 0.6809 - val_loss: 0.6529 - val_accuracy: 0.6449\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6101 - accuracy: 0.6798 - val_loss: 0.6475 - val_accuracy: 0.6452\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6096 - accuracy: 0.6790 - val_loss: 0.6581 - val_accuracy: 0.6452\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6087 - accuracy: 0.6808 - val_loss: 0.6480 - val_accuracy: 0.6456\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6089 - accuracy: 0.6804 - val_loss: 0.6522 - val_accuracy: 0.6456\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6107 - accuracy: 0.6787 - val_loss: 0.6492 - val_accuracy: 0.6453\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6102 - accuracy: 0.6799 - val_loss: 0.6504 - val_accuracy: 0.6456\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6095 - accuracy: 0.6800 - val_loss: 0.6513 - val_accuracy: 0.6459\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6099 - accuracy: 0.6806 - val_loss: 0.6485 - val_accuracy: 0.6453\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6097 - accuracy: 0.6801 - val_loss: 0.6535 - val_accuracy: 0.6452\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6093 - accuracy: 0.6794 - val_loss: 0.6478 - val_accuracy: 0.6453\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6095 - accuracy: 0.6807 - val_loss: 0.6541 - val_accuracy: 0.6450\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.6098 - accuracy: 0.6800 - val_loss: 0.6494 - val_accuracy: 0.6450\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6098 - accuracy: 0.6798 - val_loss: 0.6514 - val_accuracy: 0.6453\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6092 - accuracy: 0.6808 - val_loss: 0.6499 - val_accuracy: 0.6449\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6098 - accuracy: 0.6806 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6091 - accuracy: 0.6795 - val_loss: 0.6515 - val_accuracy: 0.6456\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.6095 - accuracy: 0.6797 - val_loss: 0.6530 - val_accuracy: 0.6455\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6092 - accuracy: 0.6803 - val_loss: 0.6502 - val_accuracy: 0.6452\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6096 - accuracy: 0.6794 - val_loss: 0.6526 - val_accuracy: 0.6456\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6097 - accuracy: 0.6809 - val_loss: 0.6504 - val_accuracy: 0.6449\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.6101 - accuracy: 0.6805 - val_loss: 0.6504 - val_accuracy: 0.6449\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6091 - accuracy: 0.6803 - val_loss: 0.6505 - val_accuracy: 0.6446\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6090 - accuracy: 0.6806 - val_loss: 0.6514 - val_accuracy: 0.6453\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6099 - accuracy: 0.6800 - val_loss: 0.6488 - val_accuracy: 0.6451\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6092 - accuracy: 0.6810 - val_loss: 0.6535 - val_accuracy: 0.6456\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6096 - accuracy: 0.6796 - val_loss: 0.6503 - val_accuracy: 0.6456\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6100 - accuracy: 0.6801 - val_loss: 0.6511 - val_accuracy: 0.6448\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6093 - accuracy: 0.6785 - val_loss: 0.6512 - val_accuracy: 0.6451\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6091 - accuracy: 0.6804 - val_loss: 0.6501 - val_accuracy: 0.6455\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6093 - accuracy: 0.6806 - val_loss: 0.6500 - val_accuracy: 0.6449\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6098 - accuracy: 0.6801 - val_loss: 0.6500 - val_accuracy: 0.6451\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6093 - accuracy: 0.6800 - val_loss: 0.6546 - val_accuracy: 0.6455\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6094 - accuracy: 0.6803 - val_loss: 0.6479 - val_accuracy: 0.6453\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6090 - accuracy: 0.6815 - val_loss: 0.6559 - val_accuracy: 0.6447\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6096 - accuracy: 0.6799 - val_loss: 0.6484 - val_accuracy: 0.6456\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6094 - accuracy: 0.6802 - val_loss: 0.6510 - val_accuracy: 0.6450\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.6097 - accuracy: 0.6803 - val_loss: 0.6510 - val_accuracy: 0.6451\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.6092 - accuracy: 0.6795 - val_loss: 0.6511 - val_accuracy: 0.6451\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.6099 - accuracy: 0.6802 - val_loss: 0.6480 - val_accuracy: 0.6443\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 0.6088 - accuracy: 0.6802 - val_loss: 0.6524 - val_accuracy: 0.6449\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.6091 - accuracy: 0.6808 - val_loss: 0.6500 - val_accuracy: 0.6457\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 0.6096 - accuracy: 0.6794 - val_loss: 0.6512 - val_accuracy: 0.6450\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.6089 - accuracy: 0.6809 - val_loss: 0.6530 - val_accuracy: 0.6456\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6092 - accuracy: 0.6798 - val_loss: 0.6501 - val_accuracy: 0.6456\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.6093 - accuracy: 0.6797 - val_loss: 0.6524 - val_accuracy: 0.6450\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6087 - accuracy: 0.6796 - val_loss: 0.6538 - val_accuracy: 0.6452\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 0.6093 - accuracy: 0.6808 - val_loss: 0.6466 - val_accuracy: 0.6451\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.6095 - accuracy: 0.6803 - val_loss: 0.6560 - val_accuracy: 0.6455\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.6093 - accuracy: 0.6804 - val_loss: 0.6499 - val_accuracy: 0.6455\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6095 - accuracy: 0.6806 - val_loss: 0.6522 - val_accuracy: 0.6456\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6092 - accuracy: 0.6800 - val_loss: 0.6487 - val_accuracy: 0.6453\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6096 - accuracy: 0.6795 - val_loss: 0.6568 - val_accuracy: 0.6453\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6094 - accuracy: 0.6812 - val_loss: 0.6466 - val_accuracy: 0.6458\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6096 - accuracy: 0.6801 - val_loss: 0.6544 - val_accuracy: 0.6455\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6092 - accuracy: 0.6814 - val_loss: 0.6472 - val_accuracy: 0.6450\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6092 - accuracy: 0.6800 - val_loss: 0.6529 - val_accuracy: 0.6449\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6086 - accuracy: 0.6805 - val_loss: 0.6494 - val_accuracy: 0.6455\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6093 - accuracy: 0.6798 - val_loss: 0.6520 - val_accuracy: 0.6450\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6088 - accuracy: 0.6809 - val_loss: 0.6537 - val_accuracy: 0.6442\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6093 - accuracy: 0.6811 - val_loss: 0.6495 - val_accuracy: 0.6447\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6092 - accuracy: 0.6807 - val_loss: 0.6530 - val_accuracy: 0.6442\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6088 - accuracy: 0.6808 - val_loss: 0.6501 - val_accuracy: 0.6450\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6092 - accuracy: 0.6804 - val_loss: 0.6526 - val_accuracy: 0.6446\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6088 - accuracy: 0.6800 - val_loss: 0.6516 - val_accuracy: 0.6455\n",
      "Epoch:  9\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6090 - accuracy: 0.6816 - val_loss: 0.6551 - val_accuracy: 0.6449\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6094 - accuracy: 0.6805 - val_loss: 0.6496 - val_accuracy: 0.6447\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6094 - accuracy: 0.6803 - val_loss: 0.6513 - val_accuracy: 0.6447\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6092 - accuracy: 0.6803 - val_loss: 0.6534 - val_accuracy: 0.6448\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6090 - accuracy: 0.6818 - val_loss: 0.6485 - val_accuracy: 0.6456\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6091 - accuracy: 0.6803 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6091 - accuracy: 0.6806 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6089 - accuracy: 0.6800 - val_loss: 0.6525 - val_accuracy: 0.6453\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6086 - accuracy: 0.6805 - val_loss: 0.6537 - val_accuracy: 0.6446\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6088 - accuracy: 0.6804 - val_loss: 0.6513 - val_accuracy: 0.6463\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6085 - accuracy: 0.6806 - val_loss: 0.6514 - val_accuracy: 0.6457\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6090 - accuracy: 0.6810 - val_loss: 0.6530 - val_accuracy: 0.6449\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6090 - accuracy: 0.6803 - val_loss: 0.6527 - val_accuracy: 0.6455\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6087 - accuracy: 0.6812 - val_loss: 0.6494 - val_accuracy: 0.6450\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6092 - accuracy: 0.6813 - val_loss: 0.6549 - val_accuracy: 0.6449\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6092 - accuracy: 0.6798 - val_loss: 0.6479 - val_accuracy: 0.6451\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6090 - accuracy: 0.6797 - val_loss: 0.6546 - val_accuracy: 0.6441\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6090 - accuracy: 0.6803 - val_loss: 0.6488 - val_accuracy: 0.6453\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6091 - accuracy: 0.6801 - val_loss: 0.6538 - val_accuracy: 0.6452\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6090 - accuracy: 0.6801 - val_loss: 0.6509 - val_accuracy: 0.6455\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6091 - accuracy: 0.6806 - val_loss: 0.6492 - val_accuracy: 0.6455\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6092 - accuracy: 0.6804 - val_loss: 0.6525 - val_accuracy: 0.6459\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6093 - accuracy: 0.6803 - val_loss: 0.6498 - val_accuracy: 0.6456\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6093 - accuracy: 0.6806 - val_loss: 0.6571 - val_accuracy: 0.6452\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6086 - accuracy: 0.6804 - val_loss: 0.6482 - val_accuracy: 0.6461\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6085 - accuracy: 0.6817 - val_loss: 0.6528 - val_accuracy: 0.6456\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6090 - accuracy: 0.6810 - val_loss: 0.6532 - val_accuracy: 0.6455\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6090 - accuracy: 0.6807 - val_loss: 0.6530 - val_accuracy: 0.6457\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6085 - accuracy: 0.6814 - val_loss: 0.6512 - val_accuracy: 0.6455\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6083 - accuracy: 0.6810 - val_loss: 0.6537 - val_accuracy: 0.6453\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6088 - accuracy: 0.6811 - val_loss: 0.6507 - val_accuracy: 0.6452\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6090 - accuracy: 0.6805 - val_loss: 0.6507 - val_accuracy: 0.6448\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6082 - accuracy: 0.6812 - val_loss: 0.6573 - val_accuracy: 0.6449\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6094 - accuracy: 0.6808 - val_loss: 0.6466 - val_accuracy: 0.6451\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6090 - accuracy: 0.6802 - val_loss: 0.6556 - val_accuracy: 0.6461\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6092 - accuracy: 0.6807 - val_loss: 0.6469 - val_accuracy: 0.6456\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6088 - accuracy: 0.6807 - val_loss: 0.6562 - val_accuracy: 0.6449\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6092 - accuracy: 0.6804 - val_loss: 0.6478 - val_accuracy: 0.6451\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6093 - accuracy: 0.6806 - val_loss: 0.6507 - val_accuracy: 0.6451\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6089 - accuracy: 0.6805 - val_loss: 0.6574 - val_accuracy: 0.6453\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6090 - accuracy: 0.6818 - val_loss: 0.6483 - val_accuracy: 0.6451\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6087 - accuracy: 0.6801 - val_loss: 0.6536 - val_accuracy: 0.6452\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6080 - accuracy: 0.6811 - val_loss: 0.6500 - val_accuracy: 0.6452\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6084 - accuracy: 0.6811 - val_loss: 0.6539 - val_accuracy: 0.6449\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6093 - accuracy: 0.6806 - val_loss: 0.6515 - val_accuracy: 0.6452\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6091 - accuracy: 0.6807 - val_loss: 0.6533 - val_accuracy: 0.6448\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6090 - accuracy: 0.6800 - val_loss: 0.6486 - val_accuracy: 0.6451\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6090 - accuracy: 0.6805 - val_loss: 0.6595 - val_accuracy: 0.6451\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6086 - accuracy: 0.6814 - val_loss: 0.6495 - val_accuracy: 0.6452\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6087 - accuracy: 0.6809 - val_loss: 0.6555 - val_accuracy: 0.6451\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.6088 - accuracy: 0.6812 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6087 - accuracy: 0.6813 - val_loss: 0.6511 - val_accuracy: 0.6448\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6086 - accuracy: 0.6811 - val_loss: 0.6513 - val_accuracy: 0.6456\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6091 - accuracy: 0.6806 - val_loss: 0.6524 - val_accuracy: 0.6450\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6081 - accuracy: 0.6810 - val_loss: 0.6526 - val_accuracy: 0.6450\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6086 - accuracy: 0.6811 - val_loss: 0.6509 - val_accuracy: 0.6448\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6081 - accuracy: 0.6805 - val_loss: 0.6539 - val_accuracy: 0.6450\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6084 - accuracy: 0.6809 - val_loss: 0.6515 - val_accuracy: 0.6451\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6080 - accuracy: 0.6803 - val_loss: 0.6537 - val_accuracy: 0.6455\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6085 - accuracy: 0.6813 - val_loss: 0.6512 - val_accuracy: 0.6450\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6089 - accuracy: 0.6804 - val_loss: 0.6556 - val_accuracy: 0.6451\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6092 - accuracy: 0.6809 - val_loss: 0.6467 - val_accuracy: 0.6450\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6090 - accuracy: 0.6798 - val_loss: 0.6530 - val_accuracy: 0.6451\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6087 - accuracy: 0.6810 - val_loss: 0.6521 - val_accuracy: 0.6452\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6092 - accuracy: 0.6811 - val_loss: 0.6480 - val_accuracy: 0.6451\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6085 - accuracy: 0.6822 - val_loss: 0.6571 - val_accuracy: 0.6456\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6089 - accuracy: 0.6807 - val_loss: 0.6487 - val_accuracy: 0.6458\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6083 - accuracy: 0.6815 - val_loss: 0.6567 - val_accuracy: 0.6452\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6086 - accuracy: 0.6801 - val_loss: 0.6520 - val_accuracy: 0.6451\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6086 - accuracy: 0.6810 - val_loss: 0.6506 - val_accuracy: 0.6450\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6089 - accuracy: 0.6799 - val_loss: 0.6541 - val_accuracy: 0.6453\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6087 - accuracy: 0.6810 - val_loss: 0.6530 - val_accuracy: 0.6446\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6085 - accuracy: 0.6807 - val_loss: 0.6535 - val_accuracy: 0.6443\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6083 - accuracy: 0.6815 - val_loss: 0.6516 - val_accuracy: 0.6453\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6086 - accuracy: 0.6811 - val_loss: 0.6528 - val_accuracy: 0.6450\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6085 - accuracy: 0.6817 - val_loss: 0.6537 - val_accuracy: 0.6449\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6089 - accuracy: 0.6799 - val_loss: 0.6505 - val_accuracy: 0.6457\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6088 - accuracy: 0.6817 - val_loss: 0.6499 - val_accuracy: 0.6452\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6089 - accuracy: 0.6815 - val_loss: 0.6514 - val_accuracy: 0.6448\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6085 - accuracy: 0.6803 - val_loss: 0.6536 - val_accuracy: 0.6447\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6087 - accuracy: 0.6799 - val_loss: 0.6509 - val_accuracy: 0.6446\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6081 - accuracy: 0.6816 - val_loss: 0.6549 - val_accuracy: 0.6448\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6083 - accuracy: 0.6802 - val_loss: 0.6546 - val_accuracy: 0.6456\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6087 - accuracy: 0.6806 - val_loss: 0.6505 - val_accuracy: 0.6453\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6084 - accuracy: 0.6809 - val_loss: 0.6557 - val_accuracy: 0.6452\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6083 - accuracy: 0.6806 - val_loss: 0.6506 - val_accuracy: 0.6446\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6086 - accuracy: 0.6811 - val_loss: 0.6506 - val_accuracy: 0.6447\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6083 - accuracy: 0.6809 - val_loss: 0.6594 - val_accuracy: 0.6446\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6088 - accuracy: 0.6804 - val_loss: 0.6498 - val_accuracy: 0.6455\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6089 - accuracy: 0.6810 - val_loss: 0.6552 - val_accuracy: 0.6452\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6086 - accuracy: 0.6814 - val_loss: 0.6483 - val_accuracy: 0.6450\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6087 - accuracy: 0.6814 - val_loss: 0.6558 - val_accuracy: 0.6455\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6079 - accuracy: 0.6812 - val_loss: 0.6521 - val_accuracy: 0.6450\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6089 - accuracy: 0.6813 - val_loss: 0.6509 - val_accuracy: 0.6447\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6089 - accuracy: 0.6803 - val_loss: 0.6533 - val_accuracy: 0.6453\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6076 - accuracy: 0.6813 - val_loss: 0.6533 - val_accuracy: 0.6451\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6093 - accuracy: 0.6809 - val_loss: 0.6507 - val_accuracy: 0.6450\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6090 - accuracy: 0.6799 - val_loss: 0.6552 - val_accuracy: 0.6452\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6090 - accuracy: 0.6805 - val_loss: 0.6487 - val_accuracy: 0.6452\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6079 - accuracy: 0.6810 - val_loss: 0.6583 - val_accuracy: 0.6452\n",
      "Epoch:  10\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6083 - accuracy: 0.6806 - val_loss: 0.6504 - val_accuracy: 0.6448\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6082 - accuracy: 0.6813 - val_loss: 0.6527 - val_accuracy: 0.6444\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6079 - accuracy: 0.6816 - val_loss: 0.6516 - val_accuracy: 0.6442\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6082 - accuracy: 0.6806 - val_loss: 0.6533 - val_accuracy: 0.6449\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6086 - accuracy: 0.6806 - val_loss: 0.6504 - val_accuracy: 0.6447\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6085 - accuracy: 0.6811 - val_loss: 0.6545 - val_accuracy: 0.6448\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6091 - accuracy: 0.6809 - val_loss: 0.6549 - val_accuracy: 0.6440\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6084 - accuracy: 0.6805 - val_loss: 0.6473 - val_accuracy: 0.6450\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6093 - accuracy: 0.6798 - val_loss: 0.6574 - val_accuracy: 0.6447\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6075 - accuracy: 0.6819 - val_loss: 0.6475 - val_accuracy: 0.6450\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6083 - accuracy: 0.6803 - val_loss: 0.6577 - val_accuracy: 0.6440\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6084 - accuracy: 0.6812 - val_loss: 0.6502 - val_accuracy: 0.6447\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6079 - accuracy: 0.6814 - val_loss: 0.6522 - val_accuracy: 0.6446\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6084 - accuracy: 0.6820 - val_loss: 0.6507 - val_accuracy: 0.6448\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6084 - accuracy: 0.6811 - val_loss: 0.6536 - val_accuracy: 0.6444\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6085 - accuracy: 0.6808 - val_loss: 0.6478 - val_accuracy: 0.6443\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.6085 - accuracy: 0.6809 - val_loss: 0.6559 - val_accuracy: 0.6442\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6087 - accuracy: 0.6805 - val_loss: 0.6509 - val_accuracy: 0.6444\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6085 - accuracy: 0.6805 - val_loss: 0.6517 - val_accuracy: 0.6448\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6088 - accuracy: 0.6807 - val_loss: 0.6518 - val_accuracy: 0.6448\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6090 - accuracy: 0.6800 - val_loss: 0.6507 - val_accuracy: 0.6448\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6081 - accuracy: 0.6819 - val_loss: 0.6554 - val_accuracy: 0.6453\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6076 - accuracy: 0.6812 - val_loss: 0.6527 - val_accuracy: 0.6446\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6084 - accuracy: 0.6804 - val_loss: 0.6519 - val_accuracy: 0.6453\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6076 - accuracy: 0.6816 - val_loss: 0.6538 - val_accuracy: 0.6444\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6076 - accuracy: 0.6811 - val_loss: 0.6512 - val_accuracy: 0.6448\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6081 - accuracy: 0.6812 - val_loss: 0.6518 - val_accuracy: 0.6453\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6088 - accuracy: 0.6802 - val_loss: 0.6530 - val_accuracy: 0.6452\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6084 - accuracy: 0.6817 - val_loss: 0.6541 - val_accuracy: 0.6443\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6082 - accuracy: 0.6819 - val_loss: 0.6477 - val_accuracy: 0.6451\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6087 - accuracy: 0.6798 - val_loss: 0.6561 - val_accuracy: 0.6450\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6081 - accuracy: 0.6808 - val_loss: 0.6547 - val_accuracy: 0.6452\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6082 - accuracy: 0.6819 - val_loss: 0.6503 - val_accuracy: 0.6455\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6081 - accuracy: 0.6805 - val_loss: 0.6543 - val_accuracy: 0.6447\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6085 - accuracy: 0.6800 - val_loss: 0.6510 - val_accuracy: 0.6451\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6077 - accuracy: 0.6822 - val_loss: 0.6567 - val_accuracy: 0.6451\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6084 - accuracy: 0.6811 - val_loss: 0.6482 - val_accuracy: 0.6453\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6089 - accuracy: 0.6816 - val_loss: 0.6536 - val_accuracy: 0.6446\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6083 - accuracy: 0.6812 - val_loss: 0.6545 - val_accuracy: 0.6450\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6080 - accuracy: 0.6817 - val_loss: 0.6527 - val_accuracy: 0.6452\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6073 - accuracy: 0.6817 - val_loss: 0.6563 - val_accuracy: 0.6455\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6079 - accuracy: 0.6811 - val_loss: 0.6537 - val_accuracy: 0.6448\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6076 - accuracy: 0.6817 - val_loss: 0.6508 - val_accuracy: 0.6457\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6079 - accuracy: 0.6811 - val_loss: 0.6548 - val_accuracy: 0.6447\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6083 - accuracy: 0.6807 - val_loss: 0.6499 - val_accuracy: 0.6452\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6076 - accuracy: 0.6820 - val_loss: 0.6578 - val_accuracy: 0.6448\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6078 - accuracy: 0.6815 - val_loss: 0.6504 - val_accuracy: 0.6448\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6073 - accuracy: 0.6824 - val_loss: 0.6534 - val_accuracy: 0.6457\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6083 - accuracy: 0.6812 - val_loss: 0.6500 - val_accuracy: 0.6448\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6077 - accuracy: 0.6815 - val_loss: 0.6583 - val_accuracy: 0.6444\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6083 - accuracy: 0.6813 - val_loss: 0.6497 - val_accuracy: 0.6440\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6085 - accuracy: 0.6809 - val_loss: 0.6521 - val_accuracy: 0.6441\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6083 - accuracy: 0.6807 - val_loss: 0.6537 - val_accuracy: 0.6447\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6087 - accuracy: 0.6798 - val_loss: 0.6537 - val_accuracy: 0.6444\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6077 - accuracy: 0.6812 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6082 - accuracy: 0.6821 - val_loss: 0.6538 - val_accuracy: 0.6444\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6084 - accuracy: 0.6807 - val_loss: 0.6506 - val_accuracy: 0.6444\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6079 - accuracy: 0.6815 - val_loss: 0.6568 - val_accuracy: 0.6446\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6082 - accuracy: 0.6809 - val_loss: 0.6514 - val_accuracy: 0.6452\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.6080 - accuracy: 0.6818 - val_loss: 0.6554 - val_accuracy: 0.6451\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6076 - accuracy: 0.6827 - val_loss: 0.6535 - val_accuracy: 0.6444\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6073 - accuracy: 0.6816 - val_loss: 0.6520 - val_accuracy: 0.6455\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6078 - accuracy: 0.6806 - val_loss: 0.6514 - val_accuracy: 0.6448\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6076 - accuracy: 0.6822 - val_loss: 0.6588 - val_accuracy: 0.6455\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6075 - accuracy: 0.6817 - val_loss: 0.6499 - val_accuracy: 0.6452\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6085 - accuracy: 0.6811 - val_loss: 0.6515 - val_accuracy: 0.6453\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6085 - accuracy: 0.6802 - val_loss: 0.6566 - val_accuracy: 0.6446\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6079 - accuracy: 0.6823 - val_loss: 0.6481 - val_accuracy: 0.6458\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6077 - accuracy: 0.6806 - val_loss: 0.6567 - val_accuracy: 0.6451\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6080 - accuracy: 0.6805 - val_loss: 0.6521 - val_accuracy: 0.6451\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6076 - accuracy: 0.6825 - val_loss: 0.6531 - val_accuracy: 0.6461\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6084 - accuracy: 0.6814 - val_loss: 0.6517 - val_accuracy: 0.6448\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6085 - accuracy: 0.6812 - val_loss: 0.6497 - val_accuracy: 0.6460\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6084 - accuracy: 0.6807 - val_loss: 0.6529 - val_accuracy: 0.6460\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6082 - accuracy: 0.6815 - val_loss: 0.6569 - val_accuracy: 0.6450\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6081 - accuracy: 0.6809 - val_loss: 0.6511 - val_accuracy: 0.6459\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6085 - accuracy: 0.6817 - val_loss: 0.6525 - val_accuracy: 0.6460\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6085 - accuracy: 0.6803 - val_loss: 0.6558 - val_accuracy: 0.6456\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6084 - accuracy: 0.6800 - val_loss: 0.6514 - val_accuracy: 0.6457\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6078 - accuracy: 0.6809 - val_loss: 0.6546 - val_accuracy: 0.6455\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6080 - accuracy: 0.6818 - val_loss: 0.6513 - val_accuracy: 0.6456\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6072 - accuracy: 0.6822 - val_loss: 0.6535 - val_accuracy: 0.6451\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6076 - accuracy: 0.6819 - val_loss: 0.6527 - val_accuracy: 0.6457\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6075 - accuracy: 0.6820 - val_loss: 0.6521 - val_accuracy: 0.6456\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6072 - accuracy: 0.6821 - val_loss: 0.6540 - val_accuracy: 0.6456\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6075 - accuracy: 0.6809 - val_loss: 0.6558 - val_accuracy: 0.6455\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6084 - accuracy: 0.6807 - val_loss: 0.6498 - val_accuracy: 0.6456\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6075 - accuracy: 0.6820 - val_loss: 0.6558 - val_accuracy: 0.6450\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6080 - accuracy: 0.6816 - val_loss: 0.6516 - val_accuracy: 0.6452\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6085 - accuracy: 0.6807 - val_loss: 0.6546 - val_accuracy: 0.6457\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6077 - accuracy: 0.6821 - val_loss: 0.6529 - val_accuracy: 0.6456\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6078 - accuracy: 0.6804 - val_loss: 0.6503 - val_accuracy: 0.6461\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6074 - accuracy: 0.6819 - val_loss: 0.6568 - val_accuracy: 0.6459\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6074 - accuracy: 0.6817 - val_loss: 0.6532 - val_accuracy: 0.6459\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6082 - accuracy: 0.6805 - val_loss: 0.6509 - val_accuracy: 0.6461\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6081 - accuracy: 0.6813 - val_loss: 0.6552 - val_accuracy: 0.6462\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6074 - accuracy: 0.6817 - val_loss: 0.6536 - val_accuracy: 0.6459\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6080 - accuracy: 0.6815 - val_loss: 0.6564 - val_accuracy: 0.6452\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6070 - accuracy: 0.6827 - val_loss: 0.6527 - val_accuracy: 0.6448\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6071 - accuracy: 0.6822 - val_loss: 0.6519 - val_accuracy: 0.6453\n",
      "Epoch:  11\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6073 - accuracy: 0.6816 - val_loss: 0.6574 - val_accuracy: 0.6456\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6077 - accuracy: 0.6815 - val_loss: 0.6537 - val_accuracy: 0.6453\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6079 - accuracy: 0.6810 - val_loss: 0.6544 - val_accuracy: 0.6455\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6082 - accuracy: 0.6817 - val_loss: 0.6474 - val_accuracy: 0.6453\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6072 - accuracy: 0.6821 - val_loss: 0.6605 - val_accuracy: 0.6452\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6080 - accuracy: 0.6814 - val_loss: 0.6501 - val_accuracy: 0.6446\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6077 - accuracy: 0.6824 - val_loss: 0.6525 - val_accuracy: 0.6453\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6081 - accuracy: 0.6814 - val_loss: 0.6531 - val_accuracy: 0.6457\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6069 - accuracy: 0.6823 - val_loss: 0.6534 - val_accuracy: 0.6455\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6079 - accuracy: 0.6809 - val_loss: 0.6559 - val_accuracy: 0.6456\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6078 - accuracy: 0.6825 - val_loss: 0.6502 - val_accuracy: 0.6455\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6075 - accuracy: 0.6812 - val_loss: 0.6521 - val_accuracy: 0.6457\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6077 - accuracy: 0.6802 - val_loss: 0.6513 - val_accuracy: 0.6450\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6072 - accuracy: 0.6819 - val_loss: 0.6548 - val_accuracy: 0.6450\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6077 - accuracy: 0.6824 - val_loss: 0.6525 - val_accuracy: 0.6458\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6082 - accuracy: 0.6819 - val_loss: 0.6517 - val_accuracy: 0.6455\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6078 - accuracy: 0.6806 - val_loss: 0.6539 - val_accuracy: 0.6457\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6080 - accuracy: 0.6814 - val_loss: 0.6544 - val_accuracy: 0.6452\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6078 - accuracy: 0.6814 - val_loss: 0.6496 - val_accuracy: 0.6457\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6073 - accuracy: 0.6826 - val_loss: 0.6584 - val_accuracy: 0.6458\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6078 - accuracy: 0.6815 - val_loss: 0.6471 - val_accuracy: 0.6451\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6075 - accuracy: 0.6816 - val_loss: 0.6574 - val_accuracy: 0.6450\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6080 - accuracy: 0.6816 - val_loss: 0.6507 - val_accuracy: 0.6457\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6076 - accuracy: 0.6810 - val_loss: 0.6499 - val_accuracy: 0.6452\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6080 - accuracy: 0.6811 - val_loss: 0.6555 - val_accuracy: 0.6450\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6078 - accuracy: 0.6810 - val_loss: 0.6506 - val_accuracy: 0.6459\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6071 - accuracy: 0.6809 - val_loss: 0.6581 - val_accuracy: 0.6450\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6075 - accuracy: 0.6818 - val_loss: 0.6502 - val_accuracy: 0.6450\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6068 - accuracy: 0.6826 - val_loss: 0.6566 - val_accuracy: 0.6455\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6069 - accuracy: 0.6816 - val_loss: 0.6518 - val_accuracy: 0.6453\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6070 - accuracy: 0.6814 - val_loss: 0.6553 - val_accuracy: 0.6450\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6078 - accuracy: 0.6807 - val_loss: 0.6501 - val_accuracy: 0.6460\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6082 - accuracy: 0.6823 - val_loss: 0.6577 - val_accuracy: 0.6453\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6078 - accuracy: 0.6813 - val_loss: 0.6501 - val_accuracy: 0.6460\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6074 - accuracy: 0.6819 - val_loss: 0.6548 - val_accuracy: 0.6462\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6080 - accuracy: 0.6816 - val_loss: 0.6515 - val_accuracy: 0.6453\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6077 - accuracy: 0.6818 - val_loss: 0.6557 - val_accuracy: 0.6452\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6073 - accuracy: 0.6829 - val_loss: 0.6553 - val_accuracy: 0.6456\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6076 - accuracy: 0.6811 - val_loss: 0.6487 - val_accuracy: 0.6458\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6079 - accuracy: 0.6819 - val_loss: 0.6548 - val_accuracy: 0.6458\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6072 - accuracy: 0.6818 - val_loss: 0.6509 - val_accuracy: 0.6456\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6069 - accuracy: 0.6831 - val_loss: 0.6527 - val_accuracy: 0.6452\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6075 - accuracy: 0.6815 - val_loss: 0.6550 - val_accuracy: 0.6457\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6071 - accuracy: 0.6816 - val_loss: 0.6501 - val_accuracy: 0.6458\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6075 - accuracy: 0.6817 - val_loss: 0.6558 - val_accuracy: 0.6455\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6072 - accuracy: 0.6819 - val_loss: 0.6529 - val_accuracy: 0.6451\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6071 - accuracy: 0.6826 - val_loss: 0.6533 - val_accuracy: 0.6451\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6078 - accuracy: 0.6819 - val_loss: 0.6524 - val_accuracy: 0.6458\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6070 - accuracy: 0.6827 - val_loss: 0.6543 - val_accuracy: 0.6449\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6073 - accuracy: 0.6814 - val_loss: 0.6545 - val_accuracy: 0.6451\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6070 - accuracy: 0.6825 - val_loss: 0.6524 - val_accuracy: 0.6458\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6082 - accuracy: 0.6807 - val_loss: 0.6536 - val_accuracy: 0.6460\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6073 - accuracy: 0.6817 - val_loss: 0.6527 - val_accuracy: 0.6456\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6078 - accuracy: 0.6806 - val_loss: 0.6562 - val_accuracy: 0.6451\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6070 - accuracy: 0.6820 - val_loss: 0.6488 - val_accuracy: 0.6447\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6073 - accuracy: 0.6814 - val_loss: 0.6609 - val_accuracy: 0.6449\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6077 - accuracy: 0.6815 - val_loss: 0.6472 - val_accuracy: 0.6448\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6073 - accuracy: 0.6815 - val_loss: 0.6561 - val_accuracy: 0.6455\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6078 - accuracy: 0.6815 - val_loss: 0.6528 - val_accuracy: 0.6451\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6081 - accuracy: 0.6810 - val_loss: 0.6544 - val_accuracy: 0.6449\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6077 - accuracy: 0.6817 - val_loss: 0.6559 - val_accuracy: 0.6451\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6070 - accuracy: 0.6826 - val_loss: 0.6490 - val_accuracy: 0.6451\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6078 - accuracy: 0.6818 - val_loss: 0.6571 - val_accuracy: 0.6448\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6069 - accuracy: 0.6829 - val_loss: 0.6515 - val_accuracy: 0.6448\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6082 - accuracy: 0.6821 - val_loss: 0.6535 - val_accuracy: 0.6446\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6066 - accuracy: 0.6829 - val_loss: 0.6537 - val_accuracy: 0.6453\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6073 - accuracy: 0.6804 - val_loss: 0.6562 - val_accuracy: 0.6444\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6068 - accuracy: 0.6820 - val_loss: 0.6520 - val_accuracy: 0.6446\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6066 - accuracy: 0.6825 - val_loss: 0.6558 - val_accuracy: 0.6449\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6068 - accuracy: 0.6819 - val_loss: 0.6526 - val_accuracy: 0.6449\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6075 - accuracy: 0.6820 - val_loss: 0.6567 - val_accuracy: 0.6442\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6071 - accuracy: 0.6820 - val_loss: 0.6508 - val_accuracy: 0.6446\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6074 - accuracy: 0.6821 - val_loss: 0.6564 - val_accuracy: 0.6449\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6069 - accuracy: 0.6832 - val_loss: 0.6553 - val_accuracy: 0.6446\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6077 - accuracy: 0.6813 - val_loss: 0.6502 - val_accuracy: 0.6450\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6071 - accuracy: 0.6815 - val_loss: 0.6574 - val_accuracy: 0.6450\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6067 - accuracy: 0.6824 - val_loss: 0.6560 - val_accuracy: 0.6450\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6071 - accuracy: 0.6818 - val_loss: 0.6503 - val_accuracy: 0.6449\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6068 - accuracy: 0.6817 - val_loss: 0.6566 - val_accuracy: 0.6444\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6069 - accuracy: 0.6824 - val_loss: 0.6502 - val_accuracy: 0.6443\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6072 - accuracy: 0.6829 - val_loss: 0.6528 - val_accuracy: 0.6447\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.6076 - accuracy: 0.6820 - val_loss: 0.6532 - val_accuracy: 0.6448\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6069 - accuracy: 0.6819 - val_loss: 0.6498 - val_accuracy: 0.6450\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6070 - accuracy: 0.6819 - val_loss: 0.6593 - val_accuracy: 0.6448\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6070 - accuracy: 0.6822 - val_loss: 0.6501 - val_accuracy: 0.6449\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6076 - accuracy: 0.6823 - val_loss: 0.6520 - val_accuracy: 0.6443\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6070 - accuracy: 0.6823 - val_loss: 0.6549 - val_accuracy: 0.6447\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6068 - accuracy: 0.6827 - val_loss: 0.6516 - val_accuracy: 0.6452\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6070 - accuracy: 0.6827 - val_loss: 0.6532 - val_accuracy: 0.6451\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6078 - accuracy: 0.6810 - val_loss: 0.6524 - val_accuracy: 0.6457\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6070 - accuracy: 0.6829 - val_loss: 0.6539 - val_accuracy: 0.6452\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6066 - accuracy: 0.6826 - val_loss: 0.6559 - val_accuracy: 0.6448\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6075 - accuracy: 0.6812 - val_loss: 0.6539 - val_accuracy: 0.6450\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6076 - accuracy: 0.6820 - val_loss: 0.6544 - val_accuracy: 0.6451\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6068 - accuracy: 0.6820 - val_loss: 0.6513 - val_accuracy: 0.6447\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6078 - accuracy: 0.6817 - val_loss: 0.6562 - val_accuracy: 0.6450\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6071 - accuracy: 0.6820 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6069 - accuracy: 0.6820 - val_loss: 0.6547 - val_accuracy: 0.6455\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6069 - accuracy: 0.6827 - val_loss: 0.6527 - val_accuracy: 0.6452\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6070 - accuracy: 0.6812 - val_loss: 0.6565 - val_accuracy: 0.6462\n",
      "Epoch:  12\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6071 - accuracy: 0.6817 - val_loss: 0.6486 - val_accuracy: 0.6457\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6073 - accuracy: 0.6818 - val_loss: 0.6589 - val_accuracy: 0.6446\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6072 - accuracy: 0.6834 - val_loss: 0.6516 - val_accuracy: 0.6448\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6072 - accuracy: 0.6813 - val_loss: 0.6556 - val_accuracy: 0.6453\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6066 - accuracy: 0.6825 - val_loss: 0.6529 - val_accuracy: 0.6447\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6065 - accuracy: 0.6837 - val_loss: 0.6535 - val_accuracy: 0.6458\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6074 - accuracy: 0.6817 - val_loss: 0.6532 - val_accuracy: 0.6457\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6066 - accuracy: 0.6827 - val_loss: 0.6569 - val_accuracy: 0.6455\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6074 - accuracy: 0.6814 - val_loss: 0.6497 - val_accuracy: 0.6461\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6073 - accuracy: 0.6810 - val_loss: 0.6550 - val_accuracy: 0.6450\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6075 - accuracy: 0.6821 - val_loss: 0.6531 - val_accuracy: 0.6452\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6069 - accuracy: 0.6824 - val_loss: 0.6502 - val_accuracy: 0.6458\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6066 - accuracy: 0.6820 - val_loss: 0.6588 - val_accuracy: 0.6453\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6071 - accuracy: 0.6828 - val_loss: 0.6520 - val_accuracy: 0.6450\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6065 - accuracy: 0.6831 - val_loss: 0.6548 - val_accuracy: 0.6452\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6065 - accuracy: 0.6819 - val_loss: 0.6566 - val_accuracy: 0.6451\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6074 - accuracy: 0.6812 - val_loss: 0.6511 - val_accuracy: 0.6452\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6072 - accuracy: 0.6816 - val_loss: 0.6576 - val_accuracy: 0.6448\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6062 - accuracy: 0.6829 - val_loss: 0.6521 - val_accuracy: 0.6448\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6076 - accuracy: 0.6812 - val_loss: 0.6546 - val_accuracy: 0.6447\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6072 - accuracy: 0.6816 - val_loss: 0.6525 - val_accuracy: 0.6443\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6069 - accuracy: 0.6826 - val_loss: 0.6549 - val_accuracy: 0.6449\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6069 - accuracy: 0.6815 - val_loss: 0.6555 - val_accuracy: 0.6451\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6063 - accuracy: 0.6822 - val_loss: 0.6536 - val_accuracy: 0.6456\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6065 - accuracy: 0.6829 - val_loss: 0.6579 - val_accuracy: 0.6453\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6069 - accuracy: 0.6836 - val_loss: 0.6495 - val_accuracy: 0.6455\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6067 - accuracy: 0.6827 - val_loss: 0.6576 - val_accuracy: 0.6455\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6065 - accuracy: 0.6813 - val_loss: 0.6558 - val_accuracy: 0.6455\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6069 - accuracy: 0.6831 - val_loss: 0.6524 - val_accuracy: 0.6458\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6065 - accuracy: 0.6830 - val_loss: 0.6525 - val_accuracy: 0.6458\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6073 - accuracy: 0.6823 - val_loss: 0.6583 - val_accuracy: 0.6446\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6072 - accuracy: 0.6819 - val_loss: 0.6520 - val_accuracy: 0.6455\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6079 - accuracy: 0.6818 - val_loss: 0.6506 - val_accuracy: 0.6455\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6073 - accuracy: 0.6815 - val_loss: 0.6617 - val_accuracy: 0.6451\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6082 - accuracy: 0.6823 - val_loss: 0.6511 - val_accuracy: 0.6457\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6068 - accuracy: 0.6826 - val_loss: 0.6526 - val_accuracy: 0.6451\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6068 - accuracy: 0.6821 - val_loss: 0.6579 - val_accuracy: 0.6455\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6073 - accuracy: 0.6823 - val_loss: 0.6518 - val_accuracy: 0.6452\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6068 - accuracy: 0.6815 - val_loss: 0.6547 - val_accuracy: 0.6452\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6068 - accuracy: 0.6817 - val_loss: 0.6540 - val_accuracy: 0.6444\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6072 - accuracy: 0.6823 - val_loss: 0.6512 - val_accuracy: 0.6448\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6078 - accuracy: 0.6816 - val_loss: 0.6601 - val_accuracy: 0.6450\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6073 - accuracy: 0.6819 - val_loss: 0.6470 - val_accuracy: 0.6455\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6074 - accuracy: 0.6813 - val_loss: 0.6610 - val_accuracy: 0.6441\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6070 - accuracy: 0.6813 - val_loss: 0.6515 - val_accuracy: 0.6448\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6065 - accuracy: 0.6834 - val_loss: 0.6534 - val_accuracy: 0.6446\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6069 - accuracy: 0.6824 - val_loss: 0.6554 - val_accuracy: 0.6444\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6070 - accuracy: 0.6817 - val_loss: 0.6538 - val_accuracy: 0.6452\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6059 - accuracy: 0.6837 - val_loss: 0.6568 - val_accuracy: 0.6450\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6064 - accuracy: 0.6816 - val_loss: 0.6534 - val_accuracy: 0.6448\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6062 - accuracy: 0.6822 - val_loss: 0.6546 - val_accuracy: 0.6441\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6060 - accuracy: 0.6826 - val_loss: 0.6593 - val_accuracy: 0.6444\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6074 - accuracy: 0.6820 - val_loss: 0.6529 - val_accuracy: 0.6447\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6064 - accuracy: 0.6830 - val_loss: 0.6556 - val_accuracy: 0.6450\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6068 - accuracy: 0.6814 - val_loss: 0.6543 - val_accuracy: 0.6455\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6067 - accuracy: 0.6831 - val_loss: 0.6541 - val_accuracy: 0.6451\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6068 - accuracy: 0.6821 - val_loss: 0.6582 - val_accuracy: 0.6452\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6064 - accuracy: 0.6830 - val_loss: 0.6533 - val_accuracy: 0.6447\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6070 - accuracy: 0.6819 - val_loss: 0.6546 - val_accuracy: 0.6451\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6074 - accuracy: 0.6817 - val_loss: 0.6539 - val_accuracy: 0.6449\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6062 - accuracy: 0.6827 - val_loss: 0.6573 - val_accuracy: 0.6441\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6077 - accuracy: 0.6817 - val_loss: 0.6548 - val_accuracy: 0.6450\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6072 - accuracy: 0.6815 - val_loss: 0.6512 - val_accuracy: 0.6448\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6067 - accuracy: 0.6818 - val_loss: 0.6606 - val_accuracy: 0.6452\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6071 - accuracy: 0.6811 - val_loss: 0.6495 - val_accuracy: 0.6453\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6073 - accuracy: 0.6816 - val_loss: 0.6553 - val_accuracy: 0.6449\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6062 - accuracy: 0.6822 - val_loss: 0.6541 - val_accuracy: 0.6457\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6065 - accuracy: 0.6822 - val_loss: 0.6541 - val_accuracy: 0.6451\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6065 - accuracy: 0.6831 - val_loss: 0.6548 - val_accuracy: 0.6449\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6065 - accuracy: 0.6822 - val_loss: 0.6554 - val_accuracy: 0.6446\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6069 - accuracy: 0.6822 - val_loss: 0.6517 - val_accuracy: 0.6452\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6072 - accuracy: 0.6813 - val_loss: 0.6523 - val_accuracy: 0.6453\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6063 - accuracy: 0.6823 - val_loss: 0.6569 - val_accuracy: 0.6465\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6071 - accuracy: 0.6814 - val_loss: 0.6511 - val_accuracy: 0.6456\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6070 - accuracy: 0.6820 - val_loss: 0.6587 - val_accuracy: 0.6448\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6058 - accuracy: 0.6826 - val_loss: 0.6546 - val_accuracy: 0.6453\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6067 - accuracy: 0.6821 - val_loss: 0.6519 - val_accuracy: 0.6451\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6065 - accuracy: 0.6821 - val_loss: 0.6577 - val_accuracy: 0.6447\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6062 - accuracy: 0.6820 - val_loss: 0.6541 - val_accuracy: 0.6452\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6064 - accuracy: 0.6827 - val_loss: 0.6555 - val_accuracy: 0.6460\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6065 - accuracy: 0.6826 - val_loss: 0.6555 - val_accuracy: 0.6461\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6067 - accuracy: 0.6829 - val_loss: 0.6522 - val_accuracy: 0.6457\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6065 - accuracy: 0.6821 - val_loss: 0.6567 - val_accuracy: 0.6453\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6069 - accuracy: 0.6818 - val_loss: 0.6544 - val_accuracy: 0.6451\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6073 - accuracy: 0.6815 - val_loss: 0.6554 - val_accuracy: 0.6453\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6066 - accuracy: 0.6812 - val_loss: 0.6549 - val_accuracy: 0.6446\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6070 - accuracy: 0.6818 - val_loss: 0.6545 - val_accuracy: 0.6453\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6062 - accuracy: 0.6821 - val_loss: 0.6555 - val_accuracy: 0.6452\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6064 - accuracy: 0.6823 - val_loss: 0.6528 - val_accuracy: 0.6447\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6071 - accuracy: 0.6809 - val_loss: 0.6565 - val_accuracy: 0.6453\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6067 - accuracy: 0.6820 - val_loss: 0.6550 - val_accuracy: 0.6456\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6067 - accuracy: 0.6822 - val_loss: 0.6547 - val_accuracy: 0.6458\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6065 - accuracy: 0.6827 - val_loss: 0.6538 - val_accuracy: 0.6462\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6063 - accuracy: 0.6835 - val_loss: 0.6547 - val_accuracy: 0.6459\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6057 - accuracy: 0.6829 - val_loss: 0.6558 - val_accuracy: 0.6461\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6069 - accuracy: 0.6817 - val_loss: 0.6550 - val_accuracy: 0.6459\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6064 - accuracy: 0.6825 - val_loss: 0.6527 - val_accuracy: 0.6463\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6068 - accuracy: 0.6822 - val_loss: 0.6556 - val_accuracy: 0.6457\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6062 - accuracy: 0.6835 - val_loss: 0.6553 - val_accuracy: 0.6452\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6052 - accuracy: 0.6839 - val_loss: 0.6545 - val_accuracy: 0.6455\n",
      "Epoch:  13\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6059 - accuracy: 0.6824 - val_loss: 0.6547 - val_accuracy: 0.6456\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6064 - accuracy: 0.6819 - val_loss: 0.6554 - val_accuracy: 0.6461\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6067 - accuracy: 0.6829 - val_loss: 0.6513 - val_accuracy: 0.6462\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6063 - accuracy: 0.6826 - val_loss: 0.6536 - val_accuracy: 0.6455\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6070 - accuracy: 0.6823 - val_loss: 0.6587 - val_accuracy: 0.6457\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6070 - accuracy: 0.6824 - val_loss: 0.6494 - val_accuracy: 0.6451\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.6059 - accuracy: 0.6829 - val_loss: 0.6579 - val_accuracy: 0.6452\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6061 - accuracy: 0.6832 - val_loss: 0.6595 - val_accuracy: 0.6451\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6060 - accuracy: 0.6840 - val_loss: 0.6519 - val_accuracy: 0.6442\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6066 - accuracy: 0.6815 - val_loss: 0.6550 - val_accuracy: 0.6452\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6063 - accuracy: 0.6830 - val_loss: 0.6569 - val_accuracy: 0.6455\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6068 - accuracy: 0.6819 - val_loss: 0.6522 - val_accuracy: 0.6453\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6065 - accuracy: 0.6819 - val_loss: 0.6565 - val_accuracy: 0.6442\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6068 - accuracy: 0.6821 - val_loss: 0.6541 - val_accuracy: 0.6437\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6061 - accuracy: 0.6829 - val_loss: 0.6577 - val_accuracy: 0.6444\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6072 - accuracy: 0.6819 - val_loss: 0.6525 - val_accuracy: 0.6451\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6064 - accuracy: 0.6823 - val_loss: 0.6560 - val_accuracy: 0.6452\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6068 - accuracy: 0.6823 - val_loss: 0.6520 - val_accuracy: 0.6456\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6064 - accuracy: 0.6828 - val_loss: 0.6561 - val_accuracy: 0.6453\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6072 - accuracy: 0.6818 - val_loss: 0.6529 - val_accuracy: 0.6457\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6067 - accuracy: 0.6828 - val_loss: 0.6548 - val_accuracy: 0.6450\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6072 - accuracy: 0.6819 - val_loss: 0.6534 - val_accuracy: 0.6439\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6068 - accuracy: 0.6819 - val_loss: 0.6541 - val_accuracy: 0.6453\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6060 - accuracy: 0.6825 - val_loss: 0.6539 - val_accuracy: 0.6450\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6061 - accuracy: 0.6824 - val_loss: 0.6546 - val_accuracy: 0.6451\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6069 - accuracy: 0.6816 - val_loss: 0.6533 - val_accuracy: 0.6455\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6063 - accuracy: 0.6818 - val_loss: 0.6574 - val_accuracy: 0.6456\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6062 - accuracy: 0.6827 - val_loss: 0.6485 - val_accuracy: 0.6455\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6066 - accuracy: 0.6829 - val_loss: 0.6613 - val_accuracy: 0.6461\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6058 - accuracy: 0.6826 - val_loss: 0.6536 - val_accuracy: 0.6451\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6058 - accuracy: 0.6827 - val_loss: 0.6564 - val_accuracy: 0.6456\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6060 - accuracy: 0.6831 - val_loss: 0.6568 - val_accuracy: 0.6451\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6063 - accuracy: 0.6818 - val_loss: 0.6521 - val_accuracy: 0.6459\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6065 - accuracy: 0.6824 - val_loss: 0.6583 - val_accuracy: 0.6455\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6059 - accuracy: 0.6824 - val_loss: 0.6553 - val_accuracy: 0.6457\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6057 - accuracy: 0.6826 - val_loss: 0.6590 - val_accuracy: 0.6458\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6062 - accuracy: 0.6825 - val_loss: 0.6493 - val_accuracy: 0.6453\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6059 - accuracy: 0.6828 - val_loss: 0.6629 - val_accuracy: 0.6450\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6061 - accuracy: 0.6822 - val_loss: 0.6498 - val_accuracy: 0.6458\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6064 - accuracy: 0.6827 - val_loss: 0.6604 - val_accuracy: 0.6462\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6063 - accuracy: 0.6828 - val_loss: 0.6486 - val_accuracy: 0.6460\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6069 - accuracy: 0.6810 - val_loss: 0.6601 - val_accuracy: 0.6460\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6063 - accuracy: 0.6826 - val_loss: 0.6527 - val_accuracy: 0.6457\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6066 - accuracy: 0.6819 - val_loss: 0.6525 - val_accuracy: 0.6462\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6062 - accuracy: 0.6825 - val_loss: 0.6563 - val_accuracy: 0.6463\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6062 - accuracy: 0.6825 - val_loss: 0.6539 - val_accuracy: 0.6451\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6053 - accuracy: 0.6834 - val_loss: 0.6563 - val_accuracy: 0.6453\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6065 - accuracy: 0.6820 - val_loss: 0.6593 - val_accuracy: 0.6457\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6065 - accuracy: 0.6813 - val_loss: 0.6565 - val_accuracy: 0.6458\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6061 - accuracy: 0.6829 - val_loss: 0.6539 - val_accuracy: 0.6448\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6059 - accuracy: 0.6837 - val_loss: 0.6557 - val_accuracy: 0.6452\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6060 - accuracy: 0.6828 - val_loss: 0.6580 - val_accuracy: 0.6444\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6064 - accuracy: 0.6826 - val_loss: 0.6512 - val_accuracy: 0.6455\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6066 - accuracy: 0.6828 - val_loss: 0.6570 - val_accuracy: 0.6451\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6069 - accuracy: 0.6820 - val_loss: 0.6544 - val_accuracy: 0.6449\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6057 - accuracy: 0.6836 - val_loss: 0.6522 - val_accuracy: 0.6451\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6064 - accuracy: 0.6832 - val_loss: 0.6582 - val_accuracy: 0.6459\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.6061 - accuracy: 0.6828 - val_loss: 0.6530 - val_accuracy: 0.6453\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6059 - accuracy: 0.6827 - val_loss: 0.6552 - val_accuracy: 0.6448\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6063 - accuracy: 0.6832 - val_loss: 0.6540 - val_accuracy: 0.6443\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6062 - accuracy: 0.6835 - val_loss: 0.6528 - val_accuracy: 0.6452\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6065 - accuracy: 0.6835 - val_loss: 0.6550 - val_accuracy: 0.6455\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6062 - accuracy: 0.6829 - val_loss: 0.6528 - val_accuracy: 0.6449\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6067 - accuracy: 0.6820 - val_loss: 0.6540 - val_accuracy: 0.6449\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6060 - accuracy: 0.6834 - val_loss: 0.6536 - val_accuracy: 0.6450\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6065 - accuracy: 0.6828 - val_loss: 0.6538 - val_accuracy: 0.6446\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6057 - accuracy: 0.6830 - val_loss: 0.6533 - val_accuracy: 0.6444\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6062 - accuracy: 0.6819 - val_loss: 0.6566 - val_accuracy: 0.6447\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6059 - accuracy: 0.6832 - val_loss: 0.6527 - val_accuracy: 0.6453\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6056 - accuracy: 0.6833 - val_loss: 0.6573 - val_accuracy: 0.6452\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6060 - accuracy: 0.6824 - val_loss: 0.6499 - val_accuracy: 0.6446\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6064 - accuracy: 0.6820 - val_loss: 0.6594 - val_accuracy: 0.6455\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6060 - accuracy: 0.6829 - val_loss: 0.6541 - val_accuracy: 0.6440\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6063 - accuracy: 0.6826 - val_loss: 0.6538 - val_accuracy: 0.6447\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6057 - accuracy: 0.6825 - val_loss: 0.6609 - val_accuracy: 0.6447\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6058 - accuracy: 0.6835 - val_loss: 0.6536 - val_accuracy: 0.6451\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6062 - accuracy: 0.6823 - val_loss: 0.6569 - val_accuracy: 0.6455\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6069 - accuracy: 0.6822 - val_loss: 0.6517 - val_accuracy: 0.6459\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6068 - accuracy: 0.6824 - val_loss: 0.6567 - val_accuracy: 0.6458\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6058 - accuracy: 0.6825 - val_loss: 0.6551 - val_accuracy: 0.6451\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6063 - accuracy: 0.6823 - val_loss: 0.6537 - val_accuracy: 0.6447\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6061 - accuracy: 0.6830 - val_loss: 0.6557 - val_accuracy: 0.6448\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6061 - accuracy: 0.6832 - val_loss: 0.6556 - val_accuracy: 0.6455\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6057 - accuracy: 0.6817 - val_loss: 0.6551 - val_accuracy: 0.6455\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6064 - accuracy: 0.6820 - val_loss: 0.6536 - val_accuracy: 0.6453\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6069 - accuracy: 0.6826 - val_loss: 0.6537 - val_accuracy: 0.6450\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6055 - accuracy: 0.6830 - val_loss: 0.6566 - val_accuracy: 0.6441\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6058 - accuracy: 0.6822 - val_loss: 0.6566 - val_accuracy: 0.6443\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6058 - accuracy: 0.6834 - val_loss: 0.6582 - val_accuracy: 0.6439\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6061 - accuracy: 0.6834 - val_loss: 0.6525 - val_accuracy: 0.6437\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6062 - accuracy: 0.6833 - val_loss: 0.6595 - val_accuracy: 0.6449\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6055 - accuracy: 0.6830 - val_loss: 0.6532 - val_accuracy: 0.6442\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6067 - accuracy: 0.6823 - val_loss: 0.6536 - val_accuracy: 0.6461\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6059 - accuracy: 0.6834 - val_loss: 0.6559 - val_accuracy: 0.6453\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6058 - accuracy: 0.6829 - val_loss: 0.6556 - val_accuracy: 0.6460\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6059 - accuracy: 0.6827 - val_loss: 0.6516 - val_accuracy: 0.6457\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6066 - accuracy: 0.6828 - val_loss: 0.6565 - val_accuracy: 0.6452\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6065 - accuracy: 0.6823 - val_loss: 0.6533 - val_accuracy: 0.6448\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6056 - accuracy: 0.6837 - val_loss: 0.6559 - val_accuracy: 0.6438\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6067 - accuracy: 0.6827 - val_loss: 0.6535 - val_accuracy: 0.6443\n",
      "Epoch:  14\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6062 - accuracy: 0.6828 - val_loss: 0.6534 - val_accuracy: 0.6450\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6061 - accuracy: 0.6828 - val_loss: 0.6553 - val_accuracy: 0.6443\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6064 - accuracy: 0.6813 - val_loss: 0.6543 - val_accuracy: 0.6446\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6056 - accuracy: 0.6833 - val_loss: 0.6561 - val_accuracy: 0.6437\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6064 - accuracy: 0.6834 - val_loss: 0.6538 - val_accuracy: 0.6442\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6059 - accuracy: 0.6827 - val_loss: 0.6564 - val_accuracy: 0.6443\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6065 - accuracy: 0.6829 - val_loss: 0.6555 - val_accuracy: 0.6453\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6059 - accuracy: 0.6822 - val_loss: 0.6549 - val_accuracy: 0.6444\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6059 - accuracy: 0.6833 - val_loss: 0.6532 - val_accuracy: 0.6448\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6067 - accuracy: 0.6822 - val_loss: 0.6565 - val_accuracy: 0.6451\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6056 - accuracy: 0.6835 - val_loss: 0.6510 - val_accuracy: 0.6452\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6057 - accuracy: 0.6827 - val_loss: 0.6589 - val_accuracy: 0.6442\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.6062 - accuracy: 0.6838 - val_loss: 0.6508 - val_accuracy: 0.6448\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6057 - accuracy: 0.6829 - val_loss: 0.6592 - val_accuracy: 0.6448\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6061 - accuracy: 0.6828 - val_loss: 0.6504 - val_accuracy: 0.6447\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6061 - accuracy: 0.6829 - val_loss: 0.6588 - val_accuracy: 0.6448\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.6059 - accuracy: 0.6836 - val_loss: 0.6540 - val_accuracy: 0.6446\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6059 - accuracy: 0.6823 - val_loss: 0.6582 - val_accuracy: 0.6460\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6061 - accuracy: 0.6825 - val_loss: 0.6542 - val_accuracy: 0.6452\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6057 - accuracy: 0.6833 - val_loss: 0.6555 - val_accuracy: 0.6449\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6060 - accuracy: 0.6820 - val_loss: 0.6508 - val_accuracy: 0.6450\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.6058 - accuracy: 0.6833 - val_loss: 0.6560 - val_accuracy: 0.6446\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6052 - accuracy: 0.6826 - val_loss: 0.6578 - val_accuracy: 0.6453\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6063 - accuracy: 0.6836 - val_loss: 0.6547 - val_accuracy: 0.6441\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6060 - accuracy: 0.6826 - val_loss: 0.6519 - val_accuracy: 0.6448\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6071 - accuracy: 0.6825 - val_loss: 0.6579 - val_accuracy: 0.6441\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6065 - accuracy: 0.6821 - val_loss: 0.6517 - val_accuracy: 0.6433\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6058 - accuracy: 0.6830 - val_loss: 0.6576 - val_accuracy: 0.6441\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6054 - accuracy: 0.6833 - val_loss: 0.6537 - val_accuracy: 0.6440\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6064 - accuracy: 0.6821 - val_loss: 0.6520 - val_accuracy: 0.6444\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6058 - accuracy: 0.6824 - val_loss: 0.6614 - val_accuracy: 0.6438\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6066 - accuracy: 0.6820 - val_loss: 0.6531 - val_accuracy: 0.6451\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6064 - accuracy: 0.6819 - val_loss: 0.6535 - val_accuracy: 0.6448\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.6055 - accuracy: 0.6835 - val_loss: 0.6558 - val_accuracy: 0.6446\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.6054 - accuracy: 0.6825 - val_loss: 0.6534 - val_accuracy: 0.6438\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6062 - accuracy: 0.6821 - val_loss: 0.6538 - val_accuracy: 0.6438\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6061 - accuracy: 0.6825 - val_loss: 0.6528 - val_accuracy: 0.6432\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6059 - accuracy: 0.6831 - val_loss: 0.6592 - val_accuracy: 0.6440\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.6057 - accuracy: 0.6828 - val_loss: 0.6484 - val_accuracy: 0.6434\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.6059 - accuracy: 0.6828 - val_loss: 0.6599 - val_accuracy: 0.6451\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6060 - accuracy: 0.6828 - val_loss: 0.6524 - val_accuracy: 0.6442\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6051 - accuracy: 0.6837 - val_loss: 0.6579 - val_accuracy: 0.6443\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.6055 - accuracy: 0.6834 - val_loss: 0.6552 - val_accuracy: 0.6446\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6065 - accuracy: 0.6824 - val_loss: 0.6573 - val_accuracy: 0.6441\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6058 - accuracy: 0.6830 - val_loss: 0.6509 - val_accuracy: 0.6440\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6069 - accuracy: 0.6819 - val_loss: 0.6626 - val_accuracy: 0.6446\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.6059 - accuracy: 0.6833 - val_loss: 0.6523 - val_accuracy: 0.6452\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6057 - accuracy: 0.6833 - val_loss: 0.6573 - val_accuracy: 0.6457\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.6059 - accuracy: 0.6825 - val_loss: 0.6577 - val_accuracy: 0.6458\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6056 - accuracy: 0.6826 - val_loss: 0.6548 - val_accuracy: 0.6457\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6061 - accuracy: 0.6822 - val_loss: 0.6559 - val_accuracy: 0.6450\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6053 - accuracy: 0.6836 - val_loss: 0.6551 - val_accuracy: 0.6451\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 33s 7s/step - loss: 0.6054 - accuracy: 0.6833 - val_loss: 0.6553 - val_accuracy: 0.6439\n",
      "Epoch 54/100\n",
      "1/6 [====>.........................] - ETA: 32:01 - loss: 0.6040 - accuracy: 0.6821"
     ]
    }
   ],
   "source": [
    "column_model.trainable = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 22:10:43.155693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 4.2486 - accuracy: 0.5010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 22:10:46.093216: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 233ms/step - loss: 4.2486 - accuracy: 0.5010 - val_loss: 4.1465 - val_accuracy: 0.4925\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 4.0639 - accuracy: 0.5002 - val_loss: 3.9654 - val_accuracy: 0.4925\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 3.8861 - accuracy: 0.5073 - val_loss: 3.7919 - val_accuracy: 0.4925\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 3.7163 - accuracy: 0.5087 - val_loss: 3.6267 - val_accuracy: 0.4925\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 3.5547 - accuracy: 0.5086 - val_loss: 3.4696 - val_accuracy: 0.4925\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 3.4012 - accuracy: 0.5094 - val_loss: 3.3203 - val_accuracy: 0.4925\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 3.2553 - accuracy: 0.5090 - val_loss: 3.1786 - val_accuracy: 0.4925\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 3.1169 - accuracy: 0.5069 - val_loss: 3.0439 - val_accuracy: 0.4925\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 2.9853 - accuracy: 0.5106 - val_loss: 2.9161 - val_accuracy: 0.4925\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 2.8605 - accuracy: 0.5091 - val_loss: 2.7947 - val_accuracy: 0.4925\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 2.7419 - accuracy: 0.5102 - val_loss: 2.6795 - val_accuracy: 0.4925\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 2.6294 - accuracy: 0.5133 - val_loss: 2.5701 - val_accuracy: 0.5075\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 2.5225 - accuracy: 0.5145 - val_loss: 2.4663 - val_accuracy: 0.5075\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 2.4211 - accuracy: 0.5092 - val_loss: 2.3678 - val_accuracy: 0.5075\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 2.3248 - accuracy: 0.5180 - val_loss: 2.2743 - val_accuracy: 0.5075\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 2.2333 - accuracy: 0.5298 - val_loss: 2.1856 - val_accuracy: 0.5075\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 2.1457 - accuracy: 0.5456 - val_loss: 2.1008 - val_accuracy: 0.5131\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 2.0550 - accuracy: 0.5630 - val_loss: 2.0154 - val_accuracy: 0.5583\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 1.9670 - accuracy: 0.6069 - val_loss: 1.9274 - val_accuracy: 0.5997\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 1.8829 - accuracy: 0.6257 - val_loss: 1.8439 - val_accuracy: 0.6167\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 1.8019 - accuracy: 0.6374 - val_loss: 1.7654 - val_accuracy: 0.6288\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 1.7262 - accuracy: 0.6433 - val_loss: 1.7018 - val_accuracy: 0.6353\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 1.6588 - accuracy: 0.6505 - val_loss: 1.6426 - val_accuracy: 0.6382\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 1.5980 - accuracy: 0.6535 - val_loss: 1.5832 - val_accuracy: 0.6394\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 1.5418 - accuracy: 0.6554 - val_loss: 1.5280 - val_accuracy: 0.6402\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 1.4883 - accuracy: 0.6550 - val_loss: 1.4767 - val_accuracy: 0.6400\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 1.4380 - accuracy: 0.6562 - val_loss: 1.4290 - val_accuracy: 0.6400\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 1.3895 - accuracy: 0.6584 - val_loss: 1.3822 - val_accuracy: 0.6414\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 1.3447 - accuracy: 0.6597 - val_loss: 1.3397 - val_accuracy: 0.6407\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 1.3018 - accuracy: 0.6601 - val_loss: 1.2986 - val_accuracy: 0.6424\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 1.2624 - accuracy: 0.6594 - val_loss: 1.2603 - val_accuracy: 0.6418\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 1.2248 - accuracy: 0.6589 - val_loss: 1.2240 - val_accuracy: 0.6409\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 1.1888 - accuracy: 0.6606 - val_loss: 1.1892 - val_accuracy: 0.6414\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 1.1547 - accuracy: 0.6614 - val_loss: 1.1564 - val_accuracy: 0.6428\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 1.1229 - accuracy: 0.6602 - val_loss: 1.1262 - val_accuracy: 0.6431\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 1.0935 - accuracy: 0.6603 - val_loss: 1.0966 - val_accuracy: 0.6426\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 2s 130ms/step - loss: 1.0639 - accuracy: 0.6624 - val_loss: 1.0691 - val_accuracy: 0.6432\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 1.0374 - accuracy: 0.6610 - val_loss: 1.0433 - val_accuracy: 0.6425\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 1.0122 - accuracy: 0.6622 - val_loss: 1.0183 - val_accuracy: 0.6428\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.9890 - accuracy: 0.6624 - val_loss: 0.9972 - val_accuracy: 0.6428\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.9665 - accuracy: 0.6627 - val_loss: 0.9734 - val_accuracy: 0.6431\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.9449 - accuracy: 0.6633 - val_loss: 0.9537 - val_accuracy: 0.6432\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.9252 - accuracy: 0.6639 - val_loss: 0.9344 - val_accuracy: 0.6431\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.9068 - accuracy: 0.6635 - val_loss: 0.9156 - val_accuracy: 0.6434\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.8885 - accuracy: 0.6646 - val_loss: 0.8991 - val_accuracy: 0.6433\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.8716 - accuracy: 0.6651 - val_loss: 0.8829 - val_accuracy: 0.6438\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.8570 - accuracy: 0.6644 - val_loss: 0.8687 - val_accuracy: 0.6434\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.8420 - accuracy: 0.6656 - val_loss: 0.8514 - val_accuracy: 0.6438\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.8287 - accuracy: 0.6640 - val_loss: 0.8397 - val_accuracy: 0.6437\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.8157 - accuracy: 0.6642 - val_loss: 0.8289 - val_accuracy: 0.6434\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.8034 - accuracy: 0.6655 - val_loss: 0.8143 - val_accuracy: 0.6438\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.7918 - accuracy: 0.6649 - val_loss: 0.8058 - val_accuracy: 0.6434\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.7818 - accuracy: 0.6645 - val_loss: 0.7945 - val_accuracy: 0.6435\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.7715 - accuracy: 0.6650 - val_loss: 0.7879 - val_accuracy: 0.6432\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.7626 - accuracy: 0.6656 - val_loss: 0.7732 - val_accuracy: 0.6435\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.7533 - accuracy: 0.6664 - val_loss: 0.7649 - val_accuracy: 0.6437\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.7450 - accuracy: 0.6657 - val_loss: 0.7588 - val_accuracy: 0.6434\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.7374 - accuracy: 0.6667 - val_loss: 0.7514 - val_accuracy: 0.6432\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.7300 - accuracy: 0.6672 - val_loss: 0.7444 - val_accuracy: 0.6438\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.7231 - accuracy: 0.6662 - val_loss: 0.7387 - val_accuracy: 0.6443\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.7165 - accuracy: 0.6666 - val_loss: 0.7335 - val_accuracy: 0.6446\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.7105 - accuracy: 0.6672 - val_loss: 0.7274 - val_accuracy: 0.6449\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.7046 - accuracy: 0.6668 - val_loss: 0.7201 - val_accuracy: 0.6446\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6995 - accuracy: 0.6667 - val_loss: 0.7141 - val_accuracy: 0.6446\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6943 - accuracy: 0.6671 - val_loss: 0.7135 - val_accuracy: 0.6439\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6904 - accuracy: 0.6673 - val_loss: 0.7089 - val_accuracy: 0.6443\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6858 - accuracy: 0.6671 - val_loss: 0.7033 - val_accuracy: 0.6444\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6816 - accuracy: 0.6688 - val_loss: 0.6991 - val_accuracy: 0.6441\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6782 - accuracy: 0.6684 - val_loss: 0.6977 - val_accuracy: 0.6444\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 2s 128ms/step - loss: 0.6749 - accuracy: 0.6674 - val_loss: 0.6923 - val_accuracy: 0.6438\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6703 - accuracy: 0.6690 - val_loss: 0.6888 - val_accuracy: 0.6449\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6681 - accuracy: 0.6687 - val_loss: 0.6919 - val_accuracy: 0.6442\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6654 - accuracy: 0.6690 - val_loss: 0.6855 - val_accuracy: 0.6443\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6620 - accuracy: 0.6686 - val_loss: 0.6805 - val_accuracy: 0.6444\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6595 - accuracy: 0.6692 - val_loss: 0.6801 - val_accuracy: 0.6444\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6572 - accuracy: 0.6692 - val_loss: 0.6758 - val_accuracy: 0.6446\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6553 - accuracy: 0.6678 - val_loss: 0.6745 - val_accuracy: 0.6449\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6533 - accuracy: 0.6693 - val_loss: 0.6708 - val_accuracy: 0.6448\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6508 - accuracy: 0.6691 - val_loss: 0.6727 - val_accuracy: 0.6456\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6490 - accuracy: 0.6688 - val_loss: 0.6781 - val_accuracy: 0.6450\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6479 - accuracy: 0.6687 - val_loss: 0.6706 - val_accuracy: 0.6446\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6462 - accuracy: 0.6682 - val_loss: 0.6662 - val_accuracy: 0.6446\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6449 - accuracy: 0.6690 - val_loss: 0.6646 - val_accuracy: 0.6455\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6431 - accuracy: 0.6688 - val_loss: 0.6624 - val_accuracy: 0.6450\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6416 - accuracy: 0.6687 - val_loss: 0.6618 - val_accuracy: 0.6446\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6410 - accuracy: 0.6690 - val_loss: 0.6585 - val_accuracy: 0.6449\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6395 - accuracy: 0.6699 - val_loss: 0.6597 - val_accuracy: 0.6451\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6388 - accuracy: 0.6681 - val_loss: 0.6649 - val_accuracy: 0.6449\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6373 - accuracy: 0.6704 - val_loss: 0.6614 - val_accuracy: 0.6449\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6363 - accuracy: 0.6700 - val_loss: 0.6601 - val_accuracy: 0.6446\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6354 - accuracy: 0.6703 - val_loss: 0.6555 - val_accuracy: 0.6446\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6342 - accuracy: 0.6698 - val_loss: 0.6555 - val_accuracy: 0.6453\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6337 - accuracy: 0.6704 - val_loss: 0.6548 - val_accuracy: 0.6452\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6331 - accuracy: 0.6701 - val_loss: 0.6555 - val_accuracy: 0.6455\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6324 - accuracy: 0.6702 - val_loss: 0.6569 - val_accuracy: 0.6452\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6320 - accuracy: 0.6698 - val_loss: 0.6553 - val_accuracy: 0.6452\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6313 - accuracy: 0.6706 - val_loss: 0.6512 - val_accuracy: 0.6448\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6305 - accuracy: 0.6702 - val_loss: 0.6520 - val_accuracy: 0.6459\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6307 - accuracy: 0.6701 - val_loss: 0.6545 - val_accuracy: 0.6456\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6302 - accuracy: 0.6695 - val_loss: 0.6550 - val_accuracy: 0.6455\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6289 - accuracy: 0.6706 - val_loss: 0.6541 - val_accuracy: 0.6453\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6291 - accuracy: 0.6696 - val_loss: 0.6546 - val_accuracy: 0.6448\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6286 - accuracy: 0.6708 - val_loss: 0.6540 - val_accuracy: 0.6455\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6275 - accuracy: 0.6721 - val_loss: 0.6507 - val_accuracy: 0.6455\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6272 - accuracy: 0.6714 - val_loss: 0.6503 - val_accuracy: 0.6449\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 2s 134ms/step - loss: 0.6273 - accuracy: 0.6718 - val_loss: 0.6504 - val_accuracy: 0.6456\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6263 - accuracy: 0.6711 - val_loss: 0.6506 - val_accuracy: 0.6455\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6263 - accuracy: 0.6710 - val_loss: 0.6527 - val_accuracy: 0.6449\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 2s 133ms/step - loss: 0.6262 - accuracy: 0.6718 - val_loss: 0.6549 - val_accuracy: 0.6444\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 2s 125ms/step - loss: 0.6261 - accuracy: 0.6714 - val_loss: 0.6509 - val_accuracy: 0.6452\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6258 - accuracy: 0.6691 - val_loss: 0.6530 - val_accuracy: 0.6446\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6249 - accuracy: 0.6718 - val_loss: 0.6551 - val_accuracy: 0.6446\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6250 - accuracy: 0.6716 - val_loss: 0.6484 - val_accuracy: 0.6460\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6242 - accuracy: 0.6720 - val_loss: 0.6464 - val_accuracy: 0.6459\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6245 - accuracy: 0.6716 - val_loss: 0.6498 - val_accuracy: 0.6457\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6247 - accuracy: 0.6714 - val_loss: 0.6517 - val_accuracy: 0.6457\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6242 - accuracy: 0.6716 - val_loss: 0.6566 - val_accuracy: 0.6448\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6244 - accuracy: 0.6715 - val_loss: 0.6556 - val_accuracy: 0.6449\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6237 - accuracy: 0.6724 - val_loss: 0.6507 - val_accuracy: 0.6453\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6234 - accuracy: 0.6715 - val_loss: 0.6518 - val_accuracy: 0.6453\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6238 - accuracy: 0.6720 - val_loss: 0.6515 - val_accuracy: 0.6455\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6226 - accuracy: 0.6727 - val_loss: 0.6500 - val_accuracy: 0.6457\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6229 - accuracy: 0.6723 - val_loss: 0.6475 - val_accuracy: 0.6458\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6228 - accuracy: 0.6723 - val_loss: 0.6453 - val_accuracy: 0.6457\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6230 - accuracy: 0.6715 - val_loss: 0.6445 - val_accuracy: 0.6458\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6230 - accuracy: 0.6721 - val_loss: 0.6456 - val_accuracy: 0.6461\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6229 - accuracy: 0.6711 - val_loss: 0.6470 - val_accuracy: 0.6453\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6223 - accuracy: 0.6728 - val_loss: 0.6502 - val_accuracy: 0.6456\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6218 - accuracy: 0.6727 - val_loss: 0.6504 - val_accuracy: 0.6457\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6218 - accuracy: 0.6721 - val_loss: 0.6457 - val_accuracy: 0.6459\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6221 - accuracy: 0.6726 - val_loss: 0.6483 - val_accuracy: 0.6459\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6220 - accuracy: 0.6728 - val_loss: 0.6512 - val_accuracy: 0.6457\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6224 - accuracy: 0.6715 - val_loss: 0.6507 - val_accuracy: 0.6460\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6218 - accuracy: 0.6726 - val_loss: 0.6536 - val_accuracy: 0.6458\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6222 - accuracy: 0.6724 - val_loss: 0.6517 - val_accuracy: 0.6458\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6213 - accuracy: 0.6731 - val_loss: 0.6502 - val_accuracy: 0.6458\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6215 - accuracy: 0.6732 - val_loss: 0.6485 - val_accuracy: 0.6458\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6211 - accuracy: 0.6730 - val_loss: 0.6479 - val_accuracy: 0.6456\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6213 - accuracy: 0.6718 - val_loss: 0.6459 - val_accuracy: 0.6451\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6225 - accuracy: 0.6716 - val_loss: 0.6423 - val_accuracy: 0.6448\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6215 - accuracy: 0.6726 - val_loss: 0.6457 - val_accuracy: 0.6451\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6209 - accuracy: 0.6732 - val_loss: 0.6488 - val_accuracy: 0.6459\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6215 - accuracy: 0.6723 - val_loss: 0.6464 - val_accuracy: 0.6458\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6210 - accuracy: 0.6732 - val_loss: 0.6477 - val_accuracy: 0.6456\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6208 - accuracy: 0.6730 - val_loss: 0.6497 - val_accuracy: 0.6452\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6211 - accuracy: 0.6722 - val_loss: 0.6444 - val_accuracy: 0.6463\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6208 - accuracy: 0.6723 - val_loss: 0.6462 - val_accuracy: 0.6465\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6211 - accuracy: 0.6725 - val_loss: 0.6445 - val_accuracy: 0.6457\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6204 - accuracy: 0.6729 - val_loss: 0.6450 - val_accuracy: 0.6452\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6209 - accuracy: 0.6735 - val_loss: 0.6470 - val_accuracy: 0.6456\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6205 - accuracy: 0.6725 - val_loss: 0.6458 - val_accuracy: 0.6458\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6206 - accuracy: 0.6732 - val_loss: 0.6453 - val_accuracy: 0.6458\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6204 - accuracy: 0.6720 - val_loss: 0.6489 - val_accuracy: 0.6456\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6199 - accuracy: 0.6730 - val_loss: 0.6503 - val_accuracy: 0.6457\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6205 - accuracy: 0.6726 - val_loss: 0.6481 - val_accuracy: 0.6458\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6199 - accuracy: 0.6737 - val_loss: 0.6488 - val_accuracy: 0.6467\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6199 - accuracy: 0.6734 - val_loss: 0.6511 - val_accuracy: 0.6461\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6199 - accuracy: 0.6728 - val_loss: 0.6497 - val_accuracy: 0.6462\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6202 - accuracy: 0.6734 - val_loss: 0.6506 - val_accuracy: 0.6465\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6207 - accuracy: 0.6728 - val_loss: 0.6484 - val_accuracy: 0.6460\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6196 - accuracy: 0.6736 - val_loss: 0.6466 - val_accuracy: 0.6469\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6200 - accuracy: 0.6735 - val_loss: 0.6513 - val_accuracy: 0.6462\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6195 - accuracy: 0.6733 - val_loss: 0.6496 - val_accuracy: 0.6462\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6201 - accuracy: 0.6730 - val_loss: 0.6483 - val_accuracy: 0.6457\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6195 - accuracy: 0.6737 - val_loss: 0.6454 - val_accuracy: 0.6458\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6198 - accuracy: 0.6731 - val_loss: 0.6441 - val_accuracy: 0.6457\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6200 - accuracy: 0.6733 - val_loss: 0.6474 - val_accuracy: 0.6465\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6197 - accuracy: 0.6739 - val_loss: 0.6472 - val_accuracy: 0.6463\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6196 - accuracy: 0.6732 - val_loss: 0.6478 - val_accuracy: 0.6459\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 2s 128ms/step - loss: 0.6197 - accuracy: 0.6727 - val_loss: 0.6495 - val_accuracy: 0.6461\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.6193 - accuracy: 0.6735 - val_loss: 0.6503 - val_accuracy: 0.6461\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6194 - accuracy: 0.6737 - val_loss: 0.6477 - val_accuracy: 0.6444\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6198 - accuracy: 0.6733 - val_loss: 0.6466 - val_accuracy: 0.6458\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6193 - accuracy: 0.6731 - val_loss: 0.6440 - val_accuracy: 0.6456\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6193 - accuracy: 0.6741 - val_loss: 0.6478 - val_accuracy: 0.6449\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6191 - accuracy: 0.6737 - val_loss: 0.6500 - val_accuracy: 0.6451\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6188 - accuracy: 0.6743 - val_loss: 0.6507 - val_accuracy: 0.6453\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 2s 133ms/step - loss: 0.6185 - accuracy: 0.6737 - val_loss: 0.6521 - val_accuracy: 0.6461\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 2s 136ms/step - loss: 0.6192 - accuracy: 0.6743 - val_loss: 0.6485 - val_accuracy: 0.6460\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 2s 135ms/step - loss: 0.6188 - accuracy: 0.6744 - val_loss: 0.6500 - val_accuracy: 0.6456\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6188 - accuracy: 0.6739 - val_loss: 0.6489 - val_accuracy: 0.6455\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6187 - accuracy: 0.6736 - val_loss: 0.6510 - val_accuracy: 0.6459\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6183 - accuracy: 0.6739 - val_loss: 0.6471 - val_accuracy: 0.6462\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6185 - accuracy: 0.6740 - val_loss: 0.6496 - val_accuracy: 0.6453\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6189 - accuracy: 0.6734 - val_loss: 0.6484 - val_accuracy: 0.6456\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6186 - accuracy: 0.6737 - val_loss: 0.6452 - val_accuracy: 0.6452\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6187 - accuracy: 0.6726 - val_loss: 0.6440 - val_accuracy: 0.6460\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6183 - accuracy: 0.6741 - val_loss: 0.6472 - val_accuracy: 0.6453\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6178 - accuracy: 0.6746 - val_loss: 0.6543 - val_accuracy: 0.6455\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6181 - accuracy: 0.6748 - val_loss: 0.6515 - val_accuracy: 0.6457\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6188 - accuracy: 0.6743 - val_loss: 0.6543 - val_accuracy: 0.6459\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6189 - accuracy: 0.6737 - val_loss: 0.6500 - val_accuracy: 0.6456\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6183 - accuracy: 0.6738 - val_loss: 0.6493 - val_accuracy: 0.6456\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6181 - accuracy: 0.6743 - val_loss: 0.6498 - val_accuracy: 0.6458\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6182 - accuracy: 0.6739 - val_loss: 0.6533 - val_accuracy: 0.6459\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6185 - accuracy: 0.6742 - val_loss: 0.6496 - val_accuracy: 0.6457\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6184 - accuracy: 0.6737 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6178 - accuracy: 0.6747 - val_loss: 0.6488 - val_accuracy: 0.6459\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6181 - accuracy: 0.6738 - val_loss: 0.6447 - val_accuracy: 0.6459\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6178 - accuracy: 0.6761 - val_loss: 0.6446 - val_accuracy: 0.6461\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6182 - accuracy: 0.6747 - val_loss: 0.6452 - val_accuracy: 0.6460\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6184 - accuracy: 0.6742 - val_loss: 0.6503 - val_accuracy: 0.6453\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6178 - accuracy: 0.6745 - val_loss: 0.6503 - val_accuracy: 0.6461\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6181 - accuracy: 0.6751 - val_loss: 0.6472 - val_accuracy: 0.6463\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6180 - accuracy: 0.6740 - val_loss: 0.6528 - val_accuracy: 0.6457\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6185 - accuracy: 0.6744 - val_loss: 0.6507 - val_accuracy: 0.6462\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6177 - accuracy: 0.6746 - val_loss: 0.6574 - val_accuracy: 0.6459\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6178 - accuracy: 0.6746 - val_loss: 0.6549 - val_accuracy: 0.6458\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6181 - accuracy: 0.6746 - val_loss: 0.6502 - val_accuracy: 0.6458\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6179 - accuracy: 0.6748 - val_loss: 0.6447 - val_accuracy: 0.6461\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6182 - accuracy: 0.6744 - val_loss: 0.6494 - val_accuracy: 0.6458\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6172 - accuracy: 0.6760 - val_loss: 0.6467 - val_accuracy: 0.6458\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6175 - accuracy: 0.6742 - val_loss: 0.6448 - val_accuracy: 0.6465\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6179 - accuracy: 0.6745 - val_loss: 0.6447 - val_accuracy: 0.6450\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6173 - accuracy: 0.6745 - val_loss: 0.6474 - val_accuracy: 0.6465\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6173 - accuracy: 0.6751 - val_loss: 0.6496 - val_accuracy: 0.6461\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6175 - accuracy: 0.6747 - val_loss: 0.6496 - val_accuracy: 0.6458\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6180 - accuracy: 0.6740 - val_loss: 0.6477 - val_accuracy: 0.6461\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6168 - accuracy: 0.6746 - val_loss: 0.6488 - val_accuracy: 0.6455\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6174 - accuracy: 0.6753 - val_loss: 0.6469 - val_accuracy: 0.6451\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6175 - accuracy: 0.6745 - val_loss: 0.6503 - val_accuracy: 0.6451\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6166 - accuracy: 0.6755 - val_loss: 0.6539 - val_accuracy: 0.6458\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6173 - accuracy: 0.6746 - val_loss: 0.6561 - val_accuracy: 0.6459\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6174 - accuracy: 0.6754 - val_loss: 0.6531 - val_accuracy: 0.6456\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6177 - accuracy: 0.6737 - val_loss: 0.6465 - val_accuracy: 0.6456\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6171 - accuracy: 0.6758 - val_loss: 0.6444 - val_accuracy: 0.6457\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6170 - accuracy: 0.6746 - val_loss: 0.6465 - val_accuracy: 0.6460\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6175 - accuracy: 0.6741 - val_loss: 0.6465 - val_accuracy: 0.6456\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6171 - accuracy: 0.6744 - val_loss: 0.6444 - val_accuracy: 0.6455\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6175 - accuracy: 0.6738 - val_loss: 0.6461 - val_accuracy: 0.6451\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6164 - accuracy: 0.6765 - val_loss: 0.6489 - val_accuracy: 0.6459\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6169 - accuracy: 0.6754 - val_loss: 0.6490 - val_accuracy: 0.6458\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6161 - accuracy: 0.6764 - val_loss: 0.6547 - val_accuracy: 0.6462\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6169 - accuracy: 0.6756 - val_loss: 0.6517 - val_accuracy: 0.6458\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6168 - accuracy: 0.6757 - val_loss: 0.6515 - val_accuracy: 0.6451\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6172 - accuracy: 0.6743 - val_loss: 0.6484 - val_accuracy: 0.6451\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6166 - accuracy: 0.6753 - val_loss: 0.6505 - val_accuracy: 0.6461\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.6169 - accuracy: 0.6748 - val_loss: 0.6481 - val_accuracy: 0.6453\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6166 - accuracy: 0.6754 - val_loss: 0.6499 - val_accuracy: 0.6449\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6168 - accuracy: 0.6753 - val_loss: 0.6492 - val_accuracy: 0.6449\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6167 - accuracy: 0.6754 - val_loss: 0.6531 - val_accuracy: 0.6444\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6163 - accuracy: 0.6755 - val_loss: 0.6500 - val_accuracy: 0.6455\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6163 - accuracy: 0.6756 - val_loss: 0.6480 - val_accuracy: 0.6459\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6163 - accuracy: 0.6763 - val_loss: 0.6460 - val_accuracy: 0.6461\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6169 - accuracy: 0.6757 - val_loss: 0.6453 - val_accuracy: 0.6455\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6174 - accuracy: 0.6739 - val_loss: 0.6473 - val_accuracy: 0.6453\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6176 - accuracy: 0.6750 - val_loss: 0.6482 - val_accuracy: 0.6463\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6174 - accuracy: 0.6749 - val_loss: 0.6490 - val_accuracy: 0.6460\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6164 - accuracy: 0.6763 - val_loss: 0.6522 - val_accuracy: 0.6457\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6168 - accuracy: 0.6753 - val_loss: 0.6499 - val_accuracy: 0.6455\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6162 - accuracy: 0.6752 - val_loss: 0.6500 - val_accuracy: 0.6462\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6160 - accuracy: 0.6772 - val_loss: 0.6485 - val_accuracy: 0.6457\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6158 - accuracy: 0.6766 - val_loss: 0.6495 - val_accuracy: 0.6460\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6160 - accuracy: 0.6764 - val_loss: 0.6480 - val_accuracy: 0.6462\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6166 - accuracy: 0.6754 - val_loss: 0.6485 - val_accuracy: 0.6460\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6162 - accuracy: 0.6761 - val_loss: 0.6430 - val_accuracy: 0.6448\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6162 - accuracy: 0.6760 - val_loss: 0.6450 - val_accuracy: 0.6460\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 2s 147ms/step - loss: 0.6164 - accuracy: 0.6760 - val_loss: 0.6452 - val_accuracy: 0.6455\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 2s 143ms/step - loss: 0.6163 - accuracy: 0.6758 - val_loss: 0.6472 - val_accuracy: 0.6456\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 2s 136ms/step - loss: 0.6155 - accuracy: 0.6762 - val_loss: 0.6472 - val_accuracy: 0.6448\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6160 - accuracy: 0.6757 - val_loss: 0.6488 - val_accuracy: 0.6453\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6156 - accuracy: 0.6765 - val_loss: 0.6477 - val_accuracy: 0.6450\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6157 - accuracy: 0.6771 - val_loss: 0.6475 - val_accuracy: 0.6457\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6158 - accuracy: 0.6752 - val_loss: 0.6468 - val_accuracy: 0.6457\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6159 - accuracy: 0.6754 - val_loss: 0.6469 - val_accuracy: 0.6453\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6157 - accuracy: 0.6765 - val_loss: 0.6495 - val_accuracy: 0.6459\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6161 - accuracy: 0.6761 - val_loss: 0.6525 - val_accuracy: 0.6456\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6159 - accuracy: 0.6771 - val_loss: 0.6535 - val_accuracy: 0.6453\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6155 - accuracy: 0.6767 - val_loss: 0.6453 - val_accuracy: 0.6459\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6162 - accuracy: 0.6759 - val_loss: 0.6474 - val_accuracy: 0.6455\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6161 - accuracy: 0.6751 - val_loss: 0.6475 - val_accuracy: 0.6460\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6158 - accuracy: 0.6752 - val_loss: 0.6487 - val_accuracy: 0.6465\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6153 - accuracy: 0.6770 - val_loss: 0.6518 - val_accuracy: 0.6461\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 2s 131ms/step - loss: 0.6156 - accuracy: 0.6758 - val_loss: 0.6531 - val_accuracy: 0.6462\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 2s 133ms/step - loss: 0.6160 - accuracy: 0.6759 - val_loss: 0.6504 - val_accuracy: 0.6466\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 2s 132ms/step - loss: 0.6163 - accuracy: 0.6766 - val_loss: 0.6523 - val_accuracy: 0.6459\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 2s 134ms/step - loss: 0.6154 - accuracy: 0.6771 - val_loss: 0.6467 - val_accuracy: 0.6447\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6151 - accuracy: 0.6774 - val_loss: 0.6495 - val_accuracy: 0.6459\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6159 - accuracy: 0.6758 - val_loss: 0.6544 - val_accuracy: 0.6460\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6156 - accuracy: 0.6765 - val_loss: 0.6618 - val_accuracy: 0.6458\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6156 - accuracy: 0.6768 - val_loss: 0.6534 - val_accuracy: 0.6456\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6156 - accuracy: 0.6774 - val_loss: 0.6510 - val_accuracy: 0.6461\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6154 - accuracy: 0.6764 - val_loss: 0.6461 - val_accuracy: 0.6460\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6154 - accuracy: 0.6764 - val_loss: 0.6521 - val_accuracy: 0.6458\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6153 - accuracy: 0.6762 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6153 - accuracy: 0.6768 - val_loss: 0.6476 - val_accuracy: 0.6450\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6159 - accuracy: 0.6761 - val_loss: 0.6458 - val_accuracy: 0.6448\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6155 - accuracy: 0.6763 - val_loss: 0.6445 - val_accuracy: 0.6457\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6153 - accuracy: 0.6769 - val_loss: 0.6488 - val_accuracy: 0.6455\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6151 - accuracy: 0.6771 - val_loss: 0.6550 - val_accuracy: 0.6459\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6156 - accuracy: 0.6765 - val_loss: 0.6500 - val_accuracy: 0.6453\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6151 - accuracy: 0.6773 - val_loss: 0.6518 - val_accuracy: 0.6461\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6155 - accuracy: 0.6764 - val_loss: 0.6527 - val_accuracy: 0.6451\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6147 - accuracy: 0.6772 - val_loss: 0.6474 - val_accuracy: 0.6458\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6154 - accuracy: 0.6765 - val_loss: 0.6486 - val_accuracy: 0.6467\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6145 - accuracy: 0.6774 - val_loss: 0.6534 - val_accuracy: 0.6461\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6151 - accuracy: 0.6769 - val_loss: 0.6506 - val_accuracy: 0.6457\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6154 - accuracy: 0.6757 - val_loss: 0.6485 - val_accuracy: 0.6458\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6147 - accuracy: 0.6761 - val_loss: 0.6537 - val_accuracy: 0.6455\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6149 - accuracy: 0.6761 - val_loss: 0.6505 - val_accuracy: 0.6460\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 2s 130ms/step - loss: 0.6147 - accuracy: 0.6769 - val_loss: 0.6511 - val_accuracy: 0.6466\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 2s 128ms/step - loss: 0.6147 - accuracy: 0.6766 - val_loss: 0.6519 - val_accuracy: 0.6458\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6148 - accuracy: 0.6771 - val_loss: 0.6518 - val_accuracy: 0.6457\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6146 - accuracy: 0.6776 - val_loss: 0.6495 - val_accuracy: 0.6451\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 2s 130ms/step - loss: 0.6146 - accuracy: 0.6770 - val_loss: 0.6470 - val_accuracy: 0.6452\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6151 - accuracy: 0.6764 - val_loss: 0.6462 - val_accuracy: 0.6446\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6149 - accuracy: 0.6775 - val_loss: 0.6502 - val_accuracy: 0.6451\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6156 - accuracy: 0.6762 - val_loss: 0.6490 - val_accuracy: 0.6449\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6153 - accuracy: 0.6766 - val_loss: 0.6455 - val_accuracy: 0.6451\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6150 - accuracy: 0.6769 - val_loss: 0.6444 - val_accuracy: 0.6455\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6148 - accuracy: 0.6772 - val_loss: 0.6485 - val_accuracy: 0.6448\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6143 - accuracy: 0.6783 - val_loss: 0.6511 - val_accuracy: 0.6453\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6147 - accuracy: 0.6773 - val_loss: 0.6449 - val_accuracy: 0.6458\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6143 - accuracy: 0.6772 - val_loss: 0.6500 - val_accuracy: 0.6457\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6148 - accuracy: 0.6772 - val_loss: 0.6512 - val_accuracy: 0.6456\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6143 - accuracy: 0.6777 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6144 - accuracy: 0.6777 - val_loss: 0.6434 - val_accuracy: 0.6453\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6147 - accuracy: 0.6764 - val_loss: 0.6486 - val_accuracy: 0.6450\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6148 - accuracy: 0.6765 - val_loss: 0.6534 - val_accuracy: 0.6448\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6144 - accuracy: 0.6774 - val_loss: 0.6495 - val_accuracy: 0.6455\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6142 - accuracy: 0.6781 - val_loss: 0.6539 - val_accuracy: 0.6453\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6142 - accuracy: 0.6778 - val_loss: 0.6555 - val_accuracy: 0.6451\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6148 - accuracy: 0.6773 - val_loss: 0.6464 - val_accuracy: 0.6448\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6149 - accuracy: 0.6766 - val_loss: 0.6468 - val_accuracy: 0.6467\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6145 - accuracy: 0.6772 - val_loss: 0.6493 - val_accuracy: 0.6451\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6145 - accuracy: 0.6776 - val_loss: 0.6478 - val_accuracy: 0.6450\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6143 - accuracy: 0.6768 - val_loss: 0.6513 - val_accuracy: 0.6453\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6140 - accuracy: 0.6768 - val_loss: 0.6461 - val_accuracy: 0.6449\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6144 - accuracy: 0.6782 - val_loss: 0.6446 - val_accuracy: 0.6461\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6145 - accuracy: 0.6780 - val_loss: 0.6454 - val_accuracy: 0.6455\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6141 - accuracy: 0.6776 - val_loss: 0.6464 - val_accuracy: 0.6448\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6140 - accuracy: 0.6772 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6139 - accuracy: 0.6784 - val_loss: 0.6522 - val_accuracy: 0.6447\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 2s 130ms/step - loss: 0.6140 - accuracy: 0.6778 - val_loss: 0.6504 - val_accuracy: 0.6447\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 2s 123ms/step - loss: 0.6143 - accuracy: 0.6770 - val_loss: 0.6480 - val_accuracy: 0.6452\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6145 - accuracy: 0.6766 - val_loss: 0.6497 - val_accuracy: 0.6455\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6138 - accuracy: 0.6779 - val_loss: 0.6493 - val_accuracy: 0.6444\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 2s 130ms/step - loss: 0.6139 - accuracy: 0.6784 - val_loss: 0.6441 - val_accuracy: 0.6452\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6140 - accuracy: 0.6773 - val_loss: 0.6479 - val_accuracy: 0.6447\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.6142 - accuracy: 0.6767 - val_loss: 0.6490 - val_accuracy: 0.6452\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6136 - accuracy: 0.6774 - val_loss: 0.6483 - val_accuracy: 0.6450\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6140 - accuracy: 0.6770 - val_loss: 0.6488 - val_accuracy: 0.6449\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 2s 141ms/step - loss: 0.6141 - accuracy: 0.6782 - val_loss: 0.6481 - val_accuracy: 0.6455\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 2s 138ms/step - loss: 0.6141 - accuracy: 0.6778 - val_loss: 0.6473 - val_accuracy: 0.6451\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 2s 126ms/step - loss: 0.6137 - accuracy: 0.6777 - val_loss: 0.6481 - val_accuracy: 0.6457\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6142 - accuracy: 0.6771 - val_loss: 0.6472 - val_accuracy: 0.6461\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6138 - accuracy: 0.6784 - val_loss: 0.6482 - val_accuracy: 0.6457\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6133 - accuracy: 0.6784 - val_loss: 0.6514 - val_accuracy: 0.6453\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6134 - accuracy: 0.6772 - val_loss: 0.6516 - val_accuracy: 0.6448\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6137 - accuracy: 0.6775 - val_loss: 0.6472 - val_accuracy: 0.6449\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6138 - accuracy: 0.6784 - val_loss: 0.6463 - val_accuracy: 0.6448\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6139 - accuracy: 0.6776 - val_loss: 0.6513 - val_accuracy: 0.6459\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6138 - accuracy: 0.6785 - val_loss: 0.6501 - val_accuracy: 0.6452\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6137 - accuracy: 0.6779 - val_loss: 0.6538 - val_accuracy: 0.6448\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6142 - accuracy: 0.6774 - val_loss: 0.6477 - val_accuracy: 0.6451\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6139 - accuracy: 0.6767 - val_loss: 0.6503 - val_accuracy: 0.6448\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6137 - accuracy: 0.6780 - val_loss: 0.6478 - val_accuracy: 0.6449\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6134 - accuracy: 0.6783 - val_loss: 0.6478 - val_accuracy: 0.6456\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6140 - accuracy: 0.6775 - val_loss: 0.6451 - val_accuracy: 0.6456\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6133 - accuracy: 0.6779 - val_loss: 0.6501 - val_accuracy: 0.6458\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6135 - accuracy: 0.6775 - val_loss: 0.6488 - val_accuracy: 0.6463\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6133 - accuracy: 0.6784 - val_loss: 0.6470 - val_accuracy: 0.6455\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6135 - accuracy: 0.6781 - val_loss: 0.6499 - val_accuracy: 0.6458\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6136 - accuracy: 0.6778 - val_loss: 0.6555 - val_accuracy: 0.6462\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6137 - accuracy: 0.6775 - val_loss: 0.6544 - val_accuracy: 0.6455\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6133 - accuracy: 0.6775 - val_loss: 0.6552 - val_accuracy: 0.6451\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6142 - accuracy: 0.6772 - val_loss: 0.6530 - val_accuracy: 0.6457\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6142 - accuracy: 0.6776 - val_loss: 0.6482 - val_accuracy: 0.6459\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6134 - accuracy: 0.6778 - val_loss: 0.6463 - val_accuracy: 0.6462\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6133 - accuracy: 0.6777 - val_loss: 0.6473 - val_accuracy: 0.6459\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6133 - accuracy: 0.6778 - val_loss: 0.6496 - val_accuracy: 0.6451\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6136 - accuracy: 0.6781 - val_loss: 0.6446 - val_accuracy: 0.6456\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6133 - accuracy: 0.6778 - val_loss: 0.6456 - val_accuracy: 0.6463\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6129 - accuracy: 0.6783 - val_loss: 0.6494 - val_accuracy: 0.6466\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6129 - accuracy: 0.6783 - val_loss: 0.6502 - val_accuracy: 0.6461\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6135 - accuracy: 0.6774 - val_loss: 0.6491 - val_accuracy: 0.6457\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6131 - accuracy: 0.6773 - val_loss: 0.6449 - val_accuracy: 0.6453\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6131 - accuracy: 0.6780 - val_loss: 0.6484 - val_accuracy: 0.6451\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6134 - accuracy: 0.6773 - val_loss: 0.6499 - val_accuracy: 0.6458\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 2s 141ms/step - loss: 0.6132 - accuracy: 0.6778 - val_loss: 0.6484 - val_accuracy: 0.6459\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 2s 128ms/step - loss: 0.6134 - accuracy: 0.6779 - val_loss: 0.6453 - val_accuracy: 0.6457\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6135 - accuracy: 0.6784 - val_loss: 0.6440 - val_accuracy: 0.6456\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6133 - accuracy: 0.6782 - val_loss: 0.6460 - val_accuracy: 0.6452\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6135 - accuracy: 0.6785 - val_loss: 0.6490 - val_accuracy: 0.6463\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6134 - accuracy: 0.6781 - val_loss: 0.6532 - val_accuracy: 0.6465\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6128 - accuracy: 0.6777 - val_loss: 0.6549 - val_accuracy: 0.6455\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6123 - accuracy: 0.6788 - val_loss: 0.6527 - val_accuracy: 0.6455\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 0.6125 - accuracy: 0.6797 - val_loss: 0.6489 - val_accuracy: 0.6462\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6131 - accuracy: 0.6775 - val_loss: 0.6514 - val_accuracy: 0.6460\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 2s 130ms/step - loss: 0.6132 - accuracy: 0.6779 - val_loss: 0.6475 - val_accuracy: 0.6447\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6130 - accuracy: 0.6786 - val_loss: 0.6542 - val_accuracy: 0.6457\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6126 - accuracy: 0.6782 - val_loss: 0.6528 - val_accuracy: 0.6452\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6128 - accuracy: 0.6778 - val_loss: 0.6521 - val_accuracy: 0.6447\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 2s 138ms/step - loss: 0.6126 - accuracy: 0.6776 - val_loss: 0.6510 - val_accuracy: 0.6452\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6127 - accuracy: 0.6781 - val_loss: 0.6559 - val_accuracy: 0.6455\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 2s 132ms/step - loss: 0.6123 - accuracy: 0.6786 - val_loss: 0.6483 - val_accuracy: 0.6453\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6129 - accuracy: 0.6781 - val_loss: 0.6479 - val_accuracy: 0.6452\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6124 - accuracy: 0.6786 - val_loss: 0.6479 - val_accuracy: 0.6462\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6124 - accuracy: 0.6786 - val_loss: 0.6514 - val_accuracy: 0.6462\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6120 - accuracy: 0.6789 - val_loss: 0.6519 - val_accuracy: 0.6462\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6120 - accuracy: 0.6788 - val_loss: 0.6493 - val_accuracy: 0.6460\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.6118 - accuracy: 0.6798 - val_loss: 0.6518 - val_accuracy: 0.6456\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 2s 128ms/step - loss: 0.6122 - accuracy: 0.6781 - val_loss: 0.6491 - val_accuracy: 0.6457\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6128 - accuracy: 0.6786 - val_loss: 0.6490 - val_accuracy: 0.6468\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6124 - accuracy: 0.6793 - val_loss: 0.6490 - val_accuracy: 0.6459\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6124 - accuracy: 0.6785 - val_loss: 0.6491 - val_accuracy: 0.6452\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6132 - accuracy: 0.6780 - val_loss: 0.6544 - val_accuracy: 0.6455\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6121 - accuracy: 0.6784 - val_loss: 0.6522 - val_accuracy: 0.6457\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6123 - accuracy: 0.6784 - val_loss: 0.6563 - val_accuracy: 0.6470\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6123 - accuracy: 0.6793 - val_loss: 0.6527 - val_accuracy: 0.6461\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6119 - accuracy: 0.6784 - val_loss: 0.6505 - val_accuracy: 0.6459\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6123 - accuracy: 0.6782 - val_loss: 0.6524 - val_accuracy: 0.6453\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6124 - accuracy: 0.6782 - val_loss: 0.6556 - val_accuracy: 0.6463\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6128 - accuracy: 0.6780 - val_loss: 0.6516 - val_accuracy: 0.6463\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6126 - accuracy: 0.6783 - val_loss: 0.6516 - val_accuracy: 0.6463\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6125 - accuracy: 0.6789 - val_loss: 0.6467 - val_accuracy: 0.6457\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6122 - accuracy: 0.6784 - val_loss: 0.6451 - val_accuracy: 0.6459\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6121 - accuracy: 0.6790 - val_loss: 0.6458 - val_accuracy: 0.6466\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6118 - accuracy: 0.6782 - val_loss: 0.6521 - val_accuracy: 0.6456\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6118 - accuracy: 0.6789 - val_loss: 0.6534 - val_accuracy: 0.6457\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6116 - accuracy: 0.6800 - val_loss: 0.6527 - val_accuracy: 0.6461\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6118 - accuracy: 0.6785 - val_loss: 0.6509 - val_accuracy: 0.6455\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6129 - accuracy: 0.6781 - val_loss: 0.6499 - val_accuracy: 0.6443\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6120 - accuracy: 0.6780 - val_loss: 0.6491 - val_accuracy: 0.6459\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6120 - accuracy: 0.6787 - val_loss: 0.6490 - val_accuracy: 0.6468\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6119 - accuracy: 0.6790 - val_loss: 0.6508 - val_accuracy: 0.6467\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6119 - accuracy: 0.6782 - val_loss: 0.6522 - val_accuracy: 0.6455\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6114 - accuracy: 0.6787 - val_loss: 0.6457 - val_accuracy: 0.6466\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6119 - accuracy: 0.6794 - val_loss: 0.6500 - val_accuracy: 0.6459\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6117 - accuracy: 0.6790 - val_loss: 0.6480 - val_accuracy: 0.6462\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6118 - accuracy: 0.6790 - val_loss: 0.6519 - val_accuracy: 0.6461\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6118 - accuracy: 0.6798 - val_loss: 0.6480 - val_accuracy: 0.6463\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6122 - accuracy: 0.6789 - val_loss: 0.6461 - val_accuracy: 0.6466\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6117 - accuracy: 0.6790 - val_loss: 0.6473 - val_accuracy: 0.6460\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6118 - accuracy: 0.6797 - val_loss: 0.6468 - val_accuracy: 0.6459\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6116 - accuracy: 0.6783 - val_loss: 0.6470 - val_accuracy: 0.6453\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6123 - accuracy: 0.6785 - val_loss: 0.6511 - val_accuracy: 0.6446\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6116 - accuracy: 0.6788 - val_loss: 0.6499 - val_accuracy: 0.6457\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6117 - accuracy: 0.6790 - val_loss: 0.6534 - val_accuracy: 0.6457\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6123 - accuracy: 0.6786 - val_loss: 0.6538 - val_accuracy: 0.6460\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6121 - accuracy: 0.6787 - val_loss: 0.6468 - val_accuracy: 0.6460\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6114 - accuracy: 0.6796 - val_loss: 0.6487 - val_accuracy: 0.6460\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6113 - accuracy: 0.6800 - val_loss: 0.6506 - val_accuracy: 0.6461\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6123 - accuracy: 0.6784 - val_loss: 0.6503 - val_accuracy: 0.6466\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6122 - accuracy: 0.6780 - val_loss: 0.6464 - val_accuracy: 0.6462\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6123 - accuracy: 0.6787 - val_loss: 0.6446 - val_accuracy: 0.6448\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6121 - accuracy: 0.6799 - val_loss: 0.6474 - val_accuracy: 0.6461\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6116 - accuracy: 0.6793 - val_loss: 0.6520 - val_accuracy: 0.6458\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6108 - accuracy: 0.6805 - val_loss: 0.6535 - val_accuracy: 0.6458\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6118 - accuracy: 0.6799 - val_loss: 0.6527 - val_accuracy: 0.6465\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6115 - accuracy: 0.6805 - val_loss: 0.6544 - val_accuracy: 0.6457\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6113 - accuracy: 0.6798 - val_loss: 0.6496 - val_accuracy: 0.6462\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6118 - accuracy: 0.6791 - val_loss: 0.6522 - val_accuracy: 0.6467\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6114 - accuracy: 0.6794 - val_loss: 0.6477 - val_accuracy: 0.6457\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6111 - accuracy: 0.6803 - val_loss: 0.6434 - val_accuracy: 0.6458\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6120 - accuracy: 0.6798 - val_loss: 0.6489 - val_accuracy: 0.6457\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6110 - accuracy: 0.6791 - val_loss: 0.6504 - val_accuracy: 0.6451\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6115 - accuracy: 0.6795 - val_loss: 0.6466 - val_accuracy: 0.6452\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6112 - accuracy: 0.6791 - val_loss: 0.6436 - val_accuracy: 0.6462\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6116 - accuracy: 0.6802 - val_loss: 0.6480 - val_accuracy: 0.6459\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6114 - accuracy: 0.6799 - val_loss: 0.6453 - val_accuracy: 0.6451\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6119 - accuracy: 0.6786 - val_loss: 0.6439 - val_accuracy: 0.6448\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.6112 - accuracy: 0.6797 - val_loss: 0.6511 - val_accuracy: 0.6462\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6115 - accuracy: 0.6790 - val_loss: 0.6557 - val_accuracy: 0.6459\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.6115 - accuracy: 0.6798 - val_loss: 0.6558 - val_accuracy: 0.6465\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6115 - accuracy: 0.6787 - val_loss: 0.6481 - val_accuracy: 0.6460\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6107 - accuracy: 0.6801 - val_loss: 0.6482 - val_accuracy: 0.6461\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6111 - accuracy: 0.6798 - val_loss: 0.6481 - val_accuracy: 0.6453\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6106 - accuracy: 0.6796 - val_loss: 0.6522 - val_accuracy: 0.6463\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6113 - accuracy: 0.6794 - val_loss: 0.6513 - val_accuracy: 0.6452\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6110 - accuracy: 0.6804 - val_loss: 0.6510 - val_accuracy: 0.6462\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6111 - accuracy: 0.6800 - val_loss: 0.6468 - val_accuracy: 0.6458\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6109 - accuracy: 0.6807 - val_loss: 0.6480 - val_accuracy: 0.6467\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6111 - accuracy: 0.6792 - val_loss: 0.6462 - val_accuracy: 0.6468\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6108 - accuracy: 0.6792 - val_loss: 0.6519 - val_accuracy: 0.6477\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6115 - accuracy: 0.6782 - val_loss: 0.6504 - val_accuracy: 0.6463\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6115 - accuracy: 0.6791 - val_loss: 0.6631 - val_accuracy: 0.6465\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6113 - accuracy: 0.6791 - val_loss: 0.6486 - val_accuracy: 0.6467\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6108 - accuracy: 0.6799 - val_loss: 0.6464 - val_accuracy: 0.6477\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6109 - accuracy: 0.6800 - val_loss: 0.6491 - val_accuracy: 0.6468\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6117 - accuracy: 0.6789 - val_loss: 0.6509 - val_accuracy: 0.6474\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6107 - accuracy: 0.6799 - val_loss: 0.6524 - val_accuracy: 0.6466\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6109 - accuracy: 0.6806 - val_loss: 0.6510 - val_accuracy: 0.6467\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6109 - accuracy: 0.6802 - val_loss: 0.6560 - val_accuracy: 0.6467\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6112 - accuracy: 0.6792 - val_loss: 0.6563 - val_accuracy: 0.6467\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6103 - accuracy: 0.6801 - val_loss: 0.6464 - val_accuracy: 0.6457\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6107 - accuracy: 0.6800 - val_loss: 0.6488 - val_accuracy: 0.6461\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6109 - accuracy: 0.6792 - val_loss: 0.6535 - val_accuracy: 0.6466\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6102 - accuracy: 0.6816 - val_loss: 0.6551 - val_accuracy: 0.6472\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6106 - accuracy: 0.6799 - val_loss: 0.6480 - val_accuracy: 0.6468\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6110 - accuracy: 0.6791 - val_loss: 0.6525 - val_accuracy: 0.6465\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6110 - accuracy: 0.6792 - val_loss: 0.6509 - val_accuracy: 0.6458\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.6112 - accuracy: 0.6795 - val_loss: 0.6506 - val_accuracy: 0.6460\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6114 - accuracy: 0.6799 - val_loss: 0.6556 - val_accuracy: 0.6467\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6111 - accuracy: 0.6810 - val_loss: 0.6495 - val_accuracy: 0.6463\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6102 - accuracy: 0.6797 - val_loss: 0.6588 - val_accuracy: 0.6475\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6108 - accuracy: 0.6801 - val_loss: 0.6533 - val_accuracy: 0.6460\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6108 - accuracy: 0.6801 - val_loss: 0.6498 - val_accuracy: 0.6457\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6108 - accuracy: 0.6802 - val_loss: 0.6472 - val_accuracy: 0.6470\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6101 - accuracy: 0.6804 - val_loss: 0.6505 - val_accuracy: 0.6465\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6105 - accuracy: 0.6804 - val_loss: 0.6489 - val_accuracy: 0.6468\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.6111 - accuracy: 0.6786 - val_loss: 0.6505 - val_accuracy: 0.6474\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.6102 - accuracy: 0.6804 - val_loss: 0.6511 - val_accuracy: 0.6466\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6102 - accuracy: 0.6807 - val_loss: 0.6571 - val_accuracy: 0.6450\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6107 - accuracy: 0.6801 - val_loss: 0.6525 - val_accuracy: 0.6460\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6100 - accuracy: 0.6810 - val_loss: 0.6534 - val_accuracy: 0.6458\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6106 - accuracy: 0.6804 - val_loss: 0.6464 - val_accuracy: 0.6471\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6104 - accuracy: 0.6801 - val_loss: 0.6470 - val_accuracy: 0.6462\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6103 - accuracy: 0.6800 - val_loss: 0.6498 - val_accuracy: 0.6461\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6102 - accuracy: 0.6809 - val_loss: 0.6496 - val_accuracy: 0.6455\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6109 - accuracy: 0.6794 - val_loss: 0.6529 - val_accuracy: 0.6460\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6112 - accuracy: 0.6800 - val_loss: 0.6533 - val_accuracy: 0.6467\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.6106 - accuracy: 0.6788 - val_loss: 0.6539 - val_accuracy: 0.6468\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6104 - accuracy: 0.6800 - val_loss: 0.6514 - val_accuracy: 0.6460\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.6105 - accuracy: 0.6800 - val_loss: 0.6526 - val_accuracy: 0.6467\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6104 - accuracy: 0.6801 - val_loss: 0.6513 - val_accuracy: 0.6462\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6099 - accuracy: 0.6807 - val_loss: 0.6539 - val_accuracy: 0.6470\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.6095 - accuracy: 0.6809 - val_loss: 0.6545 - val_accuracy: 0.6463\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.6099 - accuracy: 0.6798 - val_loss: 0.6523 - val_accuracy: 0.6463\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6100 - accuracy: 0.6799"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    for i in range(0, 5):\n",
    "        column_model.fit(train_ds, epochs=100, validation_data=val_ds)\n",
    "        column_model.optimizer.lr.assign(column_model.optimizer.lr * 0.99)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) SDF, ORP, SBP, TP, FGP, TPP, FTP, OR, DR, NR, Pace, WP, TM, HM, WP5 with unsupported characters which will be renamed to sdf, orp, sbp, tp, fgp, tpp, ftp, or, dr, nr, pace, wp, tm, hm, wp5 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: column_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: column_model/assets\n"
     ]
    }
   ],
   "source": [
    "column_model.save('column_model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  63\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers in the base model: \", len(column_model.layers))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "column_model.trainable = True\n",
    "fine_tune_at = 200\n",
    "for layer in column_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "column_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " SDF (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " integer_lookup (IntegerLookup)  (None, 1)           0           ['SDF[0][0]']                    \n",
      "                                                                                                  \n",
      " ORP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " SBP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " TP (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " FGP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " TPP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " FTP (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " OR (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " DR (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " NR (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Pace (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " WP (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " TM (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " HM (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " category_encoding (CategoryEnc  (None, 16)          0           ['integer_lookup[0][0]']         \n",
      " oding)                                                                                           \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 1)            3           ['ORP[0][0]']                    \n",
      "                                                                                                  \n",
      " normalization_1 (Normalization  (None, 1)           3           ['SBP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_2 (Normalization  (None, 1)           3           ['TP[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_3 (Normalization  (None, 1)           3           ['FGP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_4 (Normalization  (None, 1)           3           ['TPP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_5 (Normalization  (None, 1)           3           ['FTP[0][0]']                    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_6 (Normalization  (None, 1)           3           ['OR[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_7 (Normalization  (None, 1)           3           ['DR[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_8 (Normalization  (None, 1)           3           ['NR[0][0]']                     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_9 (Normalization  (None, 1)           3           ['Pace[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_10 (Normalizatio  (None, 1)           3           ['WP[0][0]']                     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " normalization_11 (Normalizatio  (None, 1)           3           ['TM[0][0]']                     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " normalization_12 (Normalizatio  (None, 1)           3           ['HM[0][0]']                     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['category_encoding[0][0]',      \n",
      "                                                                  'normalization[0][0]',          \n",
      "                                                                  'normalization_1[0][0]',        \n",
      "                                                                  'normalization_2[0][0]',        \n",
      "                                                                  'normalization_3[0][0]',        \n",
      "                                                                  'normalization_4[0][0]',        \n",
      "                                                                  'normalization_5[0][0]',        \n",
      "                                                                  'normalization_6[0][0]',        \n",
      "                                                                  'normalization_7[0][0]',        \n",
      "                                                                  'normalization_8[0][0]',        \n",
      "                                                                  'normalization_9[0][0]',        \n",
      "                                                                  'normalization_10[0][0]',       \n",
      "                                                                  'normalization_11[0][0]',       \n",
      "                                                                  'normalization_12[0][0]']       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          15360       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          262656      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512)          262656      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 512)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          262656      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 512)          262656      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          262656      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 512)          262656      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 512)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          262656      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 512)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 512)          262656      ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 512)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 512)          262656      ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 512)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 512)          262656      ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 512)          0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 512)          262656      ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 512)          0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 512)          262656      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 512)          0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 512)          262656      ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 512)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 512)          262656      ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 512)          0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1)            513         ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,955,752\n",
      "Trainable params: 0\n",
      "Non-trainable params: 3,955,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "column_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation Prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_model.trainable_variables)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tim/mambaforge/envs/env_arm/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n",
      "2023-03-13 09:52:23.329270: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.6757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 09:52:24.627503: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 230ms/step - loss: 0.6198 - accuracy: 0.6757 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.6195 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 0.6191 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6193 - accuracy: 0.6765 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6190 - accuracy: 0.6770 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6195 - accuracy: 0.6765 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.6190 - accuracy: 0.6770 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.6193 - accuracy: 0.6777 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6200 - accuracy: 0.6762 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.6200 - accuracy: 0.6762 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6191 - accuracy: 0.6770 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6199 - accuracy: 0.6760 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.6196 - accuracy: 0.6756 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6196 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.6195 - accuracy: 0.6771 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6196 - accuracy: 0.6757 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6191 - accuracy: 0.6766 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.6195 - accuracy: 0.6758 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6195 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6198 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6199 - accuracy: 0.6773 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6193 - accuracy: 0.6762 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6192 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6199 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6199 - accuracy: 0.6757 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6193 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6201 - accuracy: 0.6766 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6192 - accuracy: 0.6762 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6196 - accuracy: 0.6754 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6198 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6194 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 76ms/step - loss: 0.6188 - accuracy: 0.6780 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6198 - accuracy: 0.6759 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6194 - accuracy: 0.6771 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.6193 - accuracy: 0.6772 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6193 - accuracy: 0.6766 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6197 - accuracy: 0.6778 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6194 - accuracy: 0.6771 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6201 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6194 - accuracy: 0.6771 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.6195 - accuracy: 0.6762 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6194 - accuracy: 0.6765 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6198 - accuracy: 0.6765 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6195 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6205 - accuracy: 0.6766 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6199 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6194 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6194 - accuracy: 0.6777 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6191 - accuracy: 0.6768 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6192 - accuracy: 0.6771 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6199 - accuracy: 0.6756 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6197 - accuracy: 0.6777 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6188 - accuracy: 0.6773 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6193 - accuracy: 0.6756 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6196 - accuracy: 0.6776 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6193 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.6195 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6194 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.6194 - accuracy: 0.6761 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6194 - accuracy: 0.6770 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 78ms/step - loss: 0.6195 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 78ms/step - loss: 0.6193 - accuracy: 0.6775 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 78ms/step - loss: 0.6193 - accuracy: 0.6775 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 75ms/step - loss: 0.6202 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 76ms/step - loss: 0.6196 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6196 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 77ms/step - loss: 0.6199 - accuracy: 0.6765 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6196 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6199 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 76ms/step - loss: 0.6198 - accuracy: 0.6762 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 77ms/step - loss: 0.6195 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6191 - accuracy: 0.6776 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6192 - accuracy: 0.6770 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6190 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6196 - accuracy: 0.6767 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6198 - accuracy: 0.6757 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 75ms/step - loss: 0.6188 - accuracy: 0.6773 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 75ms/step - loss: 0.6194 - accuracy: 0.6775 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 78ms/step - loss: 0.6194 - accuracy: 0.6771 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6202 - accuracy: 0.6761 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6201 - accuracy: 0.6766 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 78ms/step - loss: 0.6193 - accuracy: 0.6772 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6198 - accuracy: 0.6761 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6189 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6200 - accuracy: 0.6760 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.6198 - accuracy: 0.6758 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6200 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6201 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 77ms/step - loss: 0.6191 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.6197 - accuracy: 0.6755 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6194 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6196 - accuracy: 0.6769 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6191 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.6193 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6194 - accuracy: 0.6773 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 78ms/step - loss: 0.6195 - accuracy: 0.6766 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.6198 - accuracy: 0.6763 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6195 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 0.6198 - accuracy: 0.6759 - val_loss: 0.6576 - val_accuracy: 0.6447\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 80ms/step - loss: 0.6192 - accuracy: 0.6764 - val_loss: 0.6576 - val_accuracy: 0.6447\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x105364fd0>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_model.fit(train_ds, epochs=100, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed2023 = {\"Seed\" : [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,10,10,10,10,11,11,11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14,15,15,15,15,16,16,16,16,16,16], \"Team\" : [\"PURDUE\", \"KANSAS\", \"ALABAMA\", \"HOUSTON\", \"MARQUETTE\", \"ARIZONA\", \"TEXAS\", \"GONZAGA\", \"KANSAS ST\", \"UCONN\", \"UCLA\", \"BAYLOR\", \"TENNESSEE\", \"TEXAS A&M\", \"XAVIER\", \"DUKE\", \" VIRGINIA\", \"MIAMI\", \"INDIANA\", \"ST MARY'S CA\", \"SAN DIEGO ST\", \"TCU\", \"IOWA ST\", \"MISSOURI\", \"NORTHWESTERN\", \"CREIGHTON\", \"MICHIGAN ST\", \"KENTUCKY\", \"MEMPHIS\", \"AUBURN\", \"MARYLAND\", \"ARKANSAS\", \"WEST VIRGINIA\", \"PENN ST\", \"FAU\", \"IOWA\", \"NC STATE\", \"ILLINOIS\", \"PROVIDENCE\", \"BOISE ST\", \"USC\", \"MISS ST\", \"UTAH ST\", \"PITTSBURGH\", \"NEVADA\", \"RUTGERS\", \"ORAL ROBERTS\", \"VCU\", \"DRAKE\", \"CHARLESTON\", \"YALE\", \"KENT ST\", \"IONA\", \"LOUISIANA\", \"KENNESAW ST\", \"GRAND CANYON\", \"UCSB\", \"FURMAN\", \"VERMONT\", \"UNC Asheville\", \"MONTANA ST\", \"COLGATE\", \"SE MISSOURI ST\", \"TX A&M Commerce\", \"FDU\", \"TEXAS ST\", \"HOWARD\", \"N KENTUCKY\"]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed2023 = pd.DataFrame(seed2023)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed2023"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
